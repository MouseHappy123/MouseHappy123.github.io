<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://MouseHappy123.github.io</id>
    <title>Gridea</title>
    <updated>2023-06-05T16:48:44.750Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://MouseHappy123.github.io"/>
    <link rel="self" href="https://MouseHappy123.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://MouseHappy123.github.io/images/avatar.png</logo>
    <icon>https://MouseHappy123.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[nginx连接数排查]]></title>
        <id>https://MouseHappy123.github.io/post/nginx-lian-jie-shu-pai-cha/</id>
        <link href="https://MouseHappy123.github.io/post/nginx-lian-jie-shu-pai-cha/">
        </link>
        <updated>2023-06-05T16:10:30.000Z</updated>
        <content type="html"><![CDATA[<h1 id="error现象">Error现象</h1>
<p>linux连接socket错误 11: Resource temporarily unavailable<br>
常见于Redis、MySQL、PHP-fpm这些应用连接socket时出现错误。</p>
<p>一般系统处于高负载时会出现Error，导致服务不可用。例如使用Nginx代理时，业务出现502 Bad Gateway，常见如下日志：</p>
<pre><code>[error] 10615#0: *172810754 connect() to unix:/xxx/www.sock failed (11: Resource temporarily unavailable) while connecting to upstream
</code></pre>
<h1 id="定位原因">定位原因</h1>
<p>系统net.core.somaxconn值过低导致backlog不够用。</p>
<h1 id="原理">原理</h1>
<p>net.core.somaxconn是Linux中的一个内核(kernel)参数，表示socket监听(listen)的backlog上限。学习更多：https://www.cnblogs.com/my-show-time/p/15206020.html</p>
<blockquote>
<p>对于一个TCP链接，Server与Client需要通过三次握手来建立网络链接，当三次握手成功之后，我们就可以看到端口状态由LISTEN转为ESTABLISHED，接着这条链路上就可以开始传送数据了</p>
</blockquote>
<blockquote>
<p>net.core.somaxconn是Linux中的一个内核(kernel)参数，表示socket监听(listen)的backlog上限。</p>
</blockquote>
<blockquote>
<p>什么是backlog？backlog就是 socket的监听队列，当一个请求(request)尚未被处理或者建立时，它就会进入backlog。</p>
</blockquote>
<blockquote>
<p>而socket server可以一次性处理backlog中的所有请求，处理后的请求不再位于监听队列中。</p>
</blockquote>
<blockquote>
<p>当Server处理请求较慢时，导致监听队列被填满后，新来的请求就会被拒绝。</p>
</blockquote>
<blockquote>
<p>backlog参数主要用于底层方法int listen(int sockfd, int backlog)， 在解释backlog参数之前，我们先了解下tcp在内核的请求过程，其实就是tcp的三次握手：<br>
<img src="https://img2022.cnblogs.com/blog/1043925/202203/1043925-20220325111920180-564432886.png" alt="" loading="lazy"></p>
</blockquote>
<ol>
<li>client发送SYN到server，将状态修改为SYN_SEND，如果server收到请求，则将状态修改为SYN_RCVD，并把该请求放到syns queue队列中。</li>
<li>server回复SYN+ACK给client，如果client收到请求，则将状态修改为ESTABLISHED，并发送ACK给server。</li>
<li>server收到ACK，将状态修改为ESTABLISHED，并把该请求从syns queue中放到accept queue。</li>
</ol>
<p>在linux系统内核中维护了两个队列：syns queue和accept queue</p>
<ul>
<li>syns queue<br>
用于保存半连接状态的请求，其大小通过/proc/sys/net/ipv4/tcp_max_syn_backlog指定，一般默认值是512，不过这个设置有效的前提是系统的syncookies功能被禁用。互联网常见的TCP SYN FLOOD恶意DOS攻击方式就是建立大量的半连接状态的请求，然后丢弃，导致syns queue不能保存其它正常的请求。</li>
<li>accept queue<br>
用于保存全连接状态的请求，其大小通过/proc/sys/net/core/somaxconn指定，在使用listen函数时，内核会根据传入的backlog参数与系统参数somaxconn，取二者的较小值。</li>
</ul>
<p>如果accpet queue队列满了，server将发送一个ECONNREFUSED错误信息Connection refused到client。</p>
<h1 id="操作系统配置">操作系统配置</h1>
<p>cat /proc/sys/net/core/somaxconn<br>
系统内核最大连接数，listen的最大值不能大于somaxconn<br>
<img src="https://MouseHappy123.github.io/post-images/1685981871353.png" alt="" loading="lazy"><br>
配置生效方式</p>
<ol>
<li>echo 4096 &gt; /proc/sys/net/core/somaxconn</li>
<li>添加 net.core.somaxconn=4096 到 /etc/sysctl.conf</li>
</ol>
<h1 id="nginx配置">nginx配置</h1>
<p>在 Nginx 中，可以使用 <code>backlog</code> 参数来配置监听队列的长度。<code>backlog</code> 参数用于指定当监听队列已满时，客户端连接的排队等待数量。<br>
示例配置如下：</p>
<pre><code class="language-nginx">server {
    listen 80 backlog=1024;
    ...
}
</code></pre>
<p>在上述配置中，<code>backlog=1024</code> 表示设置监听队列的长度为 1024。具体的值可以根据实际需求进行调整。请注意，该参数的有效范围取决于操作系统和 Nginx 的版本，具体限制可参考相关文档。</p>
<p>设置较大的 <code>backlog</code> 值可以在高并发情况下提高连接的处理能力，但同时也会占用更多的系统资源。因此，在设置 <code>backlog</code> 时需要根据服务器的硬件配置、预期的并发连接数和负载情况进行权衡和调整。</p>
<h1 id="php-fpm配置">php-fpm配置</h1>
<h2 id="进程数">进程数</h2>
<p>php-fpm worker进程数<br>
pm.max_children = 128<br>
pm.start_servers = 20<br>
pm.min_spare_servers = 5<br>
pm.max_spare_servers = 35</p>
<ul>
<li>使用说明</li>
</ul>
<pre><code>; Choose how the process manager will control the number of child processes.
; Possible Values:
;   static  - a fixed number (pm.max_children) of child processes;
;   dynamic - the number of child processes are set dynamically based on the
;             following directives. With this process management, there will be
;             always at least 1 children.
;             pm.max_children      - the maximum number of children that can
;                                    be alive at the same time.
;             pm.start_servers     - the number of children created on startup.
;             pm.min_spare_servers - the minimum number of children in 'idle'
;                                    state (waiting to process). If the number
;                                    of 'idle' processes is less than this
;                                    number then some children will be created.
;             pm.max_spare_servers - the maximum number of children in 'idle'
;                                    state (waiting to process). If the number
;                                    of 'idle' processes is greater than this
;                                    number then some children will be killed.
;  ondemand - no children are created at startup. Children will be forked when
;             new requests will connect. The following parameter are used:
;             pm.max_children           - the maximum number of children that
;                                         can be alive at the same time.
;             pm.process_idle_timeout   - The number of seconds after which
;                                         an idle process will be killed.
; Note: This value is mandatory.
pm = static
; The number of child processes to be created when pm is set to 'static' and the
; maximum number of child processes when pm is set to 'dynamic' or 'ondemand'.
; This value sets the limit on the number of simultaneous requests that will be
; served. Equivalent to the ApacheMaxClients directive with mpm_prefork.
; Equivalent to the PHP_FCGI_CHILDREN environment variable in the original PHP
; CGI. The below defaults are based on a server without much resources. Don't
; forget to tweak pm.* to fit your needs.
; Note: Used when pm is set to 'static', 'dynamic' or 'ondemand'
; Note: This value is mandatory.
pm.max_children = 128

; The number of child processes created on startup.
; Note: Used only when pm is set to 'dynamic'
; Default Value: min_spare_servers + (max_spare_servers - min_spare_servers) / 2
pm.start_servers = 20

; The desired minimum number of idle server processes.
; Note: Used only when pm is set to 'dynamic'
; Note: Mandatory when pm is set to 'dynamic'
pm.min_spare_servers = 5

; The desired maximum number of idle server processes.
; Note: Used only when pm is set to 'dynamic'
; Note: Mandatory when pm is set to 'dynamic'
pm.max_spare_servers = 35
</code></pre>
<h2 id="最大处理请求数">最大处理请求数</h2>
<p>最大处理请求数是指一个php-fpm的worker进程在处理多少个请求后就终止掉，master进程会重新respawn一个新的。<br>
这个配置的主要目的是避免php解释器或程序引用的第三方库造成的内存泄露。<br>
pm.max_requests = 10000</p>
<h2 id="最长执行时间">最长执行时间</h2>
<p>最大执行时间在php.ini和php-fpm.conf里都可以配置，配置项分别为 max_execution_time和request_terminate_timeout。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[go]]></title>
        <id>https://MouseHappy123.github.io/post/go/</id>
        <link href="https://MouseHappy123.github.io/post/go/">
        </link>
        <updated>2023-06-05T09:53:30.000Z</updated>
        <content type="html"><![CDATA[<p>按照值类型和引用类型来划分变量的话，可以参考以下分类：</p>
<p>值类型（Value Types）：</p>
<ul>
<li>基本类型：包括 bool、整数类型（int、int8、int16、int32、int64、uint、uint8、uint16、uint32、uint64）、浮点数类型（float32、float64）、字符串类型（string）、字符类型（byte、rune）等。</li>
<li>数组类型：具有固定长度且元素类型相同的数组。</li>
</ul>
<p>引用类型（Reference Types）：</p>
<ul>
<li>切片类型（Slice）：可变长度的动态数组。</li>
<li>映射类型（Map）：键值对的集合。</li>
<li>结构体类型（Struct）：自定义的复合数据类型，包含多个字段。</li>
<li>接口类型（Interface）：定义了一组方法的抽象类型。</li>
<li>函数类型（Function）：表示具有特定参数和返回值的函数。</li>
<li>通道类型（Channel）：用于协程之间的通信。</li>
</ul>
<p>在 Golang 中，值类型在赋值、传递参数和作为函数返回值时会进行拷贝，每个拷贝都有独立的内存空间，对其中一个变量的修改不会影响到其他变量。而引用类型则是通过引用（指针）来操作底层的数据，多个变量引用同一块内存空间，对其中一个变量的修改会影响到其他变量。</p>
<p>需要注意的是，即使是引用类型，在进行赋值操作时仍然是拷贝了一份引用（指针），所以多个变量指向同一个底层数据。这种情况下，对底层数据的修改会影响到所有引用该数据的变量。但如果修改的是引用本身（指针），则不会影响其他变量。</p>
<p>因此，在 Golang 中，根据变量的类型可以大致判断出它是值类型还是引用类型，从而了解变量在赋值、传递参数和作为函数返回值时的行为和影响。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka]]></title>
        <id>https://MouseHappy123.github.io/post/kafka/</id>
        <link href="https://MouseHappy123.github.io/post/kafka/">
        </link>
        <updated>2023-05-28T12:40:43.000Z</updated>
        <content type="html"><![CDATA[<p>Kafka 涉及的知识点如下图所示，本文将逐一讲解：</p>
<figure data-type="image" tabindex="1"><img src="https://pic3.zhimg.com/v2-efdc4afeddb070395b71e5f9e5e71e26_b.jpg" alt="" loading="lazy"></figure>
<p><strong>本文档参考了关于 Kafka 的官网及其他众多资料整理而成，为了整洁的排版及舒适的阅读，对于模糊不清晰的图片及黑白图片进行重新绘制成了高清彩图</strong>。</p>
<h2 id="一-消息队列"><strong>一、消息队列</strong></h2>
<h3 id="1-消息队列的介绍"><strong>1. 消息队列的介绍</strong></h3>
<p>消息（Message）是指在应用之间传送的数据，消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象。 消息队列（Message Queue）是一种应用间的通信方式，消息发送后可以立即返回，有消息系统来确保信息的可靠专递，消息发布者只管把消息发布到MQ中而不管谁来取，消息使用者只管从MQ中取消息而不管谁发布的，这样发布者和使用者都不用知道对方的存在。</p>
<h3 id="2-消息队列的应用场景"><strong>2. 消息队列的应用场景</strong></h3>
<p>消息队列在实际应用中包括如下四个场景：</p>
<ul>
<li><strong>应用耦合</strong>：多应用间通过消息队列对同一消息进行处理，避免调用接口失败导致整个过程失败；</li>
<li><strong>异步处理</strong>：多应用对消息队列中同一消息进行处理，应用间并发处理消息，相比串行处理，减少处理时间；</li>
<li><strong>限流削峰</strong>：广泛应用于秒杀或抢购活动中，避免流量过大导致应用系统挂掉的情况；</li>
<li><strong>消息驱动的系统</strong>：系统分为消息队列、消息生产者、消息消费者，生产者负责产生消息，消费者(可能有多个)负责对消息进行处理；</li>
</ul>
<p>下面详细介绍上述四个场景以及消息队列如何在上述四个场景中使用：</p>
<ol>
<li><strong>异步处理</strong></li>
</ol>
<p>具体场景：用户为了使用某个应用，进行注册，系统需要发送注册邮件并验证短信。对这两个操作的处理方式有两种：串行及并行。</p>
<ul>
<li>串行方式：新注册信息生成后，先发送注册邮件，再发送验证短信；</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://pic1.zhimg.com/v2-1c4fd3461960ae31eda0d57be83fbf4c_b.jpg" alt="" loading="lazy"></figure>
<p>在这种方式下，需要最终发送验证短信后再返回给客户端。</p>
<ul>
<li>并行处理：新注册信息写入后，由发短信和发邮件并行处理；</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://pic3.zhimg.com/v2-867e116704af8632b4651fb7c4403c02_b.jpg" alt="" loading="lazy"></figure>
<p>在这种方式下，发短信和发邮件 需处理完成后再返回给客户端。 假设以上三个子系统处理的时间均为50ms，且不考虑网络延迟，则总的处理时间：<br>
串行：50+50+50=150ms<br>
并行：50+50 = 100ms</p>
<ul>
<li>若使用消息队列：</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://pic1.zhimg.com/v2-fb9a601a2dbe22f5292d137f01722894_b.jpg" alt="" loading="lazy"></figure>
<p>在写入消息队列后立即返回成功给客户端，则总的响应时间依赖于写入消息队列的时间，而写入消息队列的时间本身是可以很快的，基本可以忽略不计，因此总的处理时间相比串行提高了2倍，相比并行提高了一倍；</p>
<ol>
<li><strong>应用耦合</strong></li>
</ol>
<p>具体场景：用户使用QQ相册上传一张图片，人脸识别系统会对该图片进行人脸识别，一般的做法是，服务器接收到图片后，图片上传系统立即调用人脸识别系统，调用完成后再返回成功，如下图所示：</p>
<p>该方法有如下缺点：</p>
<figure data-type="image" tabindex="5"><img src="https://pic2.zhimg.com/v2-75e7764bcee01b9c85ab51fe245ef86d_b.jpg" alt="" loading="lazy"></figure>
<ul>
<li>人脸识别系统被调失败，导致图片上传失败；</li>
<li>延迟高，需要人脸识别系统处理完成后，再返回给客户端，即使用户并不需要立即知道结果；</li>
<li>图片上传系统与人脸识别系统之间互相调用，需要做耦合；</li>
</ul>
<p>若使用消息队列：</p>
<figure data-type="image" tabindex="6"><img src="https://pic1.zhimg.com/v2-476929ea7dd211552877fa03d5f75468_b.jpg" alt="" loading="lazy"></figure>
<p>客户端上传图片后，图片上传系统将图片信息如uin、批次写入消息队列，直接返回成功；而人脸识别系统则定时从消息队列中取数据，完成对新增图片的识别。</p>
<p>此时图片上传系统并不需要关心人脸识别系统是否对这些图片信息的处理、以及何时对这些图片信息进行处理。事实上，由于用户并不需要立即知道人脸识别结果，人脸识别系统可以选择不同的调度策略，按照闲时、忙时、正常时间，对队列中的图片信息进行处理。</p>
<ol>
<li><strong>限流削峰</strong></li>
</ol>
<p>具体场景：购物网站开展秒杀活动，一般由于瞬时访问量过大，服务器接收过大，会导致流量暴增，相关系统无法处理请求甚至崩溃。而加入消息队列后，系统可以从消息队列中取数据，相当于消息队列做了一次缓冲。</p>
<figure data-type="image" tabindex="7"><img src="https://pic4.zhimg.com/v2-94d2d7d6d402639f01ba9bee1b1e3c3b_b.jpg" alt="" loading="lazy"></figure>
<p>该方法有如下优点：</p>
<ul>
<li>请求先入消息队列，而不是由业务处理系统直接处理，做了一次缓冲,极大地减少了业务处理系统的压力；</li>
<li>队列长度可以做限制，事实上，秒杀时，后入队列的用户无法秒杀到商品，这些请求可以直接被抛弃，返回活动已结束或商品已售完信息；</li>
</ul>
<p>4.<strong>消息驱动的系统</strong></p>
<p>具体场景：用户新上传了一批照片，人脸识别系统需要对这个用户的所有照片进行聚类，聚类完成后由对账系统重新生成用户的人脸索引(加快查询)。这三个子系统间由消息队列连接起来，前一个阶段的处理结果放入队列中，后一个阶段从队列中获取消息继续处理。</p>
<figure data-type="image" tabindex="8"><img src="https://pic3.zhimg.com/v2-9b4cbc69fc994f762ead039354e82aca_b.jpg" alt="" loading="lazy"></figure>
<p>该方法有如下优点：</p>
<ul>
<li>避免了直接调用下一个系统导致当前系统失败；</li>
<li>每个子系统对于消息的处理方式可以更为灵活，可以选择收到消息时就处理，可以选择定时处理，也可以划分时间段按不同处理速度处理；</li>
</ul>
<h3 id="3-消息队列的两种模式"><strong>3. 消息队列的两种模式</strong></h3>
<p>消息队列包括两种模式，点对点模式（point to point， queue）和发布/订阅模式（publish/subscribe，topic）</p>
<h3 id="1-点对点模式"><strong>1) 点对点模式</strong></h3>
<p>点对点模式下包括三个角色：</p>
<ul>
<li>消息队列</li>
<li>发送者 (生产者)</li>
<li>接收者（消费者）</li>
</ul>
<figure data-type="image" tabindex="9"><img src="https://pic1.zhimg.com/v2-6420fdf839ecadaa811ddca38982e244_b.jpg" alt="" loading="lazy"></figure>
<p>消息发送者生产消息发送到queue中，然后消息接收者从queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息接收者不可能消费到已经被消费的消息。</p>
<p>点对点模式特点：</p>
<ul>
<li>每个消息只有一个接收者（Consumer）(即一旦被消费，消息就不再在消息队列中)；</li>
<li>发送者和接发收者间没有依赖性，发送者发送消息之后，不管有没有接收者在运行，都不会影响到发送者下次发送消息；</li>
<li>接收者在成功接收消息之后需向队列应答成功，以便消息队列删除当前接收的消息；</li>
</ul>
<h3 id="2-发布订阅模式"><strong>2) 发布/订阅模式</strong></h3>
<p>发布/订阅模式下包括三个角色：</p>
<ul>
<li>角色主题（Topic）</li>
<li>发布者(Publisher)</li>
<li>订阅者(Subscriber)</li>
</ul>
<figure data-type="image" tabindex="10"><img src="https://pic4.zhimg.com/v2-1f3a0c7097f7b84b629e5564bffb750f_b.jpg" alt="" loading="lazy"></figure>
<p>发布者将消息发送到Topic，系统将这些消息传递给多个订阅者。</p>
<p>发布/订阅模式特点：</p>
<ul>
<li>每个消息可以有多个订阅者；</li>
<li>发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息。</li>
<li>为了消费消息，订阅者需要提前订阅该角色主题，并保持在线运行；</li>
</ul>
<h3 id="4-常用的消息队列介绍"><strong>4. 常用的消息队列介绍</strong></h3>
<h3 id="1-rabbitmq"><strong>1) RabbitMQ</strong></h3>
<p>RabbitMQ 2007年发布，是一个在AMQP(高级消息队列协议)基础上完成的，可复用的企业消息系统，是当前最主流的消息中间件之一。</p>
<h3 id="2-activemq"><strong>2) ActiveMQ</strong></h3>
<p>ActiveMQ是由Apache出品，ActiveMQ 是一个完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现。它非常快速，支持多种语言的客户端和协议，而且可以非常容易的嵌入到企业的应用环境中，并有许多高级功能。</p>
<h3 id="3-rocketmq"><strong>3) RocketMQ</strong></h3>
<p>RocketMQ出自 阿里公司的开源产品，用 Java 语言实现，在设计时参考了 Kafka，并做出了自己的一些改进，消息可靠性上比 Kafka 更好。RocketMQ在阿里集团被广泛应用在订单，交易，充值，流计算，消息推送，日志流式处理等。</p>
<h3 id="4-kafka"><strong>4) Kafka</strong></h3>
<p>Apache Kafka是一个分布式消息发布订阅系统。它最初由LinkedIn公司基于独特的设计实现为一个分布式的提交日志系统( a distributed commit log)，，之后成为Apache项目的一部分。Kafka系统快速、可扩展并且可持久化。它的分区特性，可复制和可容错都是其不错的特性。</p>
<h3 id="5-pulsar"><strong>5. Pulsar</strong></h3>
<p>Apahce Pulasr是一个企业级的发布-订阅消息系统，最初是由雅虎开发，是下一代云原生分布式消息流平台，集消息、存储、轻量化函数式计算为一体，采用计算与存储分离架构设计，支持多租户、持久化存储、多机房跨区域数据复制，具有强一致性、高吞吐、低延时及高可扩展性等流数据存储特性。</p>
<p>Pulsar 非常灵活：它既可以应用于像 Kafka 这样的分布式日志应用场景，也可以应用于像 RabbitMQ 这样的纯消息传递系统场景。它支持多种类型的订阅、多种交付保证、保留策略以及处理模式演变的方法，以及其他诸多特性。</p>
<h3 id="1-pulsar-的特性"><strong>1. Pulsar 的特性</strong></h3>
<ul>
<li><strong>内置多租户</strong>：不同的团队可以使用相同的集群并将其隔离，解决了许多管理难题。它支持隔离、身份验证、授权和配额；</li>
<li><strong>多层体系结构</strong>：Pulsar 将所有 topic 数据存储在由 Apache BookKeeper 支持的专业数据层中。存储和消息传递的分离解决了扩展、重新平衡和维护集群的许多问题。它还提高了可靠性，几乎不可能丢失数据。另外，在读取数据时可以直连 BookKeeper，且不影响实时摄取。例如，可以使用 Presto 对 topic 执行 SQL 查询，类似于 KSQL，但不会影响实时数据处理；</li>
<li><strong>虚拟 topic</strong>：由于采用 n 层体系结构，因此对 topic 的数量没有限制，topic 及其存储是分离的。用户还可以创建非持久性 topic；</li>
<li><strong>N 层存储</strong>：Kafka 的一个问题是，存储费用可能变高。因此，它很少用于存储&quot;冷&quot;数据，并且消息经常被删除，Apache Pulsar 可以借助分层存储自动将旧数据卸载到 Amazon S3 或其他数据存储系统，并且仍然向客户端展示透明视图；Pulsar 客户端可以从时间开始节点读取，就像所有消息都存在于日志中一样；</li>
</ul>
<h3 id="2-pulsar-存储架构"><strong>2. Pulsar 存储架构</strong></h3>
<p>Pulsar 的多层架构影响了存储数据的方式。<strong>Pulsar 将 topic 分区划分为分片（segment），然后将这些分片存储在 Apache BookKeeper 的存储节点上，以提高性能、可伸缩性和可用性</strong>。</p>
<figure data-type="image" tabindex="11"><img src="https://pic3.zhimg.com/v2-ce809e33e1699432bc0213fbb99b3d42_b.jpg" alt="" loading="lazy"></figure>
<p>Pulsar 的无限分布式日志以分片为中心，借助扩展日志存储（通过 Apache BookKeeper）实现，内置分层存储支持，因此分片可以均匀地分布在存储节点上。由于与任一给定 topic 相关的数据都不会与特定存储节点进行捆绑，因此很容易替换存储节点或缩扩容。另外，集群中最小或最慢的节点也不会成为存储或带宽的短板。</p>
<p>Pulsar 架构能实现<strong>分区管理，负载均衡</strong>，因此使用 Pulsar 能够快速扩展并达到高可用。这两点至关重要，所以 Pulsar 非常适合用来构建关键任务服务，如金融应用场景的计费平台，电子商务和零售商的交易处理系统，金融机构的实时风险控制系统等。</p>
<p>通过性能强大的 Netty 架构，数据从 producers 到 broker，再到 bookie 的转移都是零拷贝，不会生成副本。这一特性对所有流应用场景都非常友好，因为数据直接通过网络或磁盘进行传输，没有任何性能损失。</p>
<h3 id="3-pulsar-消息消费"><strong>3. Pulsar 消息消费</strong></h3>
<p>Pulsar 的消费模型采用了流拉取的方式。流拉取是长轮询的改进版，不仅实现了单个调用和请求之间的零等待，还可以提供双向消息流。通过流拉取模型，Pulsar 实现了端到端的低延迟，这种低延迟比所有现有的长轮询消息系统（如 Kafka）都低。</p>
<h3 id="6-kafka与pulsar对比"><strong>6. Kafka与Pulsar对比</strong></h3>
<h3 id="1-pulsar-的主要优势"><strong>1. Pulsar 的主要优势：</strong></h3>
<ul>
<li>更多功能：Pulsar Function、多租户、Schema registry、n 层存储、多种消费模式和持久性模式等；</li>
<li>更大的灵活性：3 种订阅类型（独占，共享和故障转移），用户可以在一个订阅上管理多个 topic；</li>
<li>易于操作运维：架构解耦和 n 层存储；</li>
<li>与 Presto 的 SQL 集成，可直接查询存储而不会影响 broker；</li>
<li>借助 n 层自动存储选项，可以更低成本地存储；</li>
</ul>
<h3 id="2-pulsar-的劣势"><strong>2. Pulsar 的劣势</strong></h3>
<p>Pulsar 并不完美，Pulsar 也存在一些问题：</p>
<ul>
<li>相对缺乏支持、文档和案例；</li>
<li>n 层体系结构导致需要更多组件：BookKeeper；</li>
<li>插件和客户端相对 Kafka 较少;</li>
<li>云中的支持较少，Confluent 具有托管云产品。</li>
</ul>
<h3 id="3-什么时候应该考虑-pulsar"><strong>3. 什么时候应该考虑 Pulsar</strong></h3>
<ul>
<li>同时需要像 RabbitMQ 这样的队列和 Kafka 这样的流处理程序；</li>
<li>需要易用的地理复制；</li>
<li>实现多租户，并确保每个团队的访问权限；</li>
<li>需要长时间保留消息，并且不想将其卸载到另一个存储中；</li>
<li>需要高性能，基准测试表明 Pulsar 提供了更低的延迟和更高的吞吐量；</li>
</ul>
<p><strong>总之，Pulsar还比较新，社区不完善，用的企业比较少，网上有价值的讨论和问题的解决比较少，远没有Kafka生态系统庞大，且用户量非常庞大，目前Kafka依旧是大数据领域消息队列的王者！所以我们还是以Kafka为主！</strong></p>
<h3 id="7-其他消息队列与kafka对比"><strong>7. 其他消息队列与Kafka对比</strong></h3>
<figure data-type="image" tabindex="12"><img src="https://pic3.zhimg.com/v2-725cef2306442d610378e80cf3f1553e_b.jpg" alt="" loading="lazy"></figure>
<h2 id="二-kafka基础"><strong>二、Kafka基础</strong></h2>
<h3 id="1-kafka的基本介绍"><strong>1. kafka的基本介绍</strong></h3>
<p>官网：<a href="http://kafka.apache.org/">http://kafka.apache.org/</a></p>
<p>kafka是最初由linkedin公司开发的，使用scala语言编写，kafka是一个分布式，分区的，多副本的，多订阅者的日志系统（分布式MQ系统），可以用于搜索日志，监控日志，访问日志等。</p>
<p>Kafka is a distributed,partitioned,replicated commit logservice。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。kafka对消息保存时根据Topic进行归类，发送消息者成为Producer,消息接受者成为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。无论是kafka集群，还是producer和consumer都依赖于zookeeper来保证系统可用性集群保存一些meta信息。</p>
<h3 id="2-kafka的好处"><strong>2. kafka的好处</strong></h3>
<ul>
<li><strong>可靠性</strong>：分布式的，分区，复本和容错的。</li>
<li><strong>可扩展性</strong>：kafka消息传递系统轻松缩放，无需停机。</li>
<li><strong>耐用性</strong>：kafka使用分布式提交日志，这意味着消息会尽可能快速的保存在磁盘上，因此它是持久的。</li>
<li><strong>性能</strong>：kafka对于发布和定于消息都具有高吞吐量。即使存储了许多TB的消息，他也爆出稳定的性能。</li>
<li><strong>kafka非常快</strong>：保证零停机和零数据丢失。</li>
</ul>
<h3 id="3-分布式的发布与订阅系统"><strong>3. 分布式的发布与订阅系统</strong></h3>
<p>apache kafka是一个分布式发布-订阅消息系统和一个强大的队列，可以处理大量的数据，并使能够将消息从一个端点传递到另一个端点，kafka适合离线和在线消息消费。kafka消息保留在磁盘上，并在集群内复制以防止数据丢失。kafka构建在zookeeper同步服务之上。它与apache和spark非常好的集成，应用于实时流式数据分析。</p>
<h3 id="4-kafka的主要应用场景"><strong>4. kafka的主要应用场景</strong></h3>
<h3 id="1-指标分析"><strong>1. 指标分析</strong></h3>
<p>kafka 通常用于操作监控数据。这设计聚合来自分布式应用程序的统计信息， 以产生操作的数据集中反馈</p>
<h3 id="2-日志聚合解决方法"><strong>2. 日志聚合解决方法</strong></h3>
<p>kafka可用于跨组织从多个服务器收集日志，并使他们以标准的格式提供给多个服务器。</p>
<h3 id="3-流式处理"><strong>3. 流式处理</strong></h3>
<p>流式处理框架（spark，storm，ﬂink）重主题中读取数据，对齐进行处理，并将处理后的数据写入新的主题，供 用户和应用程序使用，kafka的强耐久性在流处理的上下文中也非常的有用。</p>
<h2 id="三-kafka架构及组件"><strong>三、Kafka架构及组件</strong></h2>
<h3 id="1-kafka架构"><strong>1. kafka架构</strong></h3>
<figure data-type="image" tabindex="13"><img src="https://pic4.zhimg.com/v2-31e49760a17c2ff0f002b472f8494217_b.jpg" alt="" loading="lazy"></figure>
<ol>
<li><strong>生产者API</strong></li>
</ol>
<p>允许应用程序发布记录流至一个或者多个kafka的主题（topics）。</p>
<ol>
<li><strong>消费者API</strong></li>
</ol>
<p>允许应用程序订阅一个或者多个主题，并处理这些主题接收到的记录流。</p>
<p>3。 <strong>StreamsAPI</strong></p>
<p>允许应用程序充当流处理器（stream processor），从一个或者多个主题获取输入流，并生产一个输出流到一个或 者多个主题，能够有效的变化输入流为输出流。</p>
<ol>
<li><strong>ConnectAPI</strong></li>
</ol>
<p>允许构建和运行可重用的生产者或者消费者，能够把kafka主题连接到现有的应用程序或数据系统。例如：一个连接到关系数据库的连接器可能会获取每个表的变化。</p>
<figure data-type="image" tabindex="14"><img src="https://pic1.zhimg.com/v2-feecbd25eb7374203a2316d346e282cc_b.jpg" alt="" loading="lazy"></figure>
<p>注：<strong>在Kafka 2.8.0 版本，移除了对Zookeeper的依赖，通过KRaft进行自己的集群管理</strong>，使用Kafka内部的Quorum控制器来取代ZooKeeper，因此用户第一次可在完全不需要ZooKeeper的情况下执行Kafka，这不只节省运算资源，并且也使得Kafka效能更好，还可支持规模更大的集群。</p>
<p>过去Apache ZooKeeper是Kafka这类分布式系统的关键，ZooKeeper扮演协调代理的角色，所有代理服务器启动时，都会连接到Zookeeper进行注册，当代理状态发生变化时，Zookeeper也会储存这些数据，在过去，ZooKeeper是一个强大的工具，但是毕竟ZooKeeper是一个独立的软件，使得Kafka整个系统变得复杂，因此官方决定使用内部Quorum控制器来取代ZooKeeper。</p>
<p>这项工作从去年4月开始，而现在这项工作取得部分成果，用户将可以在2.8版本，在没有ZooKeeper的情况下执行Kafka，官方称这项功能为<strong>Kafka Raft元数据模式（KRaft）</strong>。在KRaft模式，过去由Kafka控制器和ZooKeeper所操作的元数据，将合并到这个新的Quorum控制器，并且在Kafka集群内部执行，当然，如果使用者有特殊使用情境，Quorum控制器也可以在专用的硬件上执行。</p>
<p>好，说完在新版本中移除zookeeper这个事，咱们在接着聊kafka的其他功能：</p>
<p>kafka支持消息持久化，消费端是主动拉取数据，消费状态和订阅关系由客户端负责维护，<strong>消息消费完后，不会立即删除，会保留历史消息</strong>。因此支持多订阅时，消息只会存储一份就可以。</p>
<ol>
<li><strong>broker</strong>：kafka集群中包含一个或者多个服务实例（节点），这种服务实例被称为broker（一个broker就是一个节点/一个服务器）；</li>
<li><strong>topic</strong>：每条发布到kafka集群的消息都属于某个类别，这个类别就叫做topic；</li>
<li><strong>partition</strong>：partition是一个物理上的概念，每个topic包含一个或者多个partition；</li>
<li><strong>segment</strong>：一个partition当中存在多个segment文件段，每个segment分为两部分，.log文件和 .index 文件，其中 .index 文件是索引文件，主要用于快速查询， .log 文件当中数据的偏移量位置；</li>
<li><strong>producer</strong>：消息的生产者，负责发布消息到 kafka 的 broker 中；</li>
<li><strong>consumer</strong>：消息的消费者，向 kafka 的 broker 中读取消息的客户端；</li>
<li><strong>consumer group</strong>：消费者组，每一个 consumer 属于一个特定的 consumer group（可以为每个consumer指定 groupName）；</li>
<li><strong>.log</strong>：存放数据文件；</li>
<li><strong>.index</strong>：存放.log文件的索引数据。</li>
</ol>
<h3 id="2-kafka-主要组件"><strong>2. Kafka 主要组件</strong></h3>
<h3 id="1-producer生产者"><strong>1. producer（生产者）</strong></h3>
<p>producer主要是用于生产消息，是kafka当中的消息生产者，生产的消息通过topic进行归类，保存到kafka的broker里面去。</p>
<h3 id="2-topic主题"><strong>2. topic（主题）</strong></h3>
<ol>
<li>kafka将消息以topic为单位进行归类；</li>
<li>topic特指kafka处理的消息源（feeds of messages）的不同分类；</li>
<li>topic是一种分类或者发布的一些列记录的名义上的名字。kafka主题始终是支持多用户订阅的；也就是说，一 个主题可以有零个，一个或者多个消费者订阅写入的数据；</li>
<li>在kafka集群中，可以有无数的主题；</li>
<li>生产者和消费者消费数据一般以主题为单位。更细粒度可以到分区级别。</li>
</ol>
<h3 id="3-partition分区"><strong>3. partition（分区）</strong></h3>
<p>kafka当中，topic是消息的归类，一个topic可以有多个分区（partition），每个分区保存部分topic的数据，所有的partition当中的数据全部合并起来，就是一个topic当中的所有的数据。</p>
<p>一个broker服务下，可以创建多个分区，broker数与分区数没有关系；<br>
在kafka中，每一个分区会有一个编号：编号从0开始。<br>
<strong>每一个分区内的数据是有序的，但全局的数据不能保证是有序的。</strong>（有序是指生产什么样顺序，消费时也是什么样的顺序）</p>
<h3 id="4-consumer消费者"><strong>4. consumer（消费者）</strong></h3>
<p>consumer是kafka当中的消费者，主要用于消费kafka当中的数据，消费者一定是归属于某个消费组中的。</p>
<h3 id="5-consumer-group消费者组"><strong>5. consumer group（消费者组）</strong></h3>
<p>消费者组由一个或者多个消费者组成，<strong>同一个组中的消费者对于同一条消息只消费一次</strong>。</p>
<p>每个消费者都属于某个消费者组，如果不指定，那么所有的消费者都属于默认的组。</p>
<p>每个消费者组都有一个ID，即group ID。组内的所有消费者协调在一起来消费一个订阅主题( topic)的所有分区(partition)。当然，<strong>每个分区只能由同一个消费组内的一个消费者(consumer)来消费</strong>，可以由不同的<strong>消费组</strong>来消费。</p>
<p><strong>partition数量决定了每个consumer group中并发消费者的最大数量</strong>。如下图：</p>
<figure data-type="image" tabindex="15"><img src="https://pic2.zhimg.com/v2-1cf1cf01715c113b6dba9bc33755edb9_b.jpg" alt="" loading="lazy"></figure>
<p>如上面左图所示，如果只有两个分区，即使一个组内的消费者有4个，也会有两个空闲的。<br>
如上面右图所示，有4个分区，每个消费者消费一个分区，并发量达到最大4。</p>
<p>在来看如下一幅图：</p>
<figure data-type="image" tabindex="16"><img src="https://pic3.zhimg.com/v2-bf5ce7f8345680ff24d4744ddf6c36ce_b.jpg" alt="" loading="lazy"></figure>
<p>如上图所示，不同的消费者组消费同一个topic，这个topic有4个分区，分布在两个节点上。左边的 消费组1有两个消费者，每个消费者就要消费两个分区才能把消息完整的消费完，右边的 消费组2有四个消费者，每个消费者消费一个分区即可。</p>
<p><strong>总结下kafka中分区与消费组的关系</strong>：</p>
<p>消费组： 由一个或者多个消费者组成，同一个组中的消费者对于同一条消息只消费一次。 <strong>某一个主题下的分区数，对于消费该主题的同一个消费组下的消费者数量，应该小于等于该主题下的分区数</strong>。</p>
<p>如：某一个主题有4个分区，那么消费组中的消费者应该小于等于4，而且最好与分区数成整数倍 1 2 4 这样。<strong>同一个分区下的数据，在同一时刻，不能同一个消费组的不同消费者消费</strong>。</p>
<p>总结：<strong>分区数越多，同一时间可以有越多的消费者来进行消费，消费数据的速度就会越快，提高消费的性能</strong>。</p>
<h3 id="6-partition-replicas分区副本"><strong>6. partition replicas（分区副本）</strong></h3>
<p>kafka 中的分区副本如下图所示：</p>
<figure data-type="image" tabindex="17"><img src="https://pic3.zhimg.com/v2-9c6842085ec000033ce4f95a5658d91a_b.jpg" alt="" loading="lazy"></figure>
<p><strong>副本数</strong>（replication-factor）：控制消息保存在几个broker（服务器）上，一般情况下副本数等于broker的个数。</p>
<p>一个broker服务下，不可以创建多个副本因子。<strong>创建主题时，副本因子应该小于等于可用的broker数</strong>。</p>
<p>副本因子操作以分区为单位的。每个分区都有各自的主副本和从副本；</p>
<p>主副本叫做leader，从副本叫做 follower（在有多个副本的情况下，kafka会为同一个分区下的所有分区，设定角色关系：一个leader和N个 follower），<strong>处于同步状态的副本叫做in-sync-replicas(ISR)</strong>;</p>
<p>follower通过拉的方式从leader同步数据。 <strong>消费者和生产者都是从leader读写数据，不与follower交互</strong>。</p>
<p>副本因子的作用：让kafka读取数据和写入数据时的可靠性。</p>
<p>副本因子是包含本身，同一个副本因子不能放在同一个broker中。</p>
<p>如果某一个分区有三个副本因子，就算其中一个挂掉，那么只会剩下的两个中，选择一个leader，但不会在其他的broker中，另启动一个副本（因为在另一台启动的话，存在数据传递，只要在机器之间有数据传递，就会长时间占用网络IO，kafka是一个高吞吐量的消息系统，这个情况不允许发生）所以不会在另一个broker中启动。</p>
<p>如果所有的副本都挂了，生产者如果生产数据到指定分区的话，将写入不成功。</p>
<p>lsr表示：当前可用的副本。</p>
<h3 id="7-segment文件"><strong>7. segment文件</strong></h3>
<p>一个partition当中由多个segment文件组成，每个segment文件，包含两部分，一个是 .log 文件，另外一个是 .index 文件，其中 .log 文件包含了我们发送的数据存储，.index 文件，记录的是我们.log文件的数据索引值，以便于我们加快数据的查询速度。</p>
<p><strong>索引文件与数据文件的关系</strong></p>
<p>既然它们是一一对应成对出现，必然有关系。索引文件中元数据指向对应数据文件中message的物理偏移地址。</p>
<p>比如索引文件中 3,497 代表：数据文件中的第三个message，它的偏移地址为497。</p>
<p>再来看数据文件中，Message 368772表示：在全局partiton中是第368772个message。</p>
<blockquote>
<p>注：segment index file 采取稀疏索引存储方式，减少索引文件大小，通过mmap（内存映射）可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针，它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。</p>
</blockquote>
<p>.index 与 .log 对应关系如下：</p>
<figure data-type="image" tabindex="18"><img src="https://pic3.zhimg.com/v2-cce29042ec33e8a9724ef70da24b8aca_b.jpg" alt="" loading="lazy"></figure>
<p>上图左半部分是索引文件，里面存储的是一对一对的key-value，其中key是消息在数据文件（对应的log文件）中的编号，比如“1,3,6,8……”， 分别表示在log文件中的第1条消息、第3条消息、第6条消息、第8条消息……</p>
<p>那么为什么在index文件中这些编号不是连续的呢？ 这是因为index文件中并没有为数据文件中的每条消息都建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。 这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。 但缺点是没有建立索引的Message也不能一次定位到其在数据文件的位置，从而需要做一次顺序扫描，但是这次顺序扫描的范围就很小了。</p>
<p>value 代表的是在全局partiton中的第几个消息。</p>
<p>以索引文件中元数据 3,497 为例，其中3代表在右边log数据文件中从上到下第3个消息， 497表示该消息的物理偏移地址（位置）为497(也表示在全局partiton表示第497个消息-顺序写入特性)。</p>
<p><strong>log日志目录及组成</strong> kafka在我们指定的log.dir目录下，会创建一些文件夹；名字是 （主题名字-分区名） 所组成的文件夹。 在（主题名字-分区名）的目录下，会有两个文件存在，如下所示：</p>
<pre><code class="language-text">#索引文件
00000000000000000000.index
#日志内容
00000000000000000000.log
</code></pre>
<p>在目录下的文件，会根据log日志的大小进行切分，.log文件的大小为1G的时候，就会进行切分文件；如下：</p>
<pre><code class="language-text">-rw-r--r--. 1 root root 389k  1月  17  18:03   00000000000000000000.index
-rw-r--r--. 1 root root 1.0G  1月  17  18:03   00000000000000000000.log
-rw-r--r--. 1 root root  10M  1月  17  18:03   00000000000000077894.index
-rw-r--r--. 1 root root 127M  1月  17  18:03   00000000000000077894.log
</code></pre>
<p>在kafka的设计中，将offset值作为了文件名的一部分。</p>
<p><strong>segment文件命名规则</strong>：partion全局的第一个segment从0开始，后续每个segment文件名为上一个全局 partion的最大offset（偏移message数）。数值最大为64位long大小，20位数字字符长度，没有数字就用 0 填充。</p>
<p>通过索引信息可以快速定位到message。通过index元数据全部映射到内存，可以避免segment File的IO磁盘操作；</p>
<p>通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。</p>
<p>稀疏索引：为了数据创建索引，但范围并不是为每一条创建，而是为某一个区间创建； 好处：就是可以减少索引值的数量。 不好的地方：找到索引区间之后，要得进行第二次处理。</p>
<h3 id="8-message的物理结构"><strong>8. message的物理结构</strong></h3>
<p>生产者发送到kafka的每条消息，都被kafka包装成了一个message</p>
<p>message 的物理结构如下图所示：</p>
<figure data-type="image" tabindex="19"><img src="https://pic1.zhimg.com/v2-5b1ed64c245d14ad24f1cd9d5288e174_b.jpg" alt="" loading="lazy"></figure>
<p>所以生产者发送给kafka的消息并不是直接存储起来，而是经过kafka的包装，每条消息都是上图这个结构，只有最后一个字段才是真正生产者发送的消息数据。</p>
<h2 id="四-kafka集群操作"><strong>四、Kafka集群操作</strong></h2>
<h3 id="1-创建topic"><strong>1. 创建topic</strong></h3>
<p>创建一个名字为test的主题， 有三个分区，有两个副本：</p>
<pre><code class="language-text">bin/kafka-topics.sh --create --zookeeper node01:2181 --replication-factor 2 --partitions 3 --topic test
</code></pre>
<h3 id="2-查看主题命令"><strong>2. 查看主题命令</strong></h3>
<p>查看kafka当中存在的主题：</p>
<pre><code class="language-text">bin/kafka-topics.sh  --list --zookeeper node01:2181,node02:2181,node03:2181
</code></pre>
<h3 id="3-生产者生产数据"><strong>3. 生产者生产数据</strong></h3>
<p>模拟生产者来生产数据:</p>
<pre><code class="language-text">bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test
</code></pre>
<h3 id="4-消费者消费数据"><strong>4. 消费者消费数据</strong></h3>
<p>执行以下命令来模拟消费者进行消费数据:</p>
<pre><code class="language-text">bin/kafka-console-consumer.sh --from-beginning --topic test  --zookeeper node01:2181,node02:2181,node03:2181
</code></pre>
<h3 id="5-运行describe-topics命令"><strong>5. 运行describe topics命令</strong></h3>
<p>执行以下命令运行describe查看topic的相关信息:</p>
<pre><code class="language-text">bin/kafka-topics.sh --describe --zookeeper node01:2181 --topic test
</code></pre>
<p>结果说明：</p>
<p>这是输出的解释。第一行给出了所有分区的摘要，每个附加行提供有关一个分区的信息。由于我们只有一个分 区用于此主题，因此只有一行。</p>
<p>“leader”是负责给定分区的所有读取和写入的节点。每个节点将成为随机选择的分区部分的领导者。（因为在kafka中 如果有多个副本的话，就会存在leader和follower的关系，表示当前这个副本为leader所在的broker是哪一个）</p>
<p>“replicas”是复制此分区日志的节点列表，无论它们是否为领导者，或者即使它们当前处于活动状态。（所有副本列表0,1,2）</p>
<p>“isr”是“同步”复制品的集合。这是副本列表的子集，该列表当前处于活跃状态并且已经被领导者捕获。（可用的列表数）</p>
<h3 id="6-增加topic分区数"><strong>6. 增加topic分区数</strong></h3>
<p>执行以下命令可以增加topic分区数:</p>
<pre><code class="language-text">bin/kafka-topics.sh --zookeeper zkhost:port --alter --topic topicName --partitions 8
</code></pre>
<h3 id="7-增加配置"><strong>7. 增加配置</strong></h3>
<p>动态修改kakfa的配置:</p>
<pre><code class="language-text">bin/kafka-topics.sh --zookeeper node01:2181 --alter --topic test --config flush.messages=1
</code></pre>
<h3 id="8-删除配置"><strong>8. 删除配置</strong></h3>
<p>动态删除kafka集群配置:</p>
<pre><code class="language-text">bin/kafka-topics.sh --zookeeper node01:2181 --alter --topic test --delete-config flush.messages
</code></pre>
<h3 id="9-删除topic"><strong>9. 删除topic</strong></h3>
<p>目前删除topic在默认情况下知识打上一个删除的标记，在重新启动kafka后才删除。</p>
<p>如果需要立即删除，则需要在 server.properties中配置：</p>
<p><code>delete.topic.enable=true</code></p>
<p>然后执行以下命令进行删除topic:</p>
<pre><code class="language-text">kafka-topics.sh --zookeeper zkhost:port --delete --topic topicName
</code></pre>
<h2 id="五-kafka的javaapi操作"><strong>五、Kafka的JavaAPI操作</strong></h2>
<h3 id="1-生产者代码"><strong>1. 生产者代码</strong></h3>
<p><strong>使用生产者，生产数据</strong>：</p>
<pre><code class="language-text">/**
* 订单的生产者代码，
*/
public class OrderProducer {
public static void main(String[] args) throws InterruptedException {
/* 1、连接集群，通过配置文件的方式
* 2、发送数据-topic:order，value
*/
Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;node01:9092&quot;); props.put(&quot;acks&quot;, &quot;all&quot;);
props.put(&quot;retries&quot;, 0);
props.put(&quot;batch.size&quot;, 16384);
props.put(&quot;linger.ms&quot;, 1);
props.put(&quot;buffer.memory&quot;, 33554432); 
props.put(&quot;key.serializer&quot;,
&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); 
props.put(&quot;value.serializer&quot;,
&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
 KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;String, String&gt;
(props);
for (int i = 0; i &lt; 1000; i++) {
// 发送数据 ,需要一个producerRecord对象,最少参数 String topic, V value kafkaProducer.send(new ProducerRecord&lt;String, String&gt;(&quot;order&quot;, &quot;订单信
息！&quot;+i));
Thread.sleep(100);
}
}
}
</code></pre>
<p><strong>kafka当中的数据分区</strong>:</p>
<p>kafka生产者发送的消息，都是保存在broker当中，我们可以自定义分区规则，决定消息发送到哪个partition里面去进行保存 查看ProducerRecord这个类的源码，就可以看到kafka的各种不同分区策略</p>
<p>kafka当中支持以下四种数据的分区方式：</p>
<pre><code class="language-text">//第一种分区策略，如果既没有指定分区号，也没有指定数据key，那么就会使用轮询的方式将数据均匀的发送到不同的分区里面去
  //ProducerRecord&lt;String, String&gt; producerRecord1 = new ProducerRecord&lt;&gt;(&quot;mypartition&quot;, &quot;mymessage&quot; + i);
  //kafkaProducer.send(producerRecord1);
  //第二种分区策略 如果没有指定分区号，指定了数据key，通过key.hashCode  % numPartitions来计算数据究竟会保存在哪一个分区里面
  //注意：如果数据key，没有变化   key.hashCode % numPartitions  =  固定值  所有的数据都会写入到某一个分区里面去
  //ProducerRecord&lt;String, String&gt; producerRecord2 = new ProducerRecord&lt;&gt;(&quot;mypartition&quot;, &quot;mykey&quot;, &quot;mymessage&quot; + i);
  //kafkaProducer.send(producerRecord2);
  //第三种分区策略：如果指定了分区号，那么就会将数据直接写入到对应的分区里面去
//  ProducerRecord&lt;String, String&gt; producerRecord3 = new ProducerRecord&lt;&gt;(&quot;mypartition&quot;, 0, &quot;mykey&quot;, &quot;mymessage&quot; + i);
 // kafkaProducer.send(producerRecord3);
  //第四种分区策略：自定义分区策略。如果不自定义分区规则，那么会将数据使用轮询的方式均匀的发送到各个分区里面去
  kafkaProducer.send(new ProducerRecord&lt;String, String&gt;(&quot;mypartition&quot;,&quot;mymessage&quot;+i));
</code></pre>
<p><strong>自定义分区策略</strong>:</p>
<pre><code class="language-text">public class KafkaCustomPartitioner implements Partitioner {
 @Override
 public void configure(Map&lt;String, ?&gt; configs) {
 }

 @Override
 public int partition(String topic, Object arg1, byte[] keyBytes, Object arg3, byte[] arg4, Cluster cluster) {
  List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);
     int partitionNum = partitions.size();
  Random random = new Random();
  int partition = random.nextInt(partitionNum);
     return partition;
 }

 @Override
 public void close() {
  
 }

}
</code></pre>
<p><strong>主代码中添加配置</strong>:</p>
<pre><code class="language-text">@Test
 public void kafkaProducer() throws Exception {
  //1、准备配置文件
     Properties props = new Properties();
     props.put(&quot;bootstrap.servers&quot;, &quot;node01:9092,node02:9092,node03:9092&quot;);
     props.put(&quot;acks&quot;, &quot;all&quot;);
     props.put(&quot;retries&quot;, 0);
     props.put(&quot;batch.size&quot;, 16384);
     props.put(&quot;linger.ms&quot;, 1);
     props.put(&quot;buffer.memory&quot;, 33554432);
     props.put(&quot;partitioner.class&quot;, &quot;cn.itcast.kafka.partitioner.KafkaCustomPartitioner&quot;);
     props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
     props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
     //2、创建KafkaProducer
     KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;String, String&gt;(props);
     for (int i=0;i&lt;100;i++){
         //3、发送数据
         kafkaProducer.send(new ProducerRecord&lt;String, String&gt;(&quot;testpart&quot;,&quot;0&quot;,&quot;value&quot;+i));
     }

  kafkaProducer.close();
 }
</code></pre>
<h3 id="2-消费者代码"><strong>2. 消费者代码</strong></h3>
<p><strong>消费必要条件</strong>:</p>
<p>消费者要从kafka Cluster进行消费数据，必要条件有以下四个:</p>
<ol>
<li>地址：<code>bootstrap.servers=node01:9092</code></li>
<li>序列化：<code>key.serializer=org.apache.kafka.common.serialization.StringSerializer value.serializer=org.apache.kafka.common.serialization.StringSerializer</code></li>
<li>主题（topic）：需要制定具体的某个topic（order）即可。</li>
<li>消费者组：<code>group.id=test</code></li>
</ol>
<h3 id="1-自动提交offset"><strong>1) 自动提交offset</strong></h3>
<p>消费完成之后，自动提交offset：</p>
<pre><code class="language-text">/**
* 消费订单数据--- javaben.tojson
*/
public class OrderConsumer {
public static void main(String[] args) {
// 1\连接集群
Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;hadoop-01:9092&quot;); props.put(&quot;group.id&quot;, &quot;test&quot;);

//以下两行代码 ---消费者自动提交offset值 
props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); 
props.put(&quot;auto.commit.interval.ms&quot;,  &quot;1000&quot;);
props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);
props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);
KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;String, String&gt;
(props);
//   2、发送数据 发送数据需要，订阅下要消费的topic。 order kafkaConsumer.subscribe(Arrays.asList(&quot;order&quot;)); 
while (true) {
ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(100);// jdk queue offer插入、poll获取元素。 blockingqueue put插入原生， take获取元素
for (ConsumerRecord&lt;String, String&gt; record : consumerRecords) { System.out.println(&quot;消费的数据为：&quot; + record.value());
}
}
}
}
</code></pre>
<h3 id="2-手动提交offset"><strong>2) 手动提交offset</strong></h3>
<p>如果Consumer在获取数据后，需要加入处理，数据完毕后才确认offset，需要程序来控制offset的确认。</p>
<p>关闭自动提交确认选项：<code>props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);</code></p>
<p>手动提交offset值：<code>kafkaConsumer.commitSync();</code></p>
<p>完整代码如下：</p>
<pre><code class="language-text">Properties props = new Properties(); 
props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); 
props.put(&quot;group.id&quot;, &quot;test&quot;);
//关闭自动提交确认选项
props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); 
props.put(&quot;key.deserializer&quot;,
&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); 
props.put(&quot;value.deserializer&quot;,
&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); 
KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(&quot;test&quot;));
final int minBatchSize = 200;
List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = new ArrayList&lt;&gt;(); 
while (true) {
ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);
for (ConsumerRecord&lt;String, String&gt; record : records) {
buffer.add(record);
}
if (buffer.size() &gt;= minBatchSize) { 
insertIntoDb(buffer);
// 手动提交offset值
consumer.commitSync(); 
buffer.clear();
}
}
</code></pre>
<h3 id="3-消费完每个分区之后手动提交offset"><strong>3) 消费完每个分区之后手动提交offset</strong></h3>
<p>上面的示例使用commitSync将所有已接收的记录标记为已提交。在某些情况下，可能希望通过明确指定偏移量来更好地控制已提交的记录。在下面的示例中，我们在完成处理每个分区中的记录后提交偏移量:</p>
<pre><code class="language-text">try {
while(running) {
ConsumerRecords&lt;String, String&gt; records = consumer.poll(Long.MAX_VALUE); 
for (TopicPartition partition : records.partitions()) {
List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition);
for (ConsumerRecord&lt;String, String&gt; record : partitionRecords) { System.out.println(record.offset() + &quot;: &quot; + record.value());
}
long lastOffset = partitionRecords.get(partitionRecords.size() -1).offset();
consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1)));
}
}
} finally { consumer.close();}
</code></pre>
<p><strong>注意事项</strong>：</p>
<p>提交的偏移量应始终是应用程序将读取的下一条消息的偏移量。 因此，在调用commitSync（偏移量）时，应该在最后处理的消息的偏移量中添加一个。</p>
<h3 id="4-指定分区数据进行消费"><strong>4) 指定分区数据进行消费</strong></h3>
<ol>
<li>如果进程正在维护与该分区关联的某种本地状态（如本地磁盘上的键值存储），那么它应该只获取它在磁盘上维护的分区的记录。</li>
<li>如果进程本身具有高可用性，并且如果失败则将重新启动（可能使用YARN，Mesos或AWS工具等集群管理框 架，或作为流处理框架的一部分）。在这种情况下，Kafka不需要检测故障并重新分配分区，因为消耗过程将在另一台机器上重新启动。</li>
</ol>
<pre><code class="language-text">Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); props.put(&quot;group.id&quot;, &quot;test&quot;); 
props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);
 props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); 
props.put(&quot;key.deserializer&quot;,
&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); 
props.put(&quot;value.deserializer&quot;,
&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); 
KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);
//consumer.subscribe(Arrays.asList(&quot;foo&quot;,  &quot;bar&quot;));

//手动指定消费指定分区的数据---start 
String topic = &quot;foo&quot;;
TopicPartition partition0 = new TopicPartition(topic, 0); 
TopicPartition partition1 = new TopicPartition(topic, 1); consumer.assign(Arrays.asList(partition0,  partition1));
//手动指定消费指定分区的数据---end
while (true) {
ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); 
for (ConsumerRecord&lt;String, String&gt; record : records)
System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value());
}
</code></pre>
<p><strong>注意事项</strong>：</p>
<ol>
<li>要使用此模式，只需使用要使用的分区的完整列表调用assign（Collection），而不是使用subscribe订阅主题。</li>
<li>主题与分区订阅只能二选一。</li>
</ol>
<h3 id="5-重复消费与数据丢失"><strong>5) 重复消费与数据丢失</strong></h3>
<p>说明：</p>
<ol>
<li>已经消费的数据对于kafka来说，会将消费组里面的oﬀset值进行修改，那什么时候进行修改了？是在数据消费 完成之后，比如在控制台打印完后自动提交；</li>
<li>提交过程：是通过kafka将oﬀset进行移动到下个message所处的oﬀset的位置。</li>
<li>拿到数据后，存储到hbase中或者mysql中，如果hbase或者mysql在这个时候连接不上，就会抛出异常，如果在处理数据的时候已经进行了提交，那么kafka伤的oﬀset值已经进行了修改了，但是hbase或者mysql中没有数据，这个时候就会出现数据丢失。</li>
</ol>
<p>4.什么时候提交oﬀset值？在Consumer将数据处理完成之后，再来进行oﬀset的修改提交。默认情况下oﬀset是 自动提交，需要修改为手动提交oﬀset值。</p>
<ol>
<li>如果在处理代码中正常处理了，但是在提交oﬀset请求的时候，没有连接到kafka或者出现了故障，那么该次修 改oﬀset的请求是失败的，那么下次在进行读取同一个分区中的数据时，会从已经处理掉的oﬀset值再进行处理一 次，那么在hbase中或者mysql中就会产生两条一样的数据，也就是数据重复。</li>
</ol>
<h3 id="6-consumer消费者消费数据流程"><strong>6) consumer消费者消费数据流程</strong></h3>
<p><strong>流程描述</strong>：</p>
<p>Consumer连接指定的Topic partition所在leader broker，采用pull方式从kafkalogs中获取消息。对于不同的消费模式，会将offset保存在不同的地方 官网关于high level API 以及low level API的简介： <a href="http://kafka.apache.org/0100/documentation.html#impl_consumer">http://kafka.apache.org/0100/documentation.html#impl_consumer</a></p>
<p><strong>高阶API（High Level API）</strong>：</p>
<p>kafka消费者高阶API简单；隐藏Consumer与Broker细节；相关信息保存在zookeeper中：</p>
<pre><code class="language-text">/* create a connection to the cluster */
ConsumerConnector connector = Consumer.create(consumerConfig);

interface ConsumerConnector {

/**
This method is used to get a list of KafkaStreams, which are iterators over
MessageAndMetadata objects from which you can obtain messages and their
associated metadata (currently only topic).
Input: a map of &lt;topic, #streams&gt;
Output: a map of &lt;topic, list of message streams&gt;
*/
public Map&lt;String,List&lt;KafkaStream&gt;&gt; createMessageStreams(Map&lt;String,Int&gt; topicCountMap);
/**
You can also obtain a list of KafkaStreams, that iterate over messages
from topics that match a TopicFilter. (A TopicFilter encapsulates a
whitelist or a blacklist which is a standard Java regex.)
*/
public List&lt;KafkaStream&gt; createMessageStreamsByFilter( TopicFilter topicFilter, int numStreams);
/* Commit the offsets of all messages consumed so far. */ public commitOffsets()
/* Shut down the connector */ public shutdown()
}
</code></pre>
<p><strong>说明</strong>：大部分的操作都已经封装好了，比如：当前消费到哪个位置下了，但是不够灵活（工作过程推荐使用）</p>
<p><strong>低级API(Low Level API)</strong>:</p>
<p>kafka消费者低级API非常灵活；需要自己负责维护连接Controller Broker。保存offset，Consumer Partition对应关系：</p>
<pre><code class="language-text">class SimpleConsumer {

/* Send fetch request to a broker and get back a set of messages. */ 
public ByteBufferMessageSet fetch(FetchRequest request);

/* Send a list of fetch requests to a broker and get back a response set. */ public MultiFetchResponse multifetch(List&lt;FetchRequest&gt; fetches);

/**

Get a list of valid offsets (up to maxSize) before the given time.
The result is a list of offsets, in descending order.
@param time: time in millisecs,
if set to OffsetRequest$.MODULE$.LATEST_TIME(), get from the latest
offset available. if set to OffsetRequest$.MODULE$.EARLIEST_TIME(), get from the earliest

available. public long[] getOffsetsBefore(String topic, int partition, long time, int maxNumOffsets);

* offset
*/
</code></pre>
<p><strong>说明</strong>：没有进行包装，所有的操作有用户决定，如自己的保存某一个分区下的记录，你当前消费到哪个位置。</p>
<h3 id="3-kafka-streams-api开发"><strong>3. kafka Streams API开发</strong></h3>
<p><strong>需求</strong>：使用StreamAPI获取test这个topic当中的数据，然后将数据全部转为大写，写入到test2这个topic当中去。</p>
<p><strong>第一步</strong>：创建一个topic</p>
<p>node01服务器使用以下命令来常见一个 topic 名称为test2：</p>
<pre><code class="language-text">bin/kafka-topics.sh --create  --partitions 3 --replication-factor 2 --topic test2 --zookeeper node01:2181,node02:2181,node03:2181
</code></pre>
<p><strong>第二步</strong>：开发StreamAPI</p>
<pre><code class="language-text">public class StreamAPI {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;wordcount-application&quot;);
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;node01:9092&quot;);
        props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        KStreamBuilder builder = new KStreamBuilder();
        builder.stream(&quot;test&quot;).mapValues(line -&gt; line.toString().toUpperCase()).to(&quot;test2&quot;);
        KafkaStreams streams = new KafkaStreams(builder, props);
        streams.start();
    }
}
</code></pre>
<p>执行上述代码，监听获取 <strong>test</strong> 中的数据，然后转成大写，将结果写入 <strong>test2</strong>。</p>
<p><strong>第三步</strong>：生产数据</p>
<p>node01执行以下命令，向test这个topic当中生产数据:</p>
<pre><code class="language-text">bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test
</code></pre>
<p><strong>第四步</strong>：消费数据</p>
<p>node02执行一下命令消费test2这个topic当中的数据:</p>
<pre><code class="language-text">bin/kafka-console-consumer.sh --from-beginning  --topic test2 --zookeeper node01:2181,node02:2181,node03:2181
</code></pre>
<h2 id="六-kafka中的数据不丢失机制"><strong>六、Kafka中的数据不丢失机制</strong></h2>
<h3 id="1-生产者生产数据不丢失"><strong>1. 生产者生产数据不丢失</strong></h3>
<h3 id="发送消息方式"><strong>发送消息方式</strong></h3>
<p>生产者发送给kafka数据，可以采用<strong>同步方式</strong>或<strong>异步方式</strong></p>
<p><strong>同步方式</strong>：</p>
<p>发送一批数据给kafka后，等待kafka返回结果：</p>
<ol>
<li>生产者等待10s，如果broker没有给出ack响应，就认为失败。</li>
<li>生产者重试3次，如果还没有响应，就报错.</li>
</ol>
<p><strong>异步方式</strong>：</p>
<p>发送一批数据给kafka，只是提供一个回调函数：</p>
<ol>
<li>先将数据保存在生产者端的buffer中。buffer大小是2万条 。</li>
<li>满足数据阈值或者数量阈值其中的一个条件就可以发送数据。</li>
<li>发送一批数据的大小是500条。</li>
</ol>
<blockquote>
<p>注：如果broker迟迟不给ack，而buffer又满了，开发者可以设置是否直接清空buffer中的数据。</p>
</blockquote>
<h3 id="ack机制确认机制"><strong>ack机制（确认机制）</strong></h3>
<p>生产者数据发送出去，需要服务端返回一个确认码，即ack响应码；ack的响应有三个状态值0,1，-1</p>
<p>0：生产者只负责发送数据，不关心数据是否丢失，丢失的数据，需要再次发送</p>
<p>1：partition的leader收到数据，不管follow是否同步完数据，响应的状态码为1</p>
<p>-1：所有的从节点都收到数据，响应的状态码为-1</p>
<blockquote>
<p>如果broker端一直不返回ack状态，producer永远不知道是否成功；producer可以设置一个超时时间10s，超过时间认为失败。</p>
</blockquote>
<h3 id="2-broker中数据不丢失"><strong>2. broker中数据不丢失</strong></h3>
<p>在broker中，保证数据不丢失主要是通过副本因子（冗余），防止数据丢失。</p>
<h3 id="3-消费者消费数据不丢失"><strong>3. 消费者消费数据不丢失</strong></h3>
<p>在消费者消费数据的时候，只要每个消费者记录好offset值即可，就能保证数据不丢失。也就是需要我们自己维护偏移量(offset)，可保存在 Redis 中。</p>
<h3 id="文章首发于公众号五分钟学大数据深度钻研大数据技术"><strong>文章首发于公众号：五分钟学大数据，深度钻研大数据技术</strong></h3>
<h2 id="七-kafka配置文件说明"><strong>七、Kafka配置文件说明</strong></h2>
<p><strong>Server.properties配置文件说明</strong>：</p>
<pre><code class="language-text">#broker的全局唯一编号，不能重复
broker.id=0

#用来监听链接的端口，producer或consumer将在此端口建立连接
port=9092

#处理网络请求的线程数量
num.network.threads=3

#用来处理磁盘IO的线程数量
num.io.threads=8

#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400

#接受套接字的缓冲区大小
socket.receive.buffer.bytes=102400

#请求套接字的缓冲区大小
socket.request.max.bytes=104857600

#kafka运行日志存放的路径
log.dirs=/export/data/kafka/

#topic在当前broker上的分片个数
num.partitions=2

#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1

#segment文件保留的最长时间，超时将被删除
log.retention.hours=168

#滚动生成新的segment文件的最大时间
log.roll.hours=1

#日志文件中每个segment的大小，默认为1G
log.segment.bytes=1073741824

#周期性检查文件大小的时间
log.retention.check.interval.ms=300000

#日志清理是否打开
log.cleaner.enable=true

#broker需要使用zookeeper保存meta数据
zookeeper.connect=zk01:2181,zk02:2181,zk03:2181

#zookeeper链接超时时间
zookeeper.connection.timeout.ms=6000

#partion buffer中，消息的条数达到阈值，将触发flush到磁盘
log.flush.interval.messages=10000

#消息buffer的时间，达到阈值，将触发flush到磁盘
log.flush.interval.ms=3000

#删除topic需要server.properties中设置delete.topic.enable=true否则只是标记删除
delete.topic.enable=true

#此处的host.name为本机IP(重要),如果不改,则客户端会抛出:Producer connection to localhost:9092 unsuccessful 错误!
host.name=kafka01

advertised.host.name=192.168.140.128

producer生产者配置文件说明
#指定kafka节点列表，用于获取metadata，不必全部指定
metadata.broker.list=node01:9092,node02:9092,node03:9092
# 指定分区处理类。默认kafka.producer.DefaultPartitioner，表通过key哈希到对应分区
#partitioner.class=kafka.producer.DefaultPartitioner
# 是否压缩，默认0表示不压缩，1表示用gzip压缩，2表示用snappy压缩。压缩后消息中会有头来指明消息压缩类型，故在消费者端消息解压是透明的无需指定。
compression.codec=none
# 指定序列化处理类
serializer.class=kafka.serializer.DefaultEncoder
# 如果要压缩消息，这里指定哪些topic要压缩消息，默认empty，表示不压缩。
#compressed.topics=

# 设置发送数据是否需要服务端的反馈,有三个值0,1,-1
# 0: producer不会等待broker发送ack 
# 1: 当leader接收到消息之后发送ack 
# -1: 当所有的follower都同步消息成功后发送ack. 
request.required.acks=0 

# 在向producer发送ack之前,broker允许等待的最大时间 ，如果超时,broker将会向producer发送一个error ACK.意味着上一次消息因为某种原因未能成功(比如follower未能同步成功) 
request.timeout.ms=10000

# 同步还是异步发送消息，默认“sync”表同步，&quot;async&quot;表异步。异步可以提高发送吞吐量,
也意味着消息将会在本地buffer中,并适时批量发送，但是也可能导致丢失未发送过去的消息
producer.type=sync

# 在async模式下,当message被缓存的时间超过此值后,将会批量发送给broker,默认为5000ms
# 此值和batch.num.messages协同工作.
queue.buffering.max.ms = 5000

# 在async模式下,producer端允许buffer的最大消息量
# 无论如何,producer都无法尽快的将消息发送给broker,从而导致消息在producer端大量沉积
# 此时,如果消息的条数达到阀值,将会导致producer端阻塞或者消息被抛弃，默认为10000
queue.buffering.max.messages=20000

# 如果是异步，指定每次批量发送数据量，默认为200
batch.num.messages=500

# 当消息在producer端沉积的条数达到&quot;queue.buffering.max.meesages&quot;后 
# 阻塞一定时间后,队列仍然没有enqueue(producer仍然没有发送出任何消息) 
# 此时producer可以继续阻塞或者将消息抛弃,此timeout值用于控制&quot;阻塞&quot;的时间 
# -1: 无阻塞超时限制,消息不会被抛弃 
# 0:立即清空队列,消息被抛弃 
queue.enqueue.timeout.ms=-1


# 当producer接收到error ACK,或者没有接收到ACK时,允许消息重发的次数 
# 因为broker并没有完整的机制来避免消息重复,所以当网络异常时(比如ACK丢失) 
# 有可能导致broker接收到重复的消息,默认值为3.
message.send.max.retries=3

# producer刷新topic metada的时间间隔,producer需要知道partition leader的位置,以及当前topic的情况 
# 因此producer需要一个机制来获取最新的metadata,当producer遇到特定错误时,将会立即刷新 
# (比如topic失效,partition丢失,leader失效等),此外也可以通过此参数来配置额外的刷新机制，默认值600000 
topic.metadata.refresh.interval.ms=60000
</code></pre>
<p><strong>consumer消费者配置详细说明</strong>:</p>
<pre><code class="language-text"># zookeeper连接服务器地址
zookeeper.connect=zk01:2181,zk02:2181,zk03:2181
# zookeeper的session过期时间，默认5000ms，用于检测消费者是否挂掉
zookeeper.session.timeout.ms=5000
#当消费者挂掉，其他消费者要等该指定时间才能检查到并且触发重新负载均衡
zookeeper.connection.timeout.ms=10000
# 指定多久消费者更新offset到zookeeper中。注意offset更新时基于time而不是每次获得的消息。一旦在更新zookeeper发生异常并重启，将可能拿到已拿到过的消息
zookeeper.sync.time.ms=2000
#指定消费 
group.id=itcast
# 当consumer消费一定量的消息之后,将会自动向zookeeper提交offset信息 
# 注意offset信息并不是每消费一次消息就向zk提交一次,而是现在本地保存(内存),并定期提交,默认为true
auto.commit.enable=true
# 自动更新时间。默认60 * 1000
auto.commit.interval.ms=1000
# 当前consumer的标识,可以设定,也可以有系统生成,主要用来跟踪消息消费情况,便于观察
conusmer.id=xxx 
# 消费者客户端编号，用于区分不同客户端，默认客户端程序自动产生
client.id=xxxx
# 最大取多少块缓存到消费者(默认10)
queued.max.message.chunks=50
# 当有新的consumer加入到group时,将会reblance,此后将会有partitions的消费端迁移到新  的consumer上,如果一个consumer获得了某个partition的消费权限,那么它将会向zk注册 &quot;Partition Owner registry&quot;节点信息,但是有可能此时旧的consumer尚没有释放此节点, 此值用于控制,注册节点的重试次数. 
rebalance.max.retries=5

# 获取消息的最大尺寸,broker不会像consumer输出大于此值的消息chunk 每次feth将得到多条消息,此值为总大小,提升此值,将会消耗更多的consumer端内存
fetch.min.bytes=6553600

# 当消息的尺寸不足时,server阻塞的时间,如果超时,消息将立即发送给consumer
fetch.wait.max.ms=5000
socket.receive.buffer.bytes=655360
# 如果zookeeper没有offset值或offset值超出范围。那么就给个初始的offset。有smallest、largest、anything可选，分别表示给当前最小的offset、当前最大的offset、抛异常。默认largest
auto.offset.reset=smallest
# 指定序列化处理类
derializer.class=kafka.serializer.DefaultDecoder
</code></pre>
<h2 id="八-cap理论"><strong>八、CAP理论</strong></h2>
<h3 id="1-分布式系统当中的cap理论"><strong>1. 分布式系统当中的CAP理论</strong></h3>
<p>分布式系统（distributed system）正变得越来越重要，大型网站几乎都是分布式的。</p>
<p>分布式系统的最大难点，就是各个节点的状态如何同步。</p>
<p>为了解决各个节点之间的状态同步问题，在1998年，由加州大学的计算机科学家 Eric Brewer 提出分布式系统的三个指标，分别是:</p>
<ul>
<li><strong>Consistency：一致性</strong></li>
<li><strong>Availability：可用性</strong></li>
<li><strong>Partition tolerance：分区容错性</strong></li>
</ul>
<p>Eric Brewer 说，这三个指标不可能同时做到。最多只能同时满足其中两个条件，这个结论就叫做 CAP 定理。</p>
<p><strong>CAP理论是指：分布式系统中，一致性、可用性和分区容忍性最多只能同时满足两个</strong>。</p>
<p>一致性：Consistency</p>
<ul>
<li>通过某个节点的写操作结果对后面通过其它节点的读操作可见</li>
<li>如果更新数据后，并发访问情况下后续读操作可立即感知该更新，称为强一致性</li>
<li>如果允许之后部分或者全部感知不到该更新，称为弱一致性</li>
<li>若在之后的一段时间（通常该时间不固定）后，一定可以感知到该更新，称为最终一致性</li>
</ul>
<p>可用性：Availability</p>
<ul>
<li>任何一个没有发生故障的节点必须在有限的时间内返回合理的结果</li>
</ul>
<p>分区容错性：Partition tolerance</p>
<ul>
<li>部分节点宕机或者无法与其它节点通信时，各分区间还可保持分布式系统的功能</li>
</ul>
<p>一般而言，都要求保证分区容忍性。所以在CAP理论下，更多的是需要在可用性和一致性之间做权衡。</p>
<figure data-type="image" tabindex="20"><img src="https://pic4.zhimg.com/v2-58c7360fcb6eaa5ce5d34603dbd9e91b_b.jpg" alt="" loading="lazy"></figure>
<h3 id="2-partition-tolerance"><strong>2. Partition tolerance</strong></h3>
<p>先看 Partition tolerance，中文叫做&quot;分区容错&quot;。</p>
<p>大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（partition）。分区容错的意思是，区间通信可能失败。比如，一台服务器放在中国，另一台服务器放在美国，这就是两个区，它们之间可能无法通信。</p>
<figure data-type="image" tabindex="21"><img src="https://pic1.zhimg.com/v2-22353cf78ba488f135b43c8a29ce6290_b.jpg" alt="" loading="lazy"></figure>
<p>上图中，G1 和 G2 是两台跨区的服务器。G1 向 G2 发送一条消息，G2 可能无法收到。系统设计的时候，必须考虑到这种情况。</p>
<p>一般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是存在的。即永远可能存在分区容错这个问题</p>
<h3 id="3-consistency"><strong>3. Consistency</strong></h3>
<p>Consistency 中文叫做&quot;一致性&quot;。意思是，写操作之后的读操作，必须返回该值。举例来说，某条记录是 v0，用户向 G1 发起一个写操作，将其改为 v1。</p>
<figure data-type="image" tabindex="22"><img src="https://pic2.zhimg.com/v2-9b2f9993a5d0b3de3c02e1f61fd6d1a9_b.jpg" alt="" loading="lazy"></figure>
<p>接下来，用户的读操作就会得到 v1。这就叫一致性。</p>
<figure data-type="image" tabindex="23"><img src="https://pic3.zhimg.com/v2-0918bfbbb3500e09c9d144f5dbe6f53a_b.jpg" alt="" loading="lazy"></figure>
<p>问题是，用户有可能向 G2 发起读操作，由于 G2 的值没有发生变化，因此返回的是 v0。G1 和 G2 读操作的结果不一致，这就不满足一致性了。</p>
<figure data-type="image" tabindex="24"><img src="https://pic2.zhimg.com/v2-91ce046ad279715356a2140e88a34b01_b.jpg" alt="" loading="lazy"></figure>
<p>为了让 G2 也能变为 v1，就要在 G1 写操作的时候，让 G1 向 G2 发送一条消息，要求 G2 也改成 v1。</p>
<figure data-type="image" tabindex="25"><img src="https://pic3.zhimg.com/v2-8640ff25cc63c90270e94c6953228b16_b.jpg" alt="" loading="lazy"></figure>
<p>这样的话，用户向 G2 发起读操作，也能得到 v1。</p>
<figure data-type="image" tabindex="26"><img src="https://pic3.zhimg.com/v2-c811eb852acbeda9bb58cd47d844d3aa_b.jpg" alt="" loading="lazy"></figure>
<h3 id="4-availability"><strong>4. Availability</strong></h3>
<p>Availability 中文叫做&quot;可用性&quot;，意思是只要收到用户的请求，服务器就必须给出回应。 用户可以选择向 G1 或 G2 发起读操作。不管是哪台服务器，只要收到请求，就必须告诉用户，到底是 v0 还是 v1，否则就不满足可用性。</p>
<h2 id="九-kafka中的cap机制"><strong>九、Kafka中的CAP机制</strong></h2>
<p>kafka是一个分布式的消息队列系统，既然是一个分布式的系统，那么就一定满足CAP定律，那么在kafka当中是如何遵循CAP定律的呢？kafka满足CAP定律当中的哪两个呢？</p>
<p><strong>kafka满足的是CAP定律当中的CA，其中Partition tolerance通过的是一定的机制尽量的保证分区容错性</strong>。</p>
<p><strong>其中C表示的是数据一致性。A表示数据可用性</strong>。</p>
<p>kafka首先将数据写入到不同的分区里面去，每个分区又可能有好多个副本，数据首先写入到leader分区里面去，读写的操作都是与leader分区进行通信，保证了数据的一致性原则，也就是满足了Consistency原则。然后kafka通过分区副本机制，来保证了kafka当中数据的可用性。但是也存在另外一个问题，就是副本分区当中的数据与leader当中的数据存在差别的问题如何解决，这个就是Partition tolerance的问题。</p>
<p><strong>kafka为了解决Partition tolerance的问题，使用了ISR的同步策略，来尽最大可能减少Partition tolerance的问题</strong>。</p>
<p>每个leader会维护一个ISR（a set of in-sync replicas，基本同步）列表。</p>
<p>ISR列表主要的作用就是决定哪些副本分区是可用的，也就是说可以将leader分区里面的数据同步到副本分区里面去，决定一个副本分区是否可用的条件有两个：</p>
<ul>
<li>replica.lag.time.max.ms=10000 副本分区与主分区心跳时间延迟</li>
<li>replica.lag.max.messages=4000 副本分区与主分区消息同步最大差</li>
</ul>
<p>produce 请求被认为完成时的确认值：<code>request.required.acks=0</code>。</p>
<ul>
<li>ack=0：producer不等待broker同步完成的确认，继续发送下一条(批)信息。</li>
<li>ack=1（默认）：producer要等待leader成功收到数据并得到确认，才发送下一条message。</li>
<li>ack=-1：producer得到follwer确认，才发送下一条数据。</li>
</ul>
<h2 id="十-kafka监控及运维"><strong>十、Kafka监控及运维</strong></h2>
<p>在开发工作中，消费在Kafka集群中消息，数据变化是我们关注的问题，当业务前提不复杂时，我们可以使用Kafka 命令提供带有Zookeeper客户端工具的工具，可以轻松完成我们的工作。随着业务的复杂性，增加Group和 Topic，那么我们使用Kafka提供命令工具，已经感到无能为力，那么Kafka监控系统目前尤为重要，我们需要观察 消费者应用的细节。</p>
<h3 id="1-kafka-eagle概述"><strong>1. kafka-eagle概述</strong></h3>
<p>为了简化开发者和服务工程师维护Kafka集群的工作有一个监控管理工具，叫做 Kafka-eagle。这个管理工具可以很容易地发现分布在集群中的哪些topic分布不均匀，或者是分区在整个集群分布不均匀的的情况。它支持管理多个集群、选择副本、副本重新分配以及创建Topic。同时，这个管理工具也是一个非常好的可以快速浏览这个集群的工具，</p>
<h3 id="2-环境和安装"><strong>2. 环境和安装</strong></h3>
<h3 id="1-环境要求"><strong>1. 环境要求</strong></h3>
<p>需要安装jdk，启动zk以及kafka的服务</p>
<h3 id="2-安装步骤"><strong>2. 安装步骤</strong></h3>
<ol>
<li>下载源码包</li>
</ol>
<p>kafka-eagle官网： <a href="http://download.kafka-eagle.org/">http://download.kafka-eagle.org/</a></p>
<p>我们可以从官网上面直接下载最细的安装包即可kafka-eagle-bin-1.3.2.tar.gz这个版本即可</p>
<p>代码托管地址：</p>
<p><a href="https://github.com/smartloli/kafka-eagle/releases">https://github.com/smartloli/kafka-eagle/releases</a></p>
<ol>
<li>解压</li>
</ol>
<p>这里我们选择将kafak-eagle安装在第三台。</p>
<p>直接将kafka-eagle安装包上传到node03服务器的/export/softwares路径下，然后进行解压 node03服务器执行一下命令进行解压。</p>
<ol>
<li>准备数据库</li>
</ol>
<p>kafka-eagle需要使用一个数据库来保存一些元数据信息，我们这里直接使用msyql数据库来保存即可，在node03服务器执行以下命令创建一个mysql数据库即可。</p>
<p>进入mysql客户端:</p>
<ol>
<li>修改kafak-eagle配置文件</li>
</ol>
<p>执行以下命令修改kafak-eagle配置文件:</p>
<pre><code class="language-text">vim system-config.properties
</code></pre>
<p>修改为如下：</p>
<pre><code class="language-text">kafka.eagle.zk.cluster.alias=cluster1,cluster2
cluster1.zk.list=node01:2181,node02:2181,node03:2181
cluster2.zk.list=node01:2181,node02:2181,node03:2181

kafka.eagle.driver=com.mysql.jdbc.Driver
kafka.eagle.url=jdbc:mysql://node03:3306/eagle
kafka.eagle.username=root
kafka.eagle.password=123456
</code></pre>
<ol>
<li>配置环境变量</li>
</ol>
<p>kafka-eagle必须配置环境变量，node03服务器执行以下命令来进行配置环境变量: <code>vim /etc/profile</code>：</p>
<pre><code class="language-text">export KE_HOME=/opt//kafka-eagle-bin-1.3.2/kafka-eagle-web-1.3.2
export PATH=:$KE_HOME/bin:$PATH
</code></pre>
<p>修改立即生效，执行: <code>source /etc/profile</code></p>
<ol>
<li>启动kafka-eagle</li>
</ol>
<p>执行以下界面启动kafka-eagle：</p>
<pre><code class="language-text">cd kafka-eagle-web-1.3.2/bin
chmod u+x ke.sh
./ke.sh start
</code></pre>
<ol>
<li>主界面</li>
</ol>
<p>访问kafka-eagle</p>
<p><a href="http://node03:8048/ke/account/signin?/ke/">http://node03:8048/ke/account/signin?/ke/</a></p>
<p>用户名：admin</p>
<p>密码：123456</p>
<figure data-type="image" tabindex="27"><img src="https://pic2.zhimg.com/v2-a4e945e1cb7f48f7058205f8f30f767d_b.jpg" alt="" loading="lazy"></figure>
<h2 id="十一-kafka大厂面试题"><strong>十一、Kafka大厂面试题</strong></h2>
<h3 id="1-为什么要使用-kafka"><strong>1. 为什么要使用 kafka？</strong></h3>
<ol>
<li>缓冲和削峰：上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器来保证冗余，kafka在中间可以起到一个缓冲的作用，把消息暂存在kafka中，下游服务就可以按照自己的节奏进行慢慢处理。</li>
<li>解耦和扩展性：项目开始的时候，并不能确定具体需求。消息队列可以作为一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获取扩展能力。</li>
<li>冗余：可以采用一对多的方式，一个生产者发布消息，可以被多个订阅topic的服务消费到，供多个毫无关联的业务使用。</li>
<li>健壮性：消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会影响主要业务的正常进行。</li>
<li>异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</li>
</ol>
<h3 id="2-kafka消费过的消息如何再消费"><strong>2. Kafka消费过的消息如何再消费？</strong></h3>
<p>kafka消费消息的offset是定义在zookeeper中的， 如果想重复消费kafka的消息，可以在redis中自己记录offset的checkpoint点（n个），当想重复消费消息时，通过读取redis中的checkpoint点进行zookeeper的offset重设，这样就可以达到重复消费消息的目的了</p>
<h3 id="3-kafka的数据是放在磁盘上还是内存上为什么速度会快"><strong>3. kafka的数据是放在磁盘上还是内存上，为什么速度会快？</strong></h3>
<p>kafka使用的是磁盘存储。</p>
<p>速度快是因为：</p>
<ol>
<li>顺序写入：因为硬盘是机械结构，每次读写都会寻址-&gt;写入，其中寻址是一个“机械动作”，它是耗时的。所以硬盘 “讨厌”随机I/O， 喜欢顺序I/O。为了提高读写硬盘的速度，Kafka就是使用顺序I/O。</li>
<li>Memory Mapped Files（内存映射文件）：64位操作系统中一般可以表示20G的数据文件，它的工作原理是直接利用操作系统的Page来实现文件到物理内存的直接映射。完成映射之后你对物理内存的操作会被同步到硬盘上。</li>
<li>Kafka高效文件存储设计： Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。通过索引信息可以快速定位 message和确定response的 大 小。通过index元数据全部映射到memory（内存映射文件）， 可以避免segment file的IO磁盘操作。通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。</li>
</ol>
<p><strong>注</strong>：</p>
<ol>
<li>Kafka解决查询效率的手段之一是将数据文件分段，比如有100条Message，它们的offset是从0到99。假设将数据文件分成5段，第一段为0-19，第二段为20-39，以此类推，每段放在一个单独的数据文件里面，数据文件以该段中 小的offset命名。这样在查找指定offset的 Message的时候，用二分查找就可以定位到该Message在哪个段中。</li>
<li>为数据文件建 索引数据文件分段 使得可以在一个较小的数据文件中查找对应offset的Message 了，但是这依然需要顺序扫描才能找到对应offset的Message。 为了进一步提高查找的效率，Kafka为每个分段后的数据文件建立了索引文件，文件名与数据文件的名字是一样的，只是文件扩展名为.index。</li>
</ol>
<h3 id="4-kafka数据怎么保障不丢失"><strong>4. Kafka数据怎么保障不丢失？</strong></h3>
<p>分三个点说，一个是生产者端，一个消费者端，一个broker端。</p>
<ol>
<li><strong>生产者数据的不丢失</strong></li>
</ol>
<p>kafka的ack机制：在kafka发送数据的时候，每次发送消息都会有一个确认反馈机制，确保消息正常的能够被收到，其中状态有0，1，-1。</p>
<p>如果是同步模式：<br>
ack设置为0，风险很大，一般不建议设置为0。即使设置为1，也会随着leader宕机丢失数据。所以如果要严格保证生产端数据不丢失，可设置为-1。</p>
<p>如果是异步模式：<br>
也会考虑ack的状态，除此之外，异步模式下的有个buffer，通过buffer来进行控制数据的发送，有两个值来进行控制，时间阈值与消息的数量阈值，如果buffer满了数据还没有发送出去，有个选项是配置是否立即清空buffer。可以设置为-1，永久阻塞，也就数据不再生产。异步模式下，即使设置为-1。也可能因为程序员的不科学操作，操作数据丢失，比如kill -9，但这是特别的例外情况。</p>
<blockquote>
<p>注：<br>
ack=0：producer不等待broker同步完成的确认，继续发送下一条(批)信息。<br>
ack=1（默认）：producer要等待leader成功收到数据并得到确认，才发送下一条message。<br>
ack=-1：producer得到follwer确认，才发送下一条数据。</p>
</blockquote>
<ol>
<li><strong>消费者数据的不丢失</strong></li>
</ol>
<p>通过offset commit 来保证数据的不丢失，kafka自己记录了每次消费的offset数值，下次继续消费的时候，会接着上次的offset进行消费。</p>
<p>而offset的信息在kafka0.8版本之前保存在zookeeper中，在0.8版本之后保存到topic中，即使消费者在运行过程中挂掉了，再次启动的时候会找到offset的值，找到之前消费消息的位置，接着消费，由于 offset 的信息写入的时候并不是每条消息消费完成后都写入的，所以这种情况有可能会造成重复消费，但是不会丢失消息。</p>
<p>唯一例外的情况是，我们在程序中给原本做不同功能的两个consumer组设置 KafkaSpoutConfig.bulider.setGroupid的时候设置成了一样的groupid，这种情况会导致这两个组共享同一份数据，就会产生组A消费partition1，partition2中的消息，组B消费partition3的消息，这样每个组消费的消息都会丢失，都是不完整的。 为了保证每个组都独享一份消息数据，groupid一定不要重复才行。</p>
<ol>
<li><strong>kafka集群中的broker的数据不丢失</strong></li>
</ol>
<p>每个broker中的partition我们一般都会设置有replication（副本）的个数，生产者写入的时候首先根据分发策略（有partition按partition，有key按key，都没有轮询）写入到leader中，follower（副本）再跟leader同步数据，这样有了备份，也可以保证消息数据的不丢失。</p>
<h3 id="5-采集数据为什么选择kafka"><strong>5. 采集数据为什么选择kafka？</strong></h3>
<p>采集层 主要可以使用Flume, Kafka等技术。</p>
<p>Flume：Flume 是管道流方式，提供了很多的默认实现，让用户通过参数部署，及扩展API.</p>
<p>Kafka：Kafka是一个可持久化的分布式的消息队列。 Kafka 是一个非常通用的系统。你可以有许多生产者和很多的消费者共享多个主题Topics。</p>
<p>相比之下,Flume是一个专用工具被设计为旨在往HDFS，HBase发送数据。它对HDFS有特殊的优化，并且集成了Hadoop的安全特性。</p>
<p>所以，Cloudera 建议如果数据被多个系统消费的话，使用kafka；如果数据被设计给Hadoop使用，使用Flume。</p>
<h3 id="6-kafka-重启是否会导致数据丢失"><strong>6. kafka 重启是否会导致数据丢失？</strong></h3>
<ol>
<li>kafka是将数据写到磁盘的，一般数据不会丢失。</li>
<li>但是在重启kafka过程中，如果有消费者消费消息，那么kafka如果来不及提交offset，可能会造成数据的不准确（丢失或者重复消费）。</li>
</ol>
<h3 id="7-kafka-宕机了如何解决"><strong>7. kafka 宕机了如何解决？</strong></h3>
<ol>
<li>先考虑业务是否受到影响</li>
</ol>
<p>kafka 宕机了，首先我们考虑的问题应该是所提供的服务是否因为宕机的机器而受到影响，如果服务提供没问题，如果实现做好了集群的容灾机制，那么这块就不用担心了。</p>
<ol>
<li>节点排错与恢复</li>
</ol>
<p>想要恢复集群的节点，主要的步骤就是通过日志分析来查看节点宕机的原因，从而解决，重新恢复节点。</p>
<h3 id="8-为什么kafka不支持读写分离"><strong>8. 为什么Kafka不支持读写分离？</strong></h3>
<p>在 Kafka 中，生产者写入消息、消费者读取消息的操作都是与 leader 副本进行交互的，从 而实现的是一种<strong>主写主读</strong>的生产消费模型。 Kafka 并不支持<strong>主写从读</strong>，因为主写从读有 2 个很明显的缺点:</p>
<ol>
<li>数据一致性问题：数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间 窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中 A 数据的值都为 X， 之后将主节点中 A 的值修改为 Y，那么在这个变更通知到从节点之前，应用读取从节点中的 A 数据的值并不为最新的 Y，由此便产生了数据不一致的问题。</li>
<li>延时问题：类似 Redis 这种组件，数据从写入主节点到同步至从节点中的过程需要经历 网络→主节点内存→网络→从节点内存 这几个阶段，整个过程会耗费一定的时间。而在 Kafka 中，主从同步会比 Redis 更加耗时，它需要经历 网络→主节点内存→主节点磁盘→网络→从节 点内存→从节点磁盘 这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。</li>
</ol>
<p>而kafka的<strong>主写主读</strong>的优点就很多了：</p>
<ol>
<li>可以简化代码的实现逻辑，减少出错的可能;</li>
<li>将负载粒度细化均摊，与主写从读相比，不仅负载效能更好，而且对用户可控;</li>
<li>没有延时的影响;</li>
<li>在副本稳定的情况下，不会出现数据不一致的情况。</li>
</ol>
<h3 id="9-kafka数据分区和消费者的关系"><strong>9. kafka数据分区和消费者的关系？</strong></h3>
<p>每个分区只能由同一个消费组内的一个消费者(consumer)来消费，可以由不同的消费组的消费者来消费，同组的消费者则起到并发的效果。</p>
<h3 id="10-kafka的数据offset读取流程"><strong>10. kafka的数据offset读取流程</strong></h3>
<ol>
<li>连接ZK集群，从ZK中拿到对应topic的partition信息和partition的Leader的相关信息</li>
<li>连接到对应Leader对应的broker</li>
<li>consumer将⾃自⼰己保存的offset发送给Leader</li>
<li>Leader根据offset等信息定位到segment（索引⽂文件和⽇日志⽂文件）</li>
<li>根据索引⽂文件中的内容，定位到⽇日志⽂文件中该偏移量量对应的开始位置读取相应⻓长度的数据并返回给consumer</li>
</ol>
<h3 id="11-kafka内部如何保证顺序结合外部组件如何保证消费者的顺序"><strong>11. kafka内部如何保证顺序，结合外部组件如何保证消费者的顺序？</strong></h3>
<p>kafka只能保证partition内是有序的，但是partition间的有序是没办法的。爱奇艺的搜索架构，是从业务上把需要有序的打到同⼀个partition。</p>
<h3 id="12-kafka消息数据积压kafka消费能力不足怎么处理"><strong>12. Kafka消息数据积压，Kafka消费能力不足怎么处理？</strong></h3>
<ol>
<li>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）</li>
<li>如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间&lt;生产速度），使处理的数据小于生产的数据，也会造成数据积压。</li>
</ol>
<h3 id="13-kafka单条日志传输大小"><strong>13. Kafka单条日志传输大小</strong></h3>
<p>kafka对于消息体的大小默认为单条最大值是1M但是在我们应用场景中, 常常会出现一条消息大于1M，如果不对kafka进行配置。则会出现生产者无法将消息推送到kafka或消费者无法去消费kafka里面的数据, 这时我们就要对kafka进行以下配置：server.properties</p>
<pre><code class="language-text">replica.fetch.max.bytes: 1048576  broker可复制的消息的最大字节数, 默认为1M
message.max.bytes: 1000012   kafka 会接收单个消息size的最大限制， 默认为1M左右
</code></pre>
<p><strong>注意：message.max.bytes必须小于等于replica.fetch.max.bytes，否则就会导致replica之间数据同步失败。</strong></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DDD领域驱动设计 学习]]></title>
        <id>https://MouseHappy123.github.io/post/ddd-ling-yu-qu-dong-she-ji-xue-xi/</id>
        <link href="https://MouseHappy123.github.io/post/ddd-ling-yu-qu-dong-she-ji-xue-xi/">
        </link>
        <updated>2023-05-28T11:33:29.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://github.com/hwholiday/learning_tools/tree/master/ddd-project-example">项目分层架构</a><br>
<a href="https://juejin.cn/post/7007382308667785253">文档</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elasticsearch]]></title>
        <id>https://MouseHappy123.github.io/post/elasticsearch/</id>
        <link href="https://MouseHappy123.github.io/post/elasticsearch/">
        </link>
        <updated>2023-05-21T16:27:53.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<blockquote>
<p>只有光头才能变强。</p>
</blockquote>
<blockquote>
<p><strong>文本已收录至我的GitHub精选文章，欢迎Star</strong>：<a href="https://github.com/ZhongFuCheng3y/3y" title="https://github.com/ZhongFuCheng3y/3y">github.com/ZhongFuChen…</a></p>
</blockquote>
<p>不知道大家的公司用Elasticsearch多不多，反正我公司的是有在用的。平时听同事们聊天肯定避免不了不认识的技术栈，例如说：把数据放在引擎，从引擎取出数据等等。</p>
<p>如果对引擎不了解的同学，就压根听不懂他们在说什么（我就是听不懂的一位，扎心了）。引擎一般指的是搜索引擎，现在用得比较多的就是Elasticsearch。</p>
<p>这篇文章主要是对Elasticsearch一个简单的入门，没有高深的知识和使用。至少我想做到的是：以后同事们聊引擎了，至少知道他们在讲什么。</p>
<figure data-type="image" tabindex="1"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa40e86c62~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<h2 id="什么是elasticsearch">什么是Elasticsearch？</h2>
<blockquote>
<p><a href="https://www.elastic.co/products/Elasticsearch" title="https://www.elastic.co/products/Elasticsearch">Elasticsearch</a> is a real-time, distributed storage, search, and analytics engine</p>
</blockquote>
<p>Elasticsearch 是一个<strong>实时</strong>的<strong>分布式存储、搜索、分析</strong>的引擎。</p>
<p>介绍那儿有几个关键字：</p>
<ul>
<li>实时</li>
<li>分布式</li>
<li>搜索</li>
<li>分析</li>
</ul>
<p>于是我们就得知道Elasticsearch是怎么做到实时的，Elasticsearch的架构是怎么样的（分布式）。存储、搜索和分析（得知道Elasticsearch是怎么存储、搜索和分析的）</p>
<blockquote>
<p>这些问题在这篇文章中都会有提及。</p>
<p>我已经写了200多篇原创技术文章了，后续会写<strong>大数据</strong>相关的文章，如果想看我其他文章的同学，不妨关注我吧。<strong>公众号：Java3y</strong></p>
<p>如果觉得我这篇文章还不错，对你有帮助，<strong>不要吝啬自己的赞</strong>！</p>
</blockquote>
<h2 id="为什么要用elasticsearch">为什么要用Elasticsearch</h2>
<p>在学习一项技术之前，必须先要了解为什么要使用这项技术。所以，为什么要使用Elasticsearch呢？我们在日常开发中，<strong>数据库</strong>也能做到（实时、存储、搜索、分析）。</p>
<p>相对于数据库，Elasticsearch的强大之处就是可以<strong>模糊查询</strong>。</p>
<p>有的同学可能就会说：我数据库怎么就不能模糊查询了？？我反手就给你写一个SQL：</p>
<pre><code class="language-sql">select * from user where name like '%公众号Java3y%'
</code></pre>
<p>这不就可以把<strong>公众号Java3y</strong>相关的内容搜索出来了吗？</p>
<p>的确，这样做的确可以。但是要明白的是：<code>name like %Java3y%</code>这类的查询是不走<strong>索引</strong>的，不走索引意味着：只要你的数据库的量很大（1亿条），你的查询肯定会是<strong>秒</strong>级别的</p>
<blockquote>
<p>如果对数据库索引还不是很了解的同学，建议复看一下我以前的文章。我觉得我当时写得还不赖（哈哈哈）</p>
<p><a href="https://github.com/ZhongFuCheng3y/3y" title="https://github.com/ZhongFuCheng3y/3y">GitHub</a>搜关键字：”索引“</p>
</blockquote>
<p>而且，即便给你从数据库根据<strong>模糊匹配</strong>查出相应的记录了，那往往会返回<strong>大量的数据</strong>给你，往往你需要的数据量并没有这么多，可能50条记录就足够了。</p>
<p>还有一个就是：用户输入的内容往往并没有这么的<strong>精确</strong>，比如我从Google输入<code>ElastcSeach</code>（打错字），但是Google还是能估算我想输入的是<code>Elasticsearch</code></p>
<figure data-type="image" tabindex="2"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa423b9824~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>而Elasticsearch是专门做<strong>搜索</strong>的，就是为了解决上面所讲的问题而生的，换句话说：</p>
<ul>
<li>Elasticsearch对模糊搜索非常擅长（搜索速度很快）</li>
<li>从Elasticsearch搜索到的数据可以根据<strong>评分</strong>过滤掉大部分的，只要返回评分高的给用户就好了（原生就支持排序）</li>
<li>没有那么准确的关键字也能搜出相关的结果（能匹配有相关性的记录）</li>
</ul>
<p>下面我们就来学学为什么Elasticsearch可以做到上面的几点。</p>
<h2 id="elasticsearch的数据结构">Elasticsearch的数据结构</h2>
<p>众所周知，你要在查询的时候花得更少的时间，你就需要知道他的底层数据结构是怎么样的；举个例子：</p>
<ul>
<li>树型的查找时间复杂度一般是O(logn)</li>
<li>链表的查找时间复杂度一般是O(n)</li>
<li>哈希表的查找时间复杂度一般是O(1)</li>
<li>....不同的数据结构所花的时间往往不一样，你想要查找的时候要<strong>快</strong>，就需要有底层的数据结构支持</li>
</ul>
<p>从上面说Elasticsearch的模糊查询速度很快，那Elasticsearch的底层数据结构是什么呢？我们来看看。</p>
<p>我们根据“<strong>完整的条件</strong>”查找一条记录叫做<strong>正向索引</strong>；我们一本书的章节目录就是正向索引，通过章节名称就找到对应的页码。</p>
<figure data-type="image" tabindex="3"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa444c030c~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>首先我们得知道为什么Elasticsearch为什么可以实现快速的“<strong>模糊匹配</strong>”/“<strong>相关性查询</strong>”，实际上是你写入数据到Elasticsearch的时候会进行<strong>分词</strong>。</p>
<p>还是以上图为例，上图出现了4次“<strong>算法</strong>”这个词，我们能不能根据这次词为它找他对应的目录？Elasticsearch正是这样干的，如果我们根据上图来做这个事，会得到类似这样的结果：</p>
<ul>
<li>算法 <code>-&gt;</code>2,13,42,56</li>
</ul>
<p>这代表着“算法”这个词肯定是在第二页、第十三页、第四十二页、第五十六页出现过。这种根据<strong>某个词</strong>(不完整的条件)再查找对应记录，叫做<strong>倒排索引</strong>。</p>
<p>再看下面的图，好好体会一下：</p>
<figure data-type="image" tabindex="4"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa4654b054~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>众所周知，世界上有这么多的语言，那Elasticsearch怎么<strong>切分这些词呢？</strong>，Elasticsearch<strong>内置</strong>了一些分词器</p>
<ul>
<li>Standard Analyzer 。按词切分，将词小写</li>
<li>Simple Analyzer。按非字母过滤（符号被过滤掉），将词小写</li>
<li>WhitespaceAnalyzer。按照空格切分，不转小写</li>
<li>....等等等</li>
</ul>
<p>Elasticsearch分词器主要由三部分组成：</p>
<ul>
<li>􏱀􏰉􏰂􏰈􏰂􏰆􏰄Character Filters（文本过滤器，去除HTML）</li>
<li>Tokenizer（按照规则切分，比如空格）</li>
<li>TokenFilter（将切分后的词进行处理，比如转成小写）</li>
</ul>
<p>显然，Elasticsearch是老外写的，内置的分词器都是英文类的，而我们用户搜索的时候往往搜的是中文，现在中文分词器用得最多的就是<strong>IK</strong>。</p>
<p>扯了一大堆，那Elasticsearch的数据结构是怎么样的呢？看下面的图：</p>
<figure data-type="image" tabindex="5"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa475f3bf8~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>我们输入一段文字，Elasticsearch会根据分词器对我们的那段文字进行<strong>分词</strong>（也就是图上所看到的Ada/Allen/Sara..)，这些分词汇总起来我们叫做<code>Term Dictionary</code>，而我们需要通过分词找到对应的记录，这些文档ID保存在<code>PostingList</code></p>
<p>在<code>Term Dictionary</code>中的词由于是非常非常多的，所以我们会为其进行<strong>排序</strong>，等要查找的时候就可以通过<strong>二分</strong>来查，不需要遍历整个<code>Term Dictionary</code></p>
<p>由于<code>Term Dictionary</code>的词实在太多了，不可能把<code>Term Dictionary</code>所有的词都放在内存中，于是Elasticsearch还抽了一层叫做<code>Term Index</code>，这层只存储 部分 <strong>词的前缀</strong>，<code>Term Index</code>会存在内存中（检索会特别快）</p>
<p><code>Term Index</code>在内存中是以<strong>FST</strong>（Finite State Transducers）的形式保存的，其特点是<strong>非常节省内存</strong>。FST有两个优点：</p>
<ul>
<li>1）空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；</li>
<li>2）查询速度快。O(len(str))的查询时间复杂度。</li>
</ul>
<p>前面讲到了<code>Term Index</code>是存储在内存中的，且Elasticsearch用<strong>FST</strong>（Finite State Transducers）的形式保存（节省内存空间）。<code>Term Dictionary</code>在Elasticsearch也是为他进行排序（查找的时候方便），其实<code>PostingList</code>也有对应的优化。</p>
<p><code>PostingList</code>会使用Frame Of Reference（<strong>FOR</strong>）编码技术对里边的数据进行压缩，<strong>节约磁盘空间</strong>。</p>
<figure data-type="image" tabindex="6"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa49da7431~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><code>PostingList</code>里边存的是文档ID，我们查的时候往往需要对这些文档ID做<strong>交集和并集</strong>的操作（比如在多条件查询时)，<code>PostingList</code>使用<strong>Roaring Bitmaps</strong>来对文档ID进行交并集操作。</p>
<p>使用<strong>Roaring Bitmaps</strong>的好处就是可以节省空间和快速得出交并集的结果。</p>
<figure data-type="image" tabindex="7"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa6f348e64~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>所以到这里我们总结一下Elasticsearch的数据结构有什么特点：</p>
<figure data-type="image" tabindex="8"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/20/16fc3105213df09c~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<h2 id="elasticsearch的术语和架构">Elasticsearch的术语和架构</h2>
<p>从官网的介绍我们已经知道Elasticsearch是<strong>分布式</strong>存储的，如果看过我的文章的同学，对<strong>分布式</strong>这个概念应该不陌生了。</p>
<blockquote>
<p>如果对分布式还不是很了解的同学，建议复看一下我以前的文章。我觉得我当时写得还不赖（哈哈哈）</p>
<p><a href="https://github.com/ZhongFuCheng3y/3y" title="https://github.com/ZhongFuCheng3y/3y">GitHub</a>搜关键字：”SpringCloud“,&quot;Zookeeper&quot;,&quot;Kafka&quot;,&quot;单点登录&quot;</p>
</blockquote>
<p>在讲解Elasticsearch的架构之前，首先我们得了解一下Elasticsearch的一些常见术语。</p>
<ul>
<li><strong>Index</strong>：Elasticsearch的Index相当于数据库的Table</li>
<li><strong>Type</strong>：这个在新的Elasticsearch版本已经废除（在以前的Elasticsearch版本，一个Index下支持多个Type--有点类似于消息队列一个topic下多个group的概念）</li>
<li><strong>Document</strong>：Document相当于数据库的一行记录</li>
<li><strong>Field</strong>：相当于数据库的Column的概念</li>
<li><strong>Mapping</strong>：相当于数据库的Schema的概念</li>
<li><strong>DSL</strong>：相当于数据库的SQL（给我们读取Elasticsearch数据的API）</li>
</ul>
<figure data-type="image" tabindex="9"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa741e2cb0~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>相信大家看完上面的对比图，对Elasticsearch的一些术语就不难理解了。那Elasticsearch的架构是怎么样的呢？下面我们来看看：</p>
<p>一个Elasticsearch集群会有多个Elasticsearch节点，所谓节点实际上就是运行着Elasticsearch进程的机器。</p>
<figure data-type="image" tabindex="10"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa754b4d46~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>在众多的节点中，其中会有一个<code>Master Node</code>，它主要负责维护索引元数据、负责切换主分片和副本分片身份等工作（后面会讲到分片的概念），如果主节点挂了，会选举出一个新的主节点。</p>
<figure data-type="image" tabindex="11"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa7734d5c0~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>从上面我们也已经得知，Elasticsearch最外层的是Index（相当于数据库 表的概念）；一个Index的数据我们可以分发到不同的Node上进行存储，这个操作就叫做<strong>分片</strong>。</p>
<p>比如现在我集群里边有4个节点，我现在有一个Index，想将这个Index在4个节点上存储，那我们可以设置为4个分片。这4个分片的数据<strong>合起来</strong>就是Index的数据</p>
<figure data-type="image" tabindex="12"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa7d0995a6~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>为什么要分片？原因也很简单：</p>
<ul>
<li>如果一个Index的数据量太大，只有一个分片，那只会在一个节点上存储，随着数据量的增长，一个节点未必能把一个Index存储下来。</li>
<li>多个分片，在写入或查询的时候就可以并行操作（从各个节点中读写数据，提高吞吐量）</li>
</ul>
<p>现在问题来了，如果某个节点挂了，那部分数据就丢了吗？显然Elasticsearch也会想到这个问题，所以分片会有主分片和副本分片之分（为了实现<strong>高可用</strong>）</p>
<p>数据写入的时候是<strong>写到主分片</strong>，副本分片会<strong>复制</strong>主分片的数据，读取的时候<strong>主分片和副本分片都可以读</strong>。</p>
<blockquote>
<p>Index需要分为多少个分片和副本分片都是可以通过配置设置的</p>
</blockquote>
<figure data-type="image" tabindex="13"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fa92be5184~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>如果某个节点挂了，前面所提高的<code>Master Node</code>就会把对应的副本分片提拔为主分片，这样即便节点挂了，数据就不会丢。</p>
<p>到这里我们可以简单总结一下Elasticsearch的架构了：</p>
<figure data-type="image" tabindex="14"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47faa349b120~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<h2 id="elasticsearch-写入的流程">Elasticsearch 写入的流程</h2>
<p>上面我们已经知道当我们向Elasticsearch写入数据的时候，是写到主分片上的，我们可以了解更多的细节。</p>
<p>客户端写入一条数据，到Elasticsearch集群里边就是由<strong>节点</strong>来处理这次请求：</p>
<figure data-type="image" tabindex="15"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47faa6883442~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>集群上的每个节点都是<code>coordinating node</code>（<strong>协调节点</strong>），协调节点表明这个节点可以做<strong>路由</strong>。比如<strong>节点1</strong>接收到了请求，但发现这个请求的数据应该是由<strong>节点2</strong>处理（因为主分片在<strong>节点2</strong>上），所以会把请求转发到<strong>节点2</strong>上。</p>
<ul>
<li>coodinate（<strong>协调</strong>）节点通过hash算法可以计算出是在哪个主分片上，然后<strong>路由到对应的节点</strong></li>
<li><code>shard = hash(document_id) % (num_of_primary_shards)</code></li>
</ul>
<p>路由到对应的节点以及对应的主分片时，会做以下的事：</p>
<ol>
<li>将数据写到内存缓存区</li>
<li>然后将数据写到translog缓存区</li>
<li>每隔<strong>1s</strong>数据从buffer中refresh到FileSystemCache中，生成segment文件，一旦生成segment文件，就能通过索引查询到了</li>
<li>refresh完，memory buffer就清空了。</li>
<li>每隔<strong>5s</strong>中，translog 从buffer flush到磁盘中</li>
<li>定期/定量从FileSystemCache中,结合translog内容<code>flush index</code>到磁盘中。</li>
</ol>
<figure data-type="image" tabindex="16"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47faa68cd009~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>解释一下：</p>
<ul>
<li>Elasticsearch会把数据先写入内存缓冲区，然后每隔<strong>1s</strong>刷新到文件系统缓存区（当数据被刷新到文件系统缓冲区以后，数据才可以被检索到）。所以：Elasticsearch写入的数据需要<strong>1s</strong>才能查询到</li>
<li>为了防止节点宕机，内存中的数据丢失，Elasticsearch会另写一份数据到<strong>日志文件</strong>上，但最开始的还是写到内存缓冲区，每隔<strong>5s</strong>才会将缓冲区的刷到磁盘中。所以：Elasticsearch某个节点如果挂了，可能会造成有<strong>5s</strong>的数据丢失。</li>
<li>等到磁盘上的translog文件大到一定程度或者超过了30分钟，会触发<strong>commit</strong>操作，将内存中的segement文件异步刷到磁盘中，完成持久化操作。</li>
</ul>
<p>说白了就是：写内存缓冲区（<strong>定时</strong>去生成segement，生成translog），能够<strong>让数据能被索引、被持久化</strong>。最后通过commit完成一次的持久化。</p>
<figure data-type="image" tabindex="17"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47faabc5faee~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>等主分片写完了以后，会将数据并行发送到副本集节点上，等到所有的节点写入成功就返回<strong>ack</strong>给协调节点，协调节点返回<strong>ack</strong>给客户端，完成一次的写入。</p>
<h2 id="elasticsearch更新和删除">Elasticsearch更新和删除</h2>
<p>Elasticsearch的更新和删除操作流程：</p>
<ul>
<li>给对应的<code>doc</code>记录打上<code>.del</code>标识，如果是删除操作就打上<code>delete</code>状态，如果是更新操作就把原来的<code>doc</code>标志为<code>delete</code>，然后重新新写入一条数据</li>
</ul>
<p>前面提到了，每隔<strong>1s</strong>会生成一个segement 文件，那segement文件会越来越多越来越多。Elasticsearch会有一个<strong>merge</strong>任务，会将多个segement文件<strong>合并</strong>成一个segement文件。</p>
<p>在合并的过程中，会把带有<code>delete</code>状态的<code>doc</code>给<strong>物理删除</strong>掉。</p>
<figure data-type="image" tabindex="18"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47faaa8cd3d0~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<h2 id="elasticsearch查询">Elasticsearch查询</h2>
<p>查询我们最简单的方式可以分为两种：</p>
<ul>
<li>根据ID查询doc</li>
<li>根据query（搜索词）去查询匹配的doc</li>
</ul>
<pre><code class="language-auto">public TopDocs search(Query query, int n);
public Document doc(int docID);
</code></pre>
<p>根据<strong>ID</strong>去查询具体的doc的流程是：</p>
<ul>
<li>检索内存的Translog文件</li>
<li>检索硬盘的Translog文件</li>
<li>检索硬盘的Segement文件</li>
</ul>
<p>根据<strong>query</strong>去匹配doc的流程是：</p>
<ul>
<li>同时去查询内存和硬盘的Segement文件</li>
</ul>
<figure data-type="image" tabindex="19"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fac5f982e1~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>从上面所讲的写入流程，我们就可以知道：Get（通过ID去查Doc是实时的），Query（通过query去匹配Doc是近实时的）</p>
<ul>
<li>因为segement文件是每隔一秒才生成一次的</li>
</ul>
<p>Elasticsearch查询又分可以为三个阶段：</p>
<ul>
<li>
<p>QUERY_AND_FETCH（查询完就返回整个Doc内容）</p>
</li>
<li>
<p>QUERY_THEN_FETCH（先查询出对应的Doc id ，然后再根据Doc id 匹配去对应的文档）</p>
</li>
<li>
<p>DFS_QUERY_THEN_FETCH（先算分，再查询）</p>
<ul>
<li>「这里的分指的是 <strong>词频率和文档的频率</strong>（Term Frequency、Document Frequency）众所周知，出现频率越高，相关性就更强」</li>
</ul>
</li>
</ul>
<figure data-type="image" tabindex="20"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fad7a088a5~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>一般我们用得最多的就是<strong>QUERY_THEN_FETCH</strong>，第一种查询完就返回整个Doc内容（QUERY_AND_FETCH）只适合于只需要查一个分片的请求。</p>
<p><strong>QUERY_THEN_FETCH</strong>总体的流程流程大概是：</p>
<ul>
<li>客户端请求发送到集群的某个节点上。集群上的每个节点都是coordinate node（协调节点）</li>
<li>然后协调节点将搜索的请求转发到<strong>所有分片上</strong>（主分片和副本分片都行）</li>
<li>每个分片将自己搜索出的结果<code>(doc id)</code>返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。</li>
<li>接着由协调节点根据 <code>doc id</code> 去各个节点上<strong>拉取实际</strong>的 <code>document</code> 数据，最终返回给客户端。</li>
</ul>
<p><strong>Query Phase阶段</strong>时节点做的事：</p>
<ul>
<li>协调节点向目标分片发送查询的命令（转发请求到主分片或者副本分片上）</li>
<li>数据节点（在每个分片内做过滤、排序等等操作），返回<code>doc id</code>给协调节点</li>
</ul>
<p><strong>Fetch Phase阶段</strong>时节点做的是：</p>
<ul>
<li>协调节点得到数据节点返回的<code>doc id</code>，对这些<code>doc id</code>做聚合，然后将目标数据分片发送抓取命令（希望拿到整个Doc记录）</li>
<li>数据节点按协调节点发送的<code>doc id</code>，拉取实际需要的数据返回给协调节点</li>
</ul>
<p>主流程我相信大家也不会太难理解，说白了就是：<strong>由于Elasticsearch是分布式的，所以需要从各个节点都拉取对应的数据，然后最终统一合成给客户端</strong></p>
<p>只是Elasticsearch把这些活都干了，我们在使用的时候无感知而已。</p>
<figure data-type="image" tabindex="21"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/14/16fa47fad7612ede~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<h2 id="最后">最后</h2>
<p>这篇文章主要对Elasticsearch简单入了个门，实际使用肯定还会遇到很多坑，但我目前就到这里就结束了。</p>
<figure data-type="image" tabindex="22"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/20/16fc310531f3859f~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>如果文章写得有错误的地方，欢迎<strong>友善</strong>指正交流。等年后还会继续更新大数据相关的入门文章，有兴趣的欢迎关注我的公众号。<strong>觉得这篇文章还行，可以给我一个赞👍</strong></p>
<p>参考资料：</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzI4Njg5MDA5NA==&amp;mid=2247486253&amp;idx=3&amp;sn=e703437100bec5bd377220b8c1c02ecd&amp;chksm=ebd74a2cdca0c33a49519e11397db07a636d82fb6ad183746edc5f13ec714b44d6b678616bfd&amp;token=711412693&amp;lang=zh_CN#rd" title="https://mp.weixin.qq.com/s?__biz=MzI4Njg5MDA5NA==&amp;mid=2247486253&amp;idx=3&amp;sn=e703437100bec5bd377220b8c1c02ecd&amp;chksm=ebd74a2cdca0c33a49519e11397db07a636d82fb6ad183746edc5f13ec714b44d6b678616bfd&amp;token=711412693&amp;lang=zh_CN#rd">聊聊 Elasticsearch 的倒排索引</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzI4Njg5MDA5NA==&amp;mid=2247486147&amp;idx=2&amp;sn=a47d2796ffaef44de54ca4adb91652a7&amp;chksm=ebd74bc2dca0c2d40b25f005710804bdda8b36d405efd70200e0489aa782283d84be46d8e1bc&amp;token=711412693&amp;lang=zh_CN#rd" title="https://mp.weixin.qq.com/s?__biz=MzI4Njg5MDA5NA==&amp;mid=2247486147&amp;idx=2&amp;sn=a47d2796ffaef44de54ca4adb91652a7&amp;chksm=ebd74bc2dca0c2d40b25f005710804bdda8b36d405efd70200e0489aa782283d84be46d8e1bc&amp;token=711412693&amp;lang=zh_CN#rd">为什么需要 Elasticsearch</a></li>
<li><a href="https://www.cnblogs.com/bonelee/p/6226185.html" title="https://www.cnblogs.com/bonelee/p/6226185.html">lucene字典实现原理——FST</a></li>
<li><a href="https://www.cnblogs.com/jajian/p/10465519.html" title="https://www.cnblogs.com/jajian/p/10465519.html">Elasticsearch性能优化</a></li>
<li><a href="https://www.cnblogs.com/stoneFang/p/11254521.html" title="https://www.cnblogs.com/stoneFang/p/11254521.html">深入分析Elastic Search的写入过程</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34674517" title="https://zhuanlan.zhihu.com/p/34674517">Elasticsearch内核解析 - 查询篇</a></li>
</ul>
<p>如果大家想要<strong>实时</strong>关注我更新的文章以及分享的干货的话，可以关注我的公众号「<strong>Java3y</strong>」。</p>
<ul>
<li>🔥<strong>海量视频资源</strong></li>
<li>🔥<strong>Java精美脑图</strong></li>
<li>🔥<strong>Java学习路线</strong></li>
<li>🔥<strong>开发常用工具</strong></li>
<li>🔥<strong>精美整理好的PDF电子书</strong></li>
</ul>
<p>在公众号下回复「<strong>888</strong>」即可获取！！</p>
<figure data-type="image" tabindex="23"><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/20/16fc310538aa04b5~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<blockquote>
<p><strong>本已收录至我的GitHub精选文章，欢迎Star</strong>：<a href="https://github.com/ZhongFuCheng3y/3y" title="https://github.com/ZhongFuCheng3y/3y">github.com/ZhongFuChen…</a></p>
<p><strong>求点赞</strong> <strong>求关注️</strong> <strong>求分享👥</strong> <strong>求留言💬</strong> 对我来说真的 <strong>非常有用</strong>！！！</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[对mysql的一些思考]]></title>
        <id>https://MouseHappy123.github.io/post/dui-mysql-de-yi-xie-si-kao/</id>
        <link href="https://MouseHappy123.github.io/post/dui-mysql-de-yi-xie-si-kao/">
        </link>
        <updated>2023-05-21T14:28:02.000Z</updated>
        <content type="html"><![CDATA[<h1 id="性能对比">性能对比</h1>
<table>
<thead>
<tr>
<th>存储类型</th>
<th>存储服务(例)</th>
<th>优势</th>
<th>劣势</th>
<th>性能概览</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>单机关系数据库</td>
<td>mysql(单机)</td>
<td>结构简单，支持关系查询</td>
<td>写无法扩容</td>
<td>写：700~1000，无法扩展；读：2000左右，可增加从库扩展</td>
<td></td>
</tr>
<tr>
<td>分布式关系数据库</td>
<td>DRDS(集群)</td>
<td>写容量支持水平扩容</td>
<td>无法缩分片，结构复杂，运维难度大</td>
<td>写：单分片700~1000，可增加分片扩展；读：2000左右，可增加从库扩展</td>
<td></td>
</tr>
<tr>
<td>内存kv</td>
<td>Redis</td>
<td>读写性能，支持复杂结构，原子操作</td>
<td>内存成本高，持久化有延迟，丢数据风险高</td>
<td>写：单分片1w，可扩分片；读：单副本1w，可扩副本/分片 ｜</td>
<td></td>
</tr>
<tr>
<td>ssd kv</td>
<td>mint/sndb</td>
<td>相对内存KV，单位容量存储成本低，读性能高</td>
<td>仅支持简单读写操作</td>
<td>无单实例数据，可横向扩展，已实现云原生，直接使用sndb资源计算器计算所需资源。官方建议读写比&gt;3:1</td>
<td>注意热key问题，容量评估：http://www.redis.cn/redis_memory/</td>
</tr>
<tr>
<td>nosql</td>
<td>mongodb</td>
<td>无schema，扩展容易；支持深层字段查询；水平扩展比关系数据库容易</td>
<td>性能</td>
<td>写：可增加分片扩展；读：可增加副本扩展</td>
<td>mongo分片并不好用，也不是无限的</td>
</tr>
</tbody>
</table>
<h2 id="选型原则">选型原则</h2>
<ol>
<li>说明<br>
table存储、nosql、newsql使用较少，一般都是特定需求场景 或 其他选型成本高。</li>
<li>基本原则</li>
</ol>
<ul>
<li>需求满足：有关系查询场景，必须有关系数据库，存在深层字段简单查询，可以考虑nosql，复杂检索建议考虑双写ElasticSearch。在线服务不推荐使用nosql</li>
<li>性能满足：预估业务qps，考虑存储选型是否支持对应读/写扩容</li>
<li>容量满足：容量计算，一般关系存储要预估未来2~3年容量，容量满前需要分库分表(或切换至分布式集群)；KV存储同样需要计算容量，主要考虑是成本，通常存储瓶颈选择ssd kv，qps瓶颈选择内存KV</li>
<li>数据安全：内存kv即使有持久化，也不是实时的，因此，如果有持久化诉求，不建议只考虑内存kv，可以考虑ssd kv or 关系db+内存KV</li>
<li>成本最优：一般会有多种实现方案，最终需要结合成本考虑。有时由于成本过高，我们会选择多种存储服务综合使用，例如短期数据存内存KV，长期数据存ssd KV</li>
</ul>
<ol start="3">
<li>注意事项</li>
</ol>
<ul>
<li>kps：每秒key访问的数量，一般KV存储都会使用kps评估性能。</li>
<li>tps：每秒实务数，关系db有时会关注tps</li>
<li>redis/sndb的核心差异：原子操作、复杂数据结构，有时这些诉求导致无法选择sndb这类ssd kv</li>
<li>数据大小：KV存储，对value大小一般都有要求，redis通常建议不超过100k，sndb通常也是小数据不建议超过2mb(均值更低)，超过2mb的建议走bos+cdn</li>
</ul>
<h2 id="性能排查">性能排查</h2>
<p>影响数据库性能的常见因素有：</p>
<ol>
<li>业务低效SQL，导致数据库慢查询堆积，整体性能降低<br>
a. 慢SQL治理<br>
重点关注扫描行数。大多数情况下慢SQL是由于被访问的表没有合适的索引导致。<br>
b. 快速KILL异常会话恢复性能<br>
如果是当前线上服务响应缓慢，此时可能是因为慢SQL导致数据库会话堆积，一般当活跃线程数量超过30个就可能导致性能问题。</li>
<li>读写流量过大，数据库性能容量不足（扩容）<br>
数据库性能容量的重要衡量指标有：<br>
读流量（com_reads/10，com_reads为10s内的读流量总和)<br>
写流量（com_writes/10，com_writes为10s内的写流量总和)<br>
CPU繁忙率（CPU_USED_PERCENT.MATRIX）</li>
</ol>
<ul>
<li>评估标准<br>
不同规格的存储实例能够承载的读写流量上限不同，下表是各套餐单个存储实例能够支撑的流量的对照：</li>
</ul>
<table>
<thead>
<tr>
<th>套餐规格</th>
<th>读QPS上限</th>
<th>写QPS上限</th>
<th>CPU繁忙率上限</th>
</tr>
</thead>
<tbody>
<tr>
<td>磁盘:200G;内存:8G;CPU:15核</td>
<td>300</td>
<td>500</td>
<td>不超过80%</td>
</tr>
<tr>
<td>磁盘:400G;内存:12G;CPU:30核</td>
<td>500</td>
<td>500</td>
<td>不超过80%</td>
</tr>
<tr>
<td>磁盘:800G;内存:24G;CPU:60核</td>
<td>1000</td>
<td>800</td>
<td>不超过80%</td>
</tr>
<tr>
<td>磁盘:1200G;内存:32G;CPU:90核</td>
<td>1500</td>
<td>800</td>
<td>不超过80%</td>
</tr>
<tr>
<td>磁盘:1600G;内存:48G;CPU:120核</td>
<td>2000</td>
<td>1000</td>
<td>不超过80%</td>
</tr>
<tr>
<td>注意：读QPS/写QPS/CPU繁忙率中任何一个指标达到上限，则表明该集群的性能容量即将达到瓶颈。</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>处理方式<br>
线上常见的性能问题主要原因有两个：慢SQL导致性能下降、读写流量过大。对应的处理方式如下：<br>
告警指标	处理方式<br>
读QPS达到上限	扩容从库分摊读流量；<br>
写QPS达到上限	逻辑库拆分到独立集群；如果是分布式集群可设置更多的分片数；(分摊写流量到各分片上，写时基于分区键定位子表，再根据子表hash结果映射到不同的分片，可参考tips3)<br>
CPU繁忙率达到上限	慢查询优化；非正式线上SQL访问线下库</li>
</ul>
<ol>
<li>跨地域访问导致的平响增加<br>
如果业务机器分布在多个地域，那么数据库代理节点、存储节点均需要对应在多个地域进行部署（同一个地域不同机房不区分）。<br>
需要注意的是，各地域读的流量会通过代理节点发往本地域存储节点，写流量都会发往主库，因此写操作存在跨地域访问的情况。 一般华北到华东的网络耗时为20ms左右，华东到华南网络耗时20ms左右。</li>
<li>异常事务导致的锁等待<br>
当一个事务修改大量数据后长时间不提交，则长期对被修改的记录添加行锁（在没有合适索引的情况下，甚至会对整个表加锁），导致其他写事务阻塞。需要确保：<br>
a) 对数据的更新SQL确认能走索引路径<br>
b) 严格控制事务大小<br>
c）确保事务及时提交</li>
</ol>
<h1 id="分布式">分布式</h1>
<h2 id="分库分表">分库分表</h2>
<p>水平分表<br>
一张分布式表(table)会按照指定的 partition key 和 partition mode（一般是hashmode）被水平拆分成多个数据片（称为tablet），分散在多个数据存储节点(tsc)中。分布式表主要解决单机容量问题，并且通过平行写入提升整体的写入能力。开发时不需要再考虑数据如何拆分和数据路由等问题，只需要设定 partition key 即可实现水平分表的能力。</p>
<h3 id="分片方法">分片方法</h3>
<p>基于范围：根据分片键（字段）的范围进行分布存储； 基于哈希：基于分片键哈希值的方式均匀打散到多个子表中</p>
<ul>
<li>提示：建议集群中每个分布式表的分片数量相同。这样可以确保各个表对于同一个key能够存储在同一个分片，可以减少跨分片的事务。</li>
</ul>
<h3 id="分片策略">分片策略</h3>
<p>基于范围的分片填写格式举例： [&quot;[0,150000000]&quot;, &quot;[150000000,300000000]&quot;]将数据按照范围分布在两个分片中。</p>
<p>基于哈希的分片填写格式：输入子表的数量即可。注意，必须为2的指数。例如：32，64，512....2048，建表时先预估表可能达到的行数，建议每个子表存储的行数不超过100万。对于大表一般建议不小于512不超过2048。<br>
提示：建议集群中每个分布式表的分片数量相同。这样可以确保各个表对于同一个key能够存储在同一个分片，可以减少跨分片的事务（DRDS不支持分布式事务）。</p>
<p>hash类型在建表的时候选择的，有outerFirst和innerFirst两种(建议选outerFirst方式，对于后续分片扩容更加便利)。<br>
hash类型决定了MOD在集群的分布是顺序分布还是均匀分布<br>
例如，对于8个分片的Hash分布,数据所在的子表号为分片键MOD(8), 数据均匀分布到8个子表。 假设两个分片 = [xdbtest_0000, xdbtest_0001]<br>
outerFirst:<br>
xdbtest_0000分片子表: t_tablet_0 t_tablet_2 t_tablet_4 t_tablet_6<br>
xdbtest_0001分片子表: t_tablet_1 t_tablet_3 t_tablet_5 t_tablet_7</p>
<p>innerFirst:<br>
xdbtest_0000分片子表: t_tablet_0 t_tablet_1 t_tablet_2 t_tablet_3<br>
xdbtest_0001分片子表: t_tablet_4 t_tablet_5 t_tablet_6 t_tablet_7</p>
<h3 id="tips">tips</h3>
<ol>
<li>分布式表如何选择分片键？<br>
分片键的选择一般遵循下面几个原则</li>
</ol>
<ul>
<li>优先选择贯穿整个业务表的字段。例如所有表都有userid作为主要的查询条件则可以考虑将其作为分片键。</li>
<li>一般会选择区分度高的列，这样才能使数据打散到更多的子表中。例如性别列（只有男女两个选项）就不适合作为分片键。</li>
<li>需要考虑数据的分布规律，例如根据分片键hash分布到各子表后尽量使得每个子表数据量相差不大。</li>
<li>可以将分片键设置为表的主键。</li>
<li>不能使用自增id字段作为分片键。</li>
</ul>
<ol start="2">
<li>
<p>数据库各表能使用不同的字段作为分片键吗？<br>
建议数据库中所有表采用同一个字段作为分片键（例如，每个表都使用userid作为分片字段）。原因有两个：<br>
a存储层面)如果要在一个事务中访问两个表，且两个表的分片键不同，就会导致访问的数据跨分片。而drds目前没有支持跨分片的事务，因此不允许这样访问。<br>
b业务层面)使用不同的字段作为分片键使得业务逻辑混乱，不利于维护。</p>
</li>
<li>
<p>基于hash的存储方式，如何知道某一条数据位于哪个分片，哪个子表？<br>
explain SQL带上分片键进行查询便可列出记录位于哪个子表。再根据hash类型计算子表位于哪个分片中。</p>
</li>
</ol>
<h2 id="并行计算">并行计算</h2>
<p>对于一次查询大量数据的场景，支持把一次大数据量查询分散到多个存储节点上小数据量查询，并对数据结果进行合并聚合返回给用户，这样可以提升大结果集大数据量查询的性能。</p>
<h2 id="读写分离">读写分离</h2>
<p>首先对 query 拆分成多个子句并路由到一个或多个存储节点上，每个存储节点实现透明的读写分离策略，写请求统一到存储节点主库，读请求被分流到多个只读节点和主库上，提升集群的读能力。一个事务内的读请求，不会被读写分离，而是和写请求一样在主库上执行。</p>
<h2 id="负载均衡">负载均衡</h2>
<p>如果一个存储节点存在多个只读节点，那么只读节点间实现负载均衡，避免单个节点或少数节点出现负载过大。</p>
<h2 id="高可用">高可用</h2>
<p>每个存储节点主库都有一个热备节点，当检测到主库服务异常或不可用情况，会启动故障切换流程，保证服务高可用。</p>
<h2 id="可扩展">可扩展</h2>
<p>三种类型的扩展：<br>
存储节点套餐扩展，通过提升存储节点磁盘、CPU、内存，获得单节点更大存储空间和性能。<br>
只读节点扩展，通过增加只读节点，获得整个集群更高的读吞吐能力。<br>
分片扩展，通过对数据拆分和增加分片，使得整个集群写能力和存储容量更高。</p>
<h1 id="状态机">状态机</h1>
<p>状态机可以解决业务上并发可能导致的问题，且有时加锁也不一定能解决的情况状态机也适用<br>
策略：表设计一个status状态字段</p>
<h2 id="case">case</h2>
<ol>
<li>多ip登录<br>
假设一个账号在两个IP上登录了，同时修改昵称</li>
</ol>
<p>在用户表中多设计一个状态字段，0代表正常状态，1代表审核状态，当用户的信息发生变化后，对应的用户记录都会被改成「审核中状态」，而执行语句时只允许修改正常状态的用户记录，伪<code>SQL</code>如下：</p>
<pre><code class="language-sql">-- 之前的SQL语句
update zz_user set user_name = &quot;竹子爱熊猫&quot;, ... where user_id = 888;

-- 优化后的SQL语句
update 
    zz_user 
set 
    user_name = &quot;竹子爱熊猫&quot;, status = 1, ... 
where 
    user_id = 888 and status = 0;
</code></pre>
<p>通过这样的手段，在第一个IP修改成功之后，第二个IP就无法满足SQL语句的执行条件，最终就无法真正修改用户数据。</p>
<ol start="2">
<li>一个<code>IP</code>删除订单，一个<code>IP</code>结算订单，两个操作同一时刻内进行<br>
此时加锁不能解决，因为加锁只能让并行操作串行化，但最终两个业务操作总会执行的，这里加锁之后只会出现两种情况：</li>
</ol>
<blockquote>
<p>①删除订单的请求先获取锁，先删掉了订单，结算订单的请求无法执行结算业务（因为订单都没了）。<br>
②结算订单的请求先拿到锁，用户付钱结算了订单之后，删除订单的请求获取到了锁，然后把用户已经付钱的订单删了。</p>
</blockquote>
<p>做法应该是在订单表上面也设计一个<code>status</code>状态字段。</p>
<p>订单表的状态字段，可选状态如下：</p>
<ul>
<li><code>0</code>：待结算（待支付）。</li>
<li><code>1</code>：待发货。</li>
<li><code>2</code>：待收货。</li>
<li><code>3</code>：已签收。</li>
<li><code>.....</code></li>
<li><code>9</code>：已销毁。</li>
</ul>
<p>有了上述这个状态机字段后，再回过头来看「删除订单、结算订单」这两个业务操作，本质上都是执行<code>update</code>操作，删除是将状态改为<code>9</code>，结算是将状态改为<code>1</code>，所以<code>SQL</code>语句只需要新增一个条件即可：</p>
<pre><code class="language-sql">-- 删除订单（只允许删除待结算、已签收的订单）
update zz_order set status = 9,... where status = 0 or status = 3;

-- 结算订单（只允许结算待支付的订单）
update zz_order set status = 1,... where status = 0;
</code></pre>
<p>也就是直接通过状态字段限制其他并发操作，无论「删除订单、结算订单」谁先执行，另一个操作都无法继续执行。</p>
<ol start="3">
<li>状态机字段不需要额外设计的场景<br>
同时下单</li>
</ol>
<pre><code>-- 乐观锁+状态机
update 
    zz_account 
set 
    balance = balance - 100, .......
where
    version = version + 1 and balance = [修改前的余额];

-- 行锁+状态机
update 
    zz_account 
set 
    balance = balance - 100, .......
where
    balance = [修改前的余额]
for update; //这里其实没必要手动加行锁
</code></pre>
<p>也就是多加了一个where条件：balance=[修改前的余额]，还是前面那个举例，假设下单-99这个操作先拿到锁，提现-2这个操作就会阻塞；当下单操作执行完成后，余额会变成100-99=1元；接着提现操作拿到执行权，可是在执行时，会发现余额并不是起初的100元了，因此提现的update语句不能满足条件，最终无法执行。</p>
<h2 id="有了状态机之后就不需要加锁了吗">有了状态机之后，就不需要加锁了吗？</h2>
<p>其实这种情况下，加不加锁就无所谓了，因为<code>MySQL-InnoDB</code>本身有行锁机制，多个事务并发修改同一条数据，都会被串行化执行，因此在后端加锁，只是将请求串行化的工作提前罢了，这反而会影响整体的性能。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[对redis的一些思考]]></title>
        <id>https://MouseHappy123.github.io/post/dui-redis-de-yi-xie-si-kao/</id>
        <link href="https://MouseHappy123.github.io/post/dui-redis-de-yi-xie-si-kao/">
        </link>
        <updated>2023-05-14T15:23:34.000Z</updated>
        <content type="html"><![CDATA[<h1 id="集群">集群</h1>
<h2 id="分布式redis">分布式redis</h2>
<p>分布式Redis是指将多个Redis实例组成一个逻辑集群，共同承担存储数据和处理客户端请求的工作。与单机Redis相比，分布式Redis能够提供更高的存储容量和更好的性能，同时还可以提供更好的可用性和可伸缩性。</p>
<p>分布式Redis通常有两种架构方式：主从复制和Redis集群。</p>
<p>主从复制<br>
主从复制架构包括一个主节点和多个从节点。主节点负责接收客户端请求、处理数据更新和复制数据到从节点。从节点负责向主节点请求数据更新，并将主节点的数据复制到本地，从而保证数据的可靠性和一致性。当主节点故障时，从节点可以自动选举出一个新的主节点来接替原主节点的工作。</p>
<p>Redis集群<br>
Redis集群是一种分布式Redis架构，通过数据分片的方式将数据存储在多个Redis节点中，从而扩展存储容量和处理性能。Redis集群通常包括多个主节点和从节点，每个节点都负责存储一部分数据，并与其他节点协同工作，以实现数据的一致性和可靠性。</p>
<p>Redis集群通常采用哈希槽分配算法，将数据按照哈希值分散到不同的节点上，并通过复制机制保证数据的可靠性和一致性。当某个节点故障时，集群可以自动将故障节点的哈希槽重新分配到其他节点上，从而保证整个集群的可用性和可靠性。</p>
<h2 id="分布式redis集群登录到某个节点上查询为什么能查到所有的数据">分布式redis集群，登录到某个节点上查询，为什么能查到所有的数据</h2>
<p>在 Redis 集群中，所有节点都存储了部分数据，也就是数据被分散存储在不同的节点上。当一个客户端连接到任何一个节点并发送一个命令时，该节点会根据数据的分片规则将该命令转发到适当的节点上进行处理，该节点再将处理结果返回给客户端。</p>
<p>因此，无论连接到集群中的哪个节点，客户端都可以查看到整个集群的数据，因为数据被分散存储在各个节点中，而不是仅存在于特定的节点上。这就是 Redis 集群的数据共享机制。</p>
<h2 id="redis主从同步适用场景">redis主从同步适用场景</h2>
<p>Redis主从同步适用于以下场景：</p>
<p>高可用性：通过配置Redis主从复制，可以实现主节点的热备份。当主节点发生故障或不可用时，从节点可以自动接管成为新的主节点，确保系统的高可用性。</p>
<p>负载均衡：通过将读操作分发到从节点，可以减轻主节点的读取压力，实现读写分离。这样可以提高系统的读取性能和吞吐量，同时保证数据的一致性。</p>
<p>数据备份：从节点可以作为主节点的数据备份，当主节点发生数据丢失或损坏时，可以从从节点进行数据恢复，确保数据的安全性和可靠性。</p>
<p>离线数据分析：从节点可以被用于离线数据分析或报表生成等任务，以避免对主节点造成额外的负载，同时保持主节点的高性能和低延迟。</p>
<p>扩展性：通过添加更多的从节点，可以扩展Redis的读取能力，以适应高并发读取的需求。这样可以通过水平扩展来应对增加的用户请求。</p>
<p>需要注意的是，Redis主从同步并不能提供强一致性，因为数据同步存在一定的延迟。因此，对于需要强一致性的应用场景，如金融交易等，建议使用Redis Cluster或其他更复杂的数据复制方案。另外，Redis主从同步也不能用于数据分片和水平扩展，如果需要横向扩展存储容量，可以考虑使用Redis Cluster或其他分布式存储解决方案。</p>
<h2 id="redis分布式集群的主备服务器适用主从复制吗">redis分布式集群的主备服务器适用主从复制吗</h2>
<p>是的，Redis分布式集群的主备服务器可以使用主从复制来提供高可用性和数据备份。</p>
<p>在Redis分布式集群中，每个槽(slot)可以有一个主节点和多个备份节点。主节点负责处理对应槽的读写操作，而备份节点则复制主节点的数据并提供读取服务。这种主备关系可以使用Redis的主从复制机制来实现。</p>
<p>通过主从复制，主节点将写操作同步到备份节点，确保数据的一致性和可靠性。当主节点发生故障或不可用时，可以通过自动故障转移或手动操作，将备份节点升级为新的主节点，继续提供服务。这样可以保证Redis分布式集群的高可用性和持久性。</p>
<p>需要注意的是，Redis分布式集群的主备服务器之间的主从复制与整个集群的分布式机制是相互独立的。主备服务器之间的主从复制只负责备份和故障转移，而不参与集群的数据分片和负载均衡。整个集群的数据分片和负载均衡由集群中的所有节点共同协作完成。</p>
<h2 id="redis分布式集群的主备服务器适用主从复制吗-2">redis分布式集群的主备服务器适用主从复制吗</h2>
<p>是的，Redis分布式集群的主备服务器可以使用主从复制来提供高可用性和数据备份。</p>
<p>在Redis分布式集群中，每个槽(slot)可以有一个主节点和多个备份节点。主节点负责处理对应槽的读写操作，而备份节点则复制主节点的数据并提供读取服务。这种主备关系可以使用Redis的主从复制机制来实现。</p>
<p>通过主从复制，主节点将写操作同步到备份节点，确保数据的一致性和可靠性。当主节点发生故障或不可用时，可以通过自动故障转移或手动操作，将备份节点升级为新的主节点，继续提供服务。这样可以保证Redis分布式集群的高可用性和持久性。</p>
<p>需要注意的是，Redis分布式集群的主备服务器之间的主从复制与整个集群的分布式机制是相互独立的。主备服务器之间的主从复制只负责备份和故障转移，而不参与集群的数据分片和负载均衡。整个集群的数据分片和负载均衡由集群中的所有节点共同协作完成。</p>
<h1 id="基础">基础</h1>
<h2 id="怎么保证redis多个操作的一致性">怎么保证redis多个操作的一致性</h2>
<p>要保证Redis多个操作的一致性，可以采取以下措施：</p>
<p>事务（Transaction）：Redis提供事务机制，可以将多个命令打包在一个事务里面，一次性提交执行。在事务执行期间，Redis不会处理其他客户端发来的请求，保证了事务的原子性。如果事务执行失败，所有的命令都不会被执行。</p>
<p>Watch机制：Watch机制可以用来保证Redis的原子性。可以监视某个Key，在执行事务前，先执行WATCH命令监视指定的Key，如果该Key被其他客户端修改，则事务将被打断，没有任何操作被执行。Watch机制是一种乐观锁机制。</p>
<p>分布式锁：分布式锁可以用来保证Redis多个操作的一致性。在多个客户端同时操作同一个Key时，可以通过设置分布式锁来保证每次只有一个客户端可以修改该Key。</p>
<p>总之，Redis保证多个操作的一致性，需要使用事务、Watch机制和分布式锁等技术手段。</p>
<h2 id="lua脚本可以做到吗">lua脚本可以做到吗</h2>
<p>是的，可以使用Lua脚本来保证Redis中多个操作的一致性。Redis中可以使用Lua脚本实现事务，即一次性执行多个Redis命令。Lua脚本在Redis中的执行是原子的，这意味着当Lua脚本在执行时，其他客户端无法对Redis数据库进行修改。</p>
<p>使用Lua脚本进行事务操作时，可以使用Redis提供的 EVAL 命令来执行Lua脚本。在Lua脚本中，可以使用 Redis 提供的 MULTI 命令来开始一个事务，使用 EXEC 命令来执行事务，并使用 Redis 提供的 WATCH 命令来监视一个或多个键的变化，从而保证在事务执行过程中，这些键不会被其他客户端修改。</p>
<p>除了实现事务外，Lua脚本还可以用于Redis中的其他场景，例如实现分布式锁等。</p>
<h1 id="redis-大key-热key">redis 大key 热key</h1>
<h2 id="什么算大key热key">什么算大key，热key？</h2>
<h3 id="大key定义">大key定义</h3>
<p>衡量什么是大key，string类型key的value本身很大，或者复合类型比如list的成员数量很多.</p>
<table>
<thead>
<tr>
<th>key类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>string类型</td>
<td>如果超过10M，那肯定是big-key；如果超过 1M，也不算小，可能会出现10ms以上的慢请求，并发到100的时候，甚至平响飙升秒级；如果一个文章的key的大小11KB，当QPS达到1000的时候，单个key的网络流出约是10M/s，当多个文章同时请求的时候，也可能会有10ms以上的慢请求。</td>
</tr>
<tr>
<td>hash，list等复合类型</td>
<td>如果单个value超过10M或者元素（成员）个数超过1W，那就算big-key。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>注：这里复合类型包括常用的list，hash，set，zset</p>
</blockquote>
<h3 id="热key定义">热key定义</h3>
<p>衡量什么是热点key，某个key的QPS较高（相比其他key），并引起CPU、网络流入流出很高，影响同redis实例其他数据的请求。举几个例子，更容易懂一些。</p>
<table>
<thead>
<tr>
<th>key类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>string类型</td>
<td>热门直播间ID，这个key的value不大，但是QPS达到3W/s，造成这个分片的其他key的请求等待时间变长了。</td>
</tr>
<tr>
<td>hash类型</td>
<td>某个地域的所有实例的配置存在一个hash类型key中，每个地域4W个实例（4W个member），获取每个实例配置都要请求这个key，造成这个key的网络流出达到5M/s；又比如重操作HGETALL 取一个1M的key，导致单分片的网络流出达到20M/s。</td>
</tr>
<tr>
<td>list类型</td>
<td>比如任务id列表，队列出现堆积，一旦开始大量消费的时候，LPOP的QPS达到4W，网络流出达到10-20M/s。</td>
</tr>
<tr>
<td>zset类型</td>
<td>音乐播放历史，按照听歌次数排序，上千条记录一直执行zrangebyscore复杂度较高的命令，redis的cpu达到80%。</td>
</tr>
</tbody>
</table>
<h2 id="大key和热key会带来什么问题呢">大key和热key会带来什么问题呢？</h2>
<ul>
<li>耗时变长：热key影响了同实例其他key的访问平响，严重的拉垮整个redis平响，甚至击穿，透传到DB层带来更严重的连接打满等问题。</li>
<li>内存打满：大key导致内存打满，出现OOM，旧的数据意外淘汰，新的数据无法写入，业务请求有损，分布式redis的架构中会导致某一个分片的内存用满，其余分片内存使用率很低。</li>
<li>复制失败：热key网络流入流出过高，导致redis复制断开，并且可能导致复制重连一直失败，主库压力大，慢请求变多；大key的删除会导致阻塞，发生主从切换，请求miss或者超时。</li>
</ul>
<h2 id="如何发现和定位大key和热key">如何发现和定位大key和热key？</h2>
<p>Redis RDB（Redis Database）文件是一种持久化存储机制，用于将 Redis 的数据以二进制格式写入磁盘。RDB 文件包含了 Redis 数据库的快照，可以在需要时用于数据的恢复和持久化。</p>
<p>如果你想对 Redis RDB 文件进行分析，了解其中的数据结构和内容，可以使用以下方法：</p>
<p>rdb-cli 工具：rdb-cli 是一个开源的用于解析和分析 Redis RDB 文件的命令行工具。你可以使用它来导入 RDB 文件并浏览其中的数据。rdb-cli 支持以文本格式输出 RDB 文件的内容，使你可以更好地理解其中的键、值、数据类型和元数据等信息。你可以在 rdb-cli 的 GitHub 仓库中找到详细的使用说明和示例。</p>
<p>Redis Protocol 解析器：RDB 文件采用了 Redis Protocol 的格式来存储数据。因此，你可以编写自己的解析器来读取和解析 RDB 文件。通过解析 Redis Protocol，你可以了解到每个键的数据类型、键名、过期时间等信息。这需要对 Redis Protocol 的格式和规范有一定的了解。</p>
<p>Redis 自带的 rdb.c 源码：如果你对 Redis 的源码比较熟悉，可以直接查看 Redis 的 rdb.c 源文件，其中包含了 Redis RDB 文件的解析逻辑。通过阅读源码，你可以深入了解 Redis RDB 文件的结构、编码方式以及数据的读取和处理过程。</p>
<p>需要注意的是，对于大型的 RDB 文件，分析和解析可能需要较长的时间和较大的内存消耗。因此，在进行 RDB 文件分析时，建议选择适当的工具和方法，并在测试环境中进行操作，以避免对生产环境的影响。</p>
<p>另外，Redis 也提供了其他的持久化机制，如 AOF（Append-Only File），它以追加的方式记录 Redis 的写操作，可以通过解析 AOF 文件来了解 Redis 的写入历史。</p>
<h2 id="该如何设计数据结构以避开大key和热key">该如何设计数据结构以避开大Key和热Key？</h2>
<p>老生常谈的大Key和热Key，需要拆、需要打散，如上，我们也给出各种类型Value建议大小，怎么拆呢？又如何打散呢？ 除了拆和打散，还有没有别的办法呢？下面举一些常见的场景案例。</p>
<figure data-type="image" tabindex="1"><img src="https://MouseHappy123.github.io/post-images/1685947136815.png" alt="" loading="lazy"></figure>
<blockquote>
<p>注意事项：</p>
</blockquote>
<ul>
<li>Key名字要设置合理过期时间，避免一个Key中堆积很长时间历史数据</li>
<li>如果预期Key的成员会随着时间增加而变多，建议“HSCAN，SSCAN......”的方式定期清理</li>
<li>消息队列要监控上下游的生产消费速度，或者监控队列长度，避免任务堆积</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://MouseHappy123.github.io/post-images/1685947185442.png" alt="" loading="lazy"></figure>
<blockquote>
<p>注意事项：</p>
</blockquote>
<ul>
<li>对于地域，时间，业务种类这种数据，设计Key的粒度要尽可能小，避免出现地域级别热点Key，利用分类将Key打散</li>
<li>有时候设计Key的时候习惯用Key名字对应MySQL表名字，那这个Key的范围就容易很大，我们建议将Key拆开，用一些常用数据库字段对应到Key上。</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://MouseHappy123.github.io/post-images/1685947259955.png" alt="" loading="lazy"></figure>
<blockquote>
<p>注意事项：</p>
</blockquote>
<ul>
<li>复合类型元素或者成员的Value建议小于4k，避免HGETALL，LRANGE 0 -1，SMEMBERS 取全集导致Redis压力过大</li>
<li>建议将HGETALL换成HMGET，将LRANGE 0 -1换成LRANGE 0 50获取，SMEMBERS换成SISMEMBER，ZRANGE 0 -1换成ZRANGEBYSCORE加LIMIT</li>
<li>元素或者成员Value建议是字符串，而不建议使用大JSON或者数组，可以将JSON存到单独的Hash或者String类型的Key中</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://MouseHappy123.github.io/post-images/1685947363255.jpeg" alt="" loading="lazy"></figure>
<blockquote>
<p>注意事项：</p>
</blockquote>
<ul>
<li>利用Value在Key的名字上取模，常用在各种ID，即Value是整型，比如用户ID，书籍ID，文章ID</li>
<li>Key名字使用的分隔符号不建议使用空格和可能引起运算的符号（“-”，“.”，“+”，“=”，......），建议用 “#”，“:”，“_”</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://MouseHappy123.github.io/post-images/1685947396939.jpeg" alt="" loading="lazy"></figure>
<blockquote>
<p>注意事项：</p>
</blockquote>
<ul>
<li>预期会经常排序的数据，建议控制元素（成员）个数在1000以内</li>
<li>ZREMRANGEBYRANK本身复杂度为O(log(N)+M) ，注意使用频率，天级别或者整点或者业务低峰期执行为好</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://MouseHappy123.github.io/post-images/1685947432933.png" alt="" loading="lazy"></figure>
<blockquote>
<p>注意事项：</p>
</blockquote>
<ul>
<li>使用合理的数据类型</li>
</ul>
<figure data-type="image" tabindex="7"><img src="https://MouseHappy123.github.io/post-images/1685947466597.png" alt="" loading="lazy"></figure>
<blockquote>
<p>注意事项：</p>
</blockquote>
<ul>
<li>适合读多写少的数据</li>
<li>最佳实践是，分布式Redis中，备份数据份数可以用Redis集群分片数评估QPS分布，份数最好是集群分片数的一倍及以上</li>
</ul>
<table>
<thead>
<tr>
<th>业务使用场景</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>页面 “默认” 展示</td>
<td>查询高考分数线的功能，比如：需要显示类如【北京】【211】【北京邮电大学】，那么对于这种前端页面打开默认显示的数据不建议从Redis每次动态读取，可以业务侧本地缓存</td>
</tr>
</tbody>
</table>
<blockquote>
<p>注意事项：<br>
如果是常用枚举，或者默认参数的数据,建议代码里保存</p>
</blockquote>
<table>
<thead>
<tr>
<th>业务使用场景</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>“大文本”/大图文/图片内容/小说/长篇文章/博客/评论内容/客服对话</td>
<td>将小说文章压缩后存储</td>
</tr>
</tbody>
</table>
<blockquote>
<p>注意事项：</p>
</blockquote>
<ul>
<li>JSON和XML换成轻量级的Protobuf</li>
<li>使用Snappy或GZIP压缩</li>
<li>预期Key可能过大，灌入数据的时候，使用EXPIREAT设置过期时间，不要集中到某个时间戳，最好加上随机时间</li>
</ul>
<table>
<thead>
<tr>
<th>业务使用场景</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>所有场景，注意命令时间复杂度及其使用频率</td>
<td>比如O(n)，O(log(n))以及更高复杂度的，控制元素个数，以及命令使用频率，时间复杂度详见：https://redis.io/commands/ <br> 比如INCR的Key需要设置一天的过期时间，那么可以根据返回结果判断，如果已经大于1，就不需要重复 PEXPIREAT <br>  比如Redis4系及以上可以使用MEMORY USAGE查询Key的内存占用，避免未知Key大小，执行DEBUG OBJECT 造成阻塞 <br>  比如 O(N) 的N是成员（元素）总数的慎用或者降低使用频率</td>
</tr>
</tbody>
</table>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[刨根问底 Redis， 面试过程真好使]]></title>
        <id>https://MouseHappy123.github.io/post/bao-gen-wen-di-redis-mian-shi-guo-cheng-zhen-hao-shi/</id>
        <link href="https://MouseHappy123.github.io/post/bao-gen-wen-di-redis-mian-shi-guo-cheng-zhen-hao-shi/">
        </link>
        <updated>2023-05-14T14:58:03.000Z</updated>
        <content type="html"><![CDATA[<p>大家好，这里是 <strong>菜农曰</strong>，欢迎来到我的频道。</p>
<blockquote>
<p>充满寒气的互联网如何在面试中脱颖而出，平时积累很重要，八股文更不能少！下面带来的这篇 Redis 问答希望能够在你的 offer 上增添一把🔥。</p>
</blockquote>
<p>在 Web 应用发展的初期阶段，一个网站的访问量本身就不是很高，直接使用关系型数据库就可以应付绝大部分场景。但是随着互联网时代的崛起，人们对于网站访问速度有着越来越高的要求，直接使用关系型数据库的方案在性能上就出现了瓶颈。因此在客户端与数据层之间就需要一个缓存层来分担请求压力，而 Redis 作为一款优秀的缓存中间件，在企业级架构中占有重要的地位，因此 Redis 也作为面试的必问项。</p>
<figure data-type="image" tabindex="1"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/73557223a5f84be082d2d34f9977b1c0~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>本文通过30个问题，由浅入深，最大程度上覆盖整个Redis的问答内容</strong></p>
<figure data-type="image" tabindex="2"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/73d1a767af384673acd480c0535eebaf~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<hr>
<figure data-type="image" tabindex="3"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/03aab336bf4d4f64859cfb710e630a00~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>Redis（Remote Dictionary Server）是一个开源的、键值对型的数据存储系统。使用C语言编写，遵守BSD协议，可基于内存也可持久化的日志型数据库，提供了多种语言的API，被广泛用于数据库、缓存和消息中间件。并且支持多种类型的数据结构，用于应对各种不同场景。可以存储多种不同类型值之间的映射，支持事务，持久化，LUA 脚本以及多种集群方案等。</p>
<figure data-type="image" tabindex="4"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f884443ea4494014a839ad9da731d7b8~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>优点：</strong></p>
<ol>
<li>完全基于内存操作，性能极高，读写速度快，Redis 能够支持超过 100KB/s 的读写速率</li>
<li>支持高并发，支持10万级别的并发读写</li>
<li>支持主从模式，支持读写分离与分布式</li>
<li>具有丰富的数据类型与丰富的特性（发布订阅模式）</li>
<li>支持持久化操作，不会丢失数据</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>数据库容量受到物理内存的限制，不能实现海量数据的高性能读写</li>
<li>相比关系型数据库，不支持复杂逻辑查询，且存储结构相对简单</li>
<li>虽然提供持久化能力，但实际更多是一个 disk-backed 功能，与传统意义上的持久化有所区别</li>
</ol>
<figure data-type="image" tabindex="5"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6c5546acc4224286bd73e7cb34b0a528~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<blockquote>
<p>Memcache 也是一个开源、高性能、分布式内存对象缓存系统。所有数据均存储在内存中，在服务器重启之后就会消失，需要重新加载数据，采用 hash 表的方式将所有数据缓存在内存中，采用 LRU 算法来逐渐把过期的数据清除掉。</p>
</blockquote>
<ol>
<li><strong>数据类型</strong>：Memcache 仅支持字符串类型，Redis 支持 5 种不同的数据类型</li>
<li><strong>数据持久化</strong>：Memcache 不支持持久化，Redis 支持两种持久化策略，RDB 快照 和 AOF 日志</li>
<li><strong>分布式</strong>：Memcache 不支持分布式，只能在客户端使用一致性哈希的方式来实现分布式存储，Redis3.0 之后可在服务端构建分布式存储，Redis集群没有中心节点，各个节点地位平等，具有线性可伸缩的功能。</li>
<li><strong>内存管理机制</strong>：Memcache数据量不能超出系统内存，但可以调整内存大小，淘汰策略采用LRU算法。Redis增加了 VM 特性，实现了物理内存的限制，它们之间底层实现方式以及客户端之间通信的应用协议不一样。</li>
<li><strong>数据大小限制</strong>：Memcache 单个 key-value 大小有限制，一个Value最大容量为 1MB，Redis 最大容量为512 MB</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8362b10a22cc40a69e307fae9a62c429~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>基本数据类型：</strong></p>
<ul>
<li>String（字符串）</li>
<li>Hash（哈希）</li>
<li>List（列表）</li>
<li>Set（集合）</li>
<li>ZSet（Sorted Set 有序集合）</li>
</ul>
<p><strong>高级数据类型：</strong></p>
<ul>
<li>HyperLogLog：用来做基数统计的算法，在输入元素的数量或体积非常大时，计算基数所需的空间总是固定的，并且是很小的。HyperLogLog 只会根据输入元素来计算基数，而不会存储输入元素本身</li>
<li>Geo：用来地理位置的存储和计算</li>
<li>BitMap：实际上不是特殊的存储结构，本质上是二进制字符串，可以进行位操作，常用于统计日活跃用户等</li>
</ul>
<blockquote>
<p><em>扩展：</em> geohash通过算法将1个定位的经度和纬度2个数值，转换成1个hash字符串。如果2个地方距离越近，那么他们的hash值的前缀越相同。</p>
</blockquote>
<figure data-type="image" tabindex="7"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4af7fc59da164080948cec3951d6d469~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>Redis 底层实现了简单动态字符串的类型（Simple Dynamic String，SDS）来表示 String 类型。没有直接使用C语言定义的字符串类型。</p>
<blockquote>
<p>SDS 实现相对于C语言String方式的提升</p>
</blockquote>
<ol>
<li>避免缓冲区移除。对字符修改时，可以根据 len 属性检查空间是否满足要求</li>
<li>获取字符串长度的复杂度较低</li>
<li>减少内存分配次数</li>
<li>兼容C字符串函数，可以重用C语言库的一部分函数</li>
</ol>
<figure data-type="image" tabindex="8"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d8dedc0c8b444eabb70683e9ee28087f~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>Redis 直接以内存的方式存储可以达到最快的读写速度，如果开启了持久化则通过异步的方式将数据写入磁盘，因此Redis 具有快速和数据持久化的特征。</p>
<p>在内存中操作本身就比从磁盘操作更快，且不受磁盘I/O速度的影响。如果不将数据放在内存中而是保存到磁盘，磁盘I/O速度会严重影响到Redis 的性能，而数据集大小如果达到了内存的最大限定值则不能继续插入新值。</p>
<p>如果打开了虚拟内存功能，当内存用尽时，Redis就会把那些不经常使用的数据存储到磁盘，如果Redis中的虚拟内存被禁了，它就会操作系统的虚拟内存（交换内存），但这时Redis的性能会急剧下降。如果配置了淘汰机制，会根据已配置的数据淘汰机制来淘汰旧数据。</p>
<figure data-type="image" tabindex="9"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cea5a40d195e45199e531c9e8f79a59e~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>1、<strong>尽可能使用哈希表（hash 数据结构）</strong> ：Redis 在储存小于100个字段的Hash结构上，其存储效率是非常高的。所以在不需要集合（set）操作或 list 的push/pop 操作的时候，尽可能使用 hash 结构。</p>
<p>2、<strong>根据业务场景，考虑使用 BitMap</strong></p>
<p>3、<strong>充分利用共享对象池</strong>：Redis 启动时会自动创建【0-9999】的整数对象池，对于 0-9999的内部整数类型的元素，整数值对象都会直接引用整数对象池中的对象，因此尽量使用 0-9999 整数对象可节省内存。</p>
<p>4、<strong>合理使用内存回收策略</strong>：过期数据清除、expire 设置数据过期时间等</p>
<figure data-type="image" tabindex="10"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/91a0f5d92e5240eeb5388f235f1ec68f~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<blockquote>
<p>Redis 能够用来实现分布式锁的命令有 INCR、SETNX、SET，并利用过期时间命令 expire 作为辅助</p>
</blockquote>
<p><strong>方式1：利用 INCR</strong></p>
<p>如果 key 不存在，则初始化值为 0，然后再利用 <code>INCR</code> 进行加 1 操作。后续用户如果获取到的值大于等于 1，说明已经被其他线程加锁。当持有锁的用户在执行完任务后，利用 <code>DECR</code> 命令将 key 的值减 1，则表示释放锁。</p>
<p><strong>方式2：利用 SETNX</strong></p>
<p>先使用 <code>setnx</code> 来争抢锁，抢到之后利用 <code>expire</code> 设置一个过期时间防止未能释放锁。<code>setnx</code> 的意义是如果 key 不存在，则将key设置为 value，返回 1。如果已存在，则不做任何操作，返回 0。</p>
<p><strong>方式3：利用 SET</strong></p>
<p><code>set</code> 指令有非常复杂的参数，相当于合成了 <code>setnx</code> 和 <code>expire</code> 两条命令的功能。其命令格式如：<code>set($Key,$value, array('nx', 'ex'=&gt;$ttl))</code>。</p>
<figure data-type="image" tabindex="11"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b019c688819a4c10bd53d24571e84182~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<ol>
<li>完全基于内存</li>
<li>数据结构简单，操作方便，并且不同数据结构能够应对于不同场景</li>
<li>采用单线程（网络请求模块使用单线程，其他模块仍用了多线程），避免了不必要的上下文切换和竞争条件，也不存在多进程或多线程切换导致CPU消耗，不需要考虑各种锁的问题。</li>
<li>使用多路I/O复用模型，为非阻塞I/O</li>
<li>Redis 本身设定了 VM 机制，没有使用 OS 的Swap，可以实现冷热数据分离，避免因为内存不足而造成访问速度下降的问题</li>
</ol>
<figure data-type="image" tabindex="12"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6b07019825c74601a4c84aec916efeae~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>1、RDB（Redis DataBase）持久化</strong></p>
<p>RDB 是 Redis 中默认的持久化机制，按照一定的时间将内存中的数据以快照的方式保存到磁盘中，它会产生一个特殊类型的文件 <code>.rdb</code> 文件，同时可以通过配置文件中的 <code>save</code> 参数来定义快照的周期</p>
<p>在 RDB 中有两个核心概念 <code>fork</code> 和 <code>cow</code>，在执行备份的流程如下：</p>
<p>在执行<code>bgsave</code>的时候，Redis 会 fork 主进程得到一个新的子进程，子进程是共享主进程内存数据的，会将数据写到磁盘上的一个临时的 <code>.rdb</code> 文件中，当子进程写完临时文件后，会将原来的 <code>.rdb</code> 文件替换掉，这个就是 fork 的概念。那 cow 全称是 copy-on-write ，当主进程执行读操作的时候是访问共享内存的，而主进程执行写操作的时候，则会拷贝一份数据，执行写操作。</p>
<p><em><strong>优点</strong></em></p>
<ol>
<li>只有一个文件 dump.rdb ，方便持久化</li>
<li>容错性好，一个文件可以保存到安全的磁盘</li>
<li>实现了性能最大化，fork 单独子进程来完成持久化，让主进程继续处理命令，主进程不进行任何 I/O 操作，从而保证了Redis的高性能</li>
<li>RDB 是一个紧凑压缩的二进制文化，RDB重启时的加载效率比AOF持久化更高，在数据量大时更明显</li>
</ol>
<p><em><strong>缺点</strong></em></p>
<ol>
<li>可能出现数据丢失，在两次RDB持久化的时间间隔中，如果出现宕机，则会丢失这段时间中的数据</li>
<li>由于RDB是通过fork子进程来协助完成数据持久化，如果当数据集较大时，可能会导致整个服务器间歇性暂停服务</li>
</ol>
<p><strong>2、AOF（Append Only File）持久化</strong></p>
<p>AOF 全称是 Append Only File（追加文件）。当 Redis 处理每一个写命令都会记录在 AOF 文件中，可以看做是命令日志文件。该方式需要设置 AOF 的同步选项，因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区中，同步选项有三种配置项选择：</p>
<ul>
<li><code>always</code>：同步刷盘，可靠性高，但性能影响较大</li>
<li><code>everysec</code>：每秒刷盘，性能适中，最多丢失 1 秒的数据</li>
<li><code>no</code>：操作系统控制，性能最好，可靠性最差</li>
</ul>
<p>为了解决 AOF 文件体检不断增大的问题，用户可以向 Redis 发送 <code>bgrewriteaof</code> 命令，可以将 AOF 文件进行压缩，也可以选择自动触发，在配置文件中配置</p>
<pre><code class="language-arduino">auto-aof-rewrite-precentage 100
auto-aof-rewrite-min-zise 64mb
</code></pre>
<p><em><strong>优点</strong></em></p>
<ol>
<li>实现持久化，数据安全，AOF持久化可以配置 appendfsync 属性为 always，每进行一次命令操作就记录到AOF文件中一次，数据最多丢失一次</li>
<li>通过 append 模式写文件，即使中途服务器宕机，可以通过 Redis-check-aof 工具解决数据一致性问题</li>
<li>AOF 机制的 rewrite 模式。AOF 文件的文件大小触碰到临界点时，rewrite 模式会被运行，重写内存中的所有数据，从而缩小文件体积</li>
</ol>
<p><em><strong>缺点</strong></em></p>
<ol>
<li>AOF 文件大，通常比 RDB 文件大很多</li>
<li>比 RDB 持久化启动效率低，数据集大的时候较为明显</li>
<li>AOF 文件体积可能迅速变大，需要定期执行重写操作来降低文件体积</li>
</ol>
<figure data-type="image" tabindex="13"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8e9e286875c340ccb2b722c888e01206~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>方式1：定时删除</strong></p>
<p>在设置 Key 的过期时间的同时，会创建一个定时器 timer，定时器在 Key 过期时间来临时，会立即执行对 Key 的删除操作</p>
<p><em>特点：</em> 对内存友好，对 CPU 不友好。存在较多过期键时，利用定时器删除过期键会占用相当一部分CPU</p>
<p><strong>方式2：惰性删除</strong></p>
<p>key 不使用时不管 key 过不过期，只会在每次使用的时候再检查 Key 是否过期，如果过期的话就会删除该 Key。</p>
<p><em>特点：</em> 对 CPU 友好，对内存不友好。不会花费额外的CPU资源来检测Key是否过期，但如果存在较多未使用且过期的Key时，所占用的内存就不会得到释放</p>
<p><strong>方式3：定期删除</strong></p>
<p>每隔一段时间就会对数据库进行一次检查，删除里面的过期Key，而检查多少个数据库，则由算法决定</p>
<p><em>特点：</em> 定期删除是对上面两种过期策略的折中，也就是对内存友好和CPU友好的折中方法。每隔一段时间执行一次删除过期键任务，并通过限制操作的时长和频率来减少对CPU时间的占用。</p>
<figure data-type="image" tabindex="14"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8d236c1209bc40018401966f39cda1b6~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<blockquote>
<p>Redis 主从同步分为增量同步和全量同步Redis 会先尝试进行增量同步，如果不成功则会进行全量同步。</p>
</blockquote>
<p><strong>增量同步：</strong></p>
<p>Slave 初始化后开始正常工作时主服务器发生的写操作同步到从服务器的过程。增量同步的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令。</p>
<p><strong>全量同步：</strong></p>
<p>Slave 初始化时它会发送一个 <code>psync</code> 命令到主服务器，如果是第一次同步，主服务器会做一次<code>bgsave</code>，并同时将后续的修改操作记录到内存 buffer 中，待 <code>bgsave</code> 完成后再将 RDB 文件全量同步到从服务器，从服务器接收完成后会将 RDB 快照加载到内存然后写入到本地磁盘，处理完成后，再通知主服务器将期间修改的操作记录同步到复制节点进行重放就完成了整个全量同步过程。</p>
<figure data-type="image" tabindex="15"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fae6f1edcb244bedaf0b9bedbd46f1b6~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>在Redis中，最大使用内存大小由Redis.conf中的参数maxmemory决定，默认值为0，表示不限制，这时实际相当于当前系统的内存。但如果随着数据的增加，如果对内存中的数据没有管理机制，那么数据集大小达到或超过最大内存的大小时，则会造成Redis崩溃。因此需要内存数据淘汰机制。</p>
<p><strong>设有过期时间</strong></p>
<ol>
<li><code>volatile-lru</code>：尝试回收最少使用的键</li>
<li><code>volatile-random</code>：回收随机的键</li>
<li><code>volatile-ttl</code>：优先回收存活时间较短的键</li>
</ol>
<p><strong>没有过期时间</strong></p>
<ol>
<li><code>allkey-lru</code>：尝试回收最少使用的键</li>
<li><code>allkeys-random</code>：回收随机的键</li>
<li><code>noeviction</code>：当内存达到限制并且客户端尝试执行新增，会返回错误</li>
</ol>
<blockquote>
<p><em>淘汰策略的规则</em></p>
<ul>
<li>如果数据呈现幂律分布，也就是一部分数据访问频率高，一部分数据访问频率低，则使用 allKeys-lru</li>
<li>如果数据呈现平等分布，也就是所有的数据访问频率大体相同，则使用 allKeys-random</li>
<li>关于 lru 策略，Redis中并不会准确的删除所有键中最近最少使用的键，而是随机抽取5个键（个数由参数maxmemory-samples决定，默认值是5），删除这5个键中最近最少使用的键。</li>
</ul>
</blockquote>
<figure data-type="image" tabindex="16"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5229bbabfb9b4af79cce1c147ad28413~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>问题1：缓存穿透</strong></p>
<blockquote>
<p>缓存穿透是指缓存和数据库上都没有的数据，导致所有请求都落到数据库上，造成数据库短时间内承受大量的请求而导致宕机</p>
</blockquote>
<p><em>解决：</em></p>
<ol>
<li>使用布隆过滤器：将查询的参数都存储到一个 bitmap 中，在查询缓存前，如果 bitmap 存在则进行底层缓存的数据查询，如果不存在则进行拦截，不再进行缓存的数据查询</li>
<li>缓存空对象：如果数据库查询的为空，则依然把这个数据缓存并设置过期时间，当多次访问的时候可以直接返回结果，避免造成多次访问数据库，但要保证当数据库有数据时及时更新缓存。</li>
</ol>
<p><strong>问题2：缓存击穿</strong></p>
<blockquote>
<p>缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），就会导致所有请求都落到数据库上，造成数据库段时间内承受大量的请求而宕机</p>
</blockquote>
<p><em>解决：</em></p>
<ol>
<li>设置热点数据永不过期</li>
<li>可以使用互斥锁更新，保证同一进程中针对同一个数据不会并发请求到 DB，减小DB的压力</li>
<li>使用随机退避方式，失效时随机 sleep 一个很短的时间，再次查询，如果失败再执行更新</li>
</ol>
<p><strong>问题3：缓存雪崩</strong></p>
<blockquote>
<p>缓存雪崩是指大量缓存同一时间内大面积失效，后面的请求都会落到数据库上，造成数据库段时间无法承受大量的请求而宕掉</p>
</blockquote>
<p><em>解决：</em></p>
<ol>
<li>在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个Key只允许一个线程查询和写缓存，其他线程等待</li>
<li>通过缓存 reload 机制，预先去更新缓存，在即将发生高并发访问前手动触发加载缓存</li>
<li>对于不同的key设置不同的过期时间，让缓存失效的时间点尽量均匀，比如我们可以在原有的失效时间基础上增加一个随机值，比如1~5分钟随机，这样每一个缓存的过期时间的重复率就会降低。</li>
<li>设置二级缓存，或者双缓存策略。</li>
</ol>
<figure data-type="image" tabindex="17"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/86a9ab65a0574835bc67278da98b0930~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>缓存降级，其实都应该是指服务降级。在访问量剧增、服务响应出现问题（如响应延迟或不响应）或非核心服务影响到核心流程的性能的情况下，仍然需要保证核心服务可用，尽管可能一些非主要服务不可用，这时就可以采取服务降级策略。</p>
<p>服务降级的最终目的是保证核心服务可用，即使是有损的。服务降级应当事先确定好降级方案，确定哪些服务是可以降级的，哪些服务是不可降级的。根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心服务的正常运行。</p>
<p>降级往往会指定不同的级别，面临不同的异常等级执行不同的处理。根据服务方式：可以拒接服务，可以延迟服务，也可以随机提供服务。根据服务范围：可以暂时禁用某些功能或禁用某些功能模块。总之服务降级需要根据不同的业务需求采用不同的降级策略。主要的目的就是服务虽然有损但是总比没有好。</p>
<figure data-type="image" tabindex="18"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cc64d57aad76479db8f1b81c2894d602~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<ol>
<li>数据实时同步失效或更新。这是一种增量主动型的方案，能保证数据强一致性，在数据库数据更新之后，主动请求缓存更新</li>
<li>数据异步更新。这是一种增量被动型方案，数据一致性稍弱，数据更新会有延迟，更新数据库数据后，通过异步方式，用多线程方式或消息队列来实现更新</li>
<li>定时任务更新。这是一种增/全量被动型方案，通过定时任务按一定频率调度更新，数据一致性最差</li>
</ol>
<figure data-type="image" tabindex="19"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2c0d91c6ade6466c9579749965b14fc3~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<ol>
<li>直接写个缓存刷新页面，上线时手工操作</li>
<li>数据量不大，可以在项目启动时自动进行加载</li>
<li>定时刷新缓存</li>
</ol>
<figure data-type="image" tabindex="20"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/17e57a34a21245a588bea83a48d6103c~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<blockquote>
<p>Sentinel（哨兵）适用于监控 Redis 集群中 Master 和 Slave 状态的工具，是Redis的高可用性解决方案</p>
</blockquote>
<p><strong>主要作用</strong></p>
<ol>
<li>监控。哨兵会不断检查用户的Master和Slave是否运作正常</li>
<li>提醒。当被监控的某个Redis节点出现问题时，哨兵可以通过API向管理员或其他应用程序发送通知</li>
<li>自动故障迁移。当一个Master不能正常工作时，哨兵会开始一次自动故障迁移操作，它会将集群中一个Slave提升为新的Master，并让其他Slave改为与新的Master进行同步。当客户端试图连接失败的Master时，集群也会想客户端返回新的Master地址。当主从服务器切换后，新Master的Redis.conf，Slave的Redis.conf和Sentinel的Redis.conf三者配置文件都会发生相应的改变。</li>
</ol>
<figure data-type="image" tabindex="21"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7529e180458f47ac8993313ebd2588a8~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>问题背景</strong></p>
<blockquote>
<p>Redis 是基于TCP协议的请求/响应服务器，每次通信都要经过TCP协议的三次握手，所以当需要执行的命令足够复杂时，会产生很大的网络延迟，并且网络的传输时间成本和服务器开销没有计入其中，总的延迟可能更大。</p>
</blockquote>
<p><strong>Pipeline解决</strong></p>
<ul>
<li>Pipeline 主要就是为了解决存在这种情况的场景，使用Pipeline模式，客户端可以一次性发送多个命令，无需等待服务端返回，这样可以将多次I/O往返的时间缩短为一次，大大减少了网络往返时间，提高了系统性能。</li>
<li>Pipeline 是基于队列实现，基于先进先出的原理，满足了数据顺序性。同时一次提交的命令很多的话，队列需要非常大量的内存来组织返回数据内容，如果大量使用Pipeline的话，应当合理分批次提交命令。</li>
<li>Pipeline的默认同步个数为<code>53</code>个，累加到 53 条数据时会把数据提交</li>
</ul>
<p><em>注意：</em> Redis 集群中使用不了 Pipeline，对可靠性要求很高，每次操作都需要立即获取本次操作结果的场景都不适合用 Pipeline</p>
<figure data-type="image" tabindex="22"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/401cbbcd770e4d9f96e246c22dcbd1f1~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<ol>
<li>Master 最好不要做 RDB 持久化，因为这时 save 命令调度 rdbSave 函数，会阻塞主线程的工作，当数据集比较大时可能造成主线程间断性暂停服务</li>
<li>如果数据比较重要，将某个 Slave 节点开启AOF数据备份，策略设置为每秒一次</li>
<li>为了主从复制速度和连接的稳定性，Master 和 Slave 最好在同一个局域网中</li>
<li>尽量避免在运行压力很大的主库上增加从库</li>
<li>主从复制不要用图状结构，用单向链表结构更为稳定，<code>Mater-&gt;Slave1-&gt;Slave2-&gt;Slave3...</code> 这样的结构方便解决单点故障问题，实现 Slave 对 Master 的替换，如果 Master 崩溃，可以立即启用 Slave1替换Mater，而其他依赖关系则保持不变。</li>
</ol>
<figure data-type="image" tabindex="23"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/03d858703c1c482bab4136ef75e4715b~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>方式1：先更新数据库，再更新缓存</strong></p>
<p>这种是常规的做法，但是如果更新缓存失败，将会导致缓存是旧数据，数据库是新数据</p>
<p><strong>方式2：先删除缓存，再写入数据库</strong></p>
<p>这种方式能够解决方式1的问题，但是仅限于低并发的场景，不然如果有新的请求在删完缓存之后，写数据库之前进来，那么缓存就会马上更新数据库更新之前数据，造成数据不一致的问题</p>
<p><strong>方式3：延时双删策略</strong></p>
<p>这种方式是先删除缓存，然后更新数据库，最后延迟个几百毫秒再删除缓存</p>
<p><strong>方式4：直接操作缓存，定期写入数据库</strong></p>
<figure data-type="image" tabindex="24"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d84298f66be247c0ad0ec4c768f5dcde~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<blockquote>
<p>虽然Redis的Transactions 提供的并不是严格的 ACID的事务（如一串用EXEC提交执行的命令，如果在执行中服务器宕机，那么会有一部分命令执行一部分命令未执行），但这些Transactions还是提供了基本的命令打包执行的功能（在服务器不出问题的情况下，可以保证一连串的命令是顺序在一起执行的。</p>
</blockquote>
<p>Redis 事务的本质就是四个原语：</p>
<ol>
<li><code>multi</code>：用于开启一个事务，它总是返回 OK，当 multi 执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会被立即执行，而是放到一个队列中，当 exec 命令被调用的时候，所有队列d 命令才会执行</li>
<li><code>exec</code>：执行所有事务队列内的命令，返回事务内所有命令的返回值，当操作被打断的时候，返回空值 nil</li>
<li><code>watch</code>：是一个乐观锁。可以为 redis 事务提供 CAS 操作，可以监控一个或多个键。一旦其中有一个键被修改（删除），之后的事务就不会执行，监控一直持续到 exec 命令执行之后</li>
<li><code>discard</code>：调用 discard，客户端可以清空事务队列中的命令，并放弃执行事务</li>
</ol>
<p>事务支持一次执行多个命令，一个事务中的所有命令都会被序列化。在事务执行的过程中，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令队列中。Redis 不支持回滚事务，在事务失败的时候不会回滚，而是继续执行余下的命令。</p>
<figure data-type="image" tabindex="25"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8f521b1d190744c8a59584720da804df~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>方式1：Cluster 3.0</strong></p>
<p>这是Redis 自带的集群功能，它采用的分布式算法是哈希槽，而不是一致性Hash。支持主从结构，可以扩展多个从服务器，当主节点挂了可以很快切换到一个从节点作主节点，然后其他从节点都会切换读取最新的主节点。</p>
<p><strong>方式2：Twemproxy</strong></p>
<p>Twitter 开源的一个轻量级后端代理。可以管理 Redis 或 Memcache 集群。相对于 Redis 集群来说，易于管理。它的使用方法和Redis集群没有任何区别，只需要设置多个Redis实例后，在本需要连接 Redis 的地方改为连接 Twemproxy ，它就会以一个代理的身份接收请求并使用一致性Hash算法，将请求连接到具体的Redis节点上，将结果再返回Twemproxy。对于客户端来说，Twemproxy 相当于是缓存数据库的总入口，它不需要知道后端如何部署的。Twemproxy 会检测与每个节点的连接是否正常，如果存在异常节点就会将其剔除，等一段时间后，Twemproxy 还会再次尝试连接被剔除的节点。</p>
<p><strong>方式3：Codis</strong></p>
<p>它是一个 Redis 分布式的解决方法，对于应用使用 Codis Proxy 的连接和使用Redis的服务没有明显区别，应用能够像使用单机 Redis 一样，让 Codis 底层处理请求转发，实现不停机实现数据迁移等工作。</p>
<figure data-type="image" tabindex="26"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e63e3c50a16d4644bca7af0dc23a18cb~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<blockquote>
<p>什么是脑裂问题</p>
</blockquote>
<p>脑裂问题通常是因为网络问题导致的。让 master、slave 和 sentinel 三类节点处于不同的网络分区。此时哨兵无法感知到 master 的存在，会将 slave 提升为 master 节点。此时就会存在两个 master，就像大脑分裂，那么原来的客户端往继续往旧的 master 写入数据，而新的master 就会丢失这些数据</p>
<blockquote>
<p>如何解决</p>
</blockquote>
<p>通过配置文件修改两个参数</p>
<pre><code class="language-lua">min-slaves-to-write 3  # 表示连接到 master 最少 slave 的数量
min-slaves-max-lag 10  # 表示slave连接到master最大的延迟时间
--------------------新版本写法-----------------
min-replicas-to-write 3
min-replicas-max-lag  10
</code></pre>
<p>配置这两个参数之后，如果发生集群脑裂，原先的master节点接收到写入请求就会拒绝，就会减少数据同步之后的数据丢失</p>
<figure data-type="image" tabindex="27"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0a5d1c1f911f4f2db20f3aa496b6a17a~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>一般使用 <code>List</code> 结构作为队列。<code>Rpush</code> 生产消息，<code>Lpop</code> 消费消息。当 <code>Lpop</code> 没有消费的时候，需要适当 sleep 一会再重试。但是重复 sleep 会耗费性能，所以我们可以利用 list 的 <code>blpop</code> 指令，在还没有消息到来时，它会阻塞直到消息到来。</p>
<p>我们也可以使用 <code>pub/sub</code> 主题订阅者模式，实现 1：N 的消费队列，但是在消费者下线的时候，生产的消息会丢失</p>
<figure data-type="image" tabindex="28"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e2818e2d09ad416c95865c8c78258039~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>可以使用 <code>zset</code> 结构，可以拿时间戳作为 score，消息的内容作为key，通过调用 <code>zadd</code> 来生产消息，消费者使用 <code>zrangebyscore</code> 指令轮询获取 N 秒之前的数据进行处理</p>
<figure data-type="image" tabindex="29"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/89d10db926d547c584b40248b0407620~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>Redis Cluster提供了自动将数据分散到各个不同节点的能力，但采用的策略并不是一致性Hash，而是哈希槽。Redis 集群将整个Key的数值域分成16384个哈希槽，每个Key通过 CRC16检验后对16384驱魔来决定放置到那个槽中，集群的每个节点都负责其中一部分的哈希槽。</p>
<figure data-type="image" tabindex="30"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7e91d70546504825b865aa239f2decc0~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>1、数据缓存</strong></p>
<p>经典的场景，现在几乎是所有中大型网站都在用的提升手段，合理地利用缓存能够提升网站访问速度</p>
<p><strong>2、排行榜</strong></p>
<p>可以借助Redis提供的有序集合（<code>sorted set</code>）能力实现排行榜的功能</p>
<p><strong>3、计数器</strong></p>
<p>可以借助Redis提供的 <code>incr</code> 命令来实现计数器功能，因为是单线程的原子操作，保证了统计不会出错，而且速度快</p>
<p><strong>4、分布式session共享</strong></p>
<p>集群模式下，可以基于 Redis 实现 session 共享</p>
<p><strong>5、分布式锁</strong></p>
<p>在分布式架构中，为了保证并发访问时操作的原子性，可以利用Redis来实现分布式锁的功能</p>
<p><strong>6、最新列表</strong></p>
<p>可以借助Redis列表结构，<code>LPUSH</code>、<code>LPOP</code>、<code>LTRIM</code>等命令来完成内容的查询</p>
<p><strong>7、位操作</strong></p>
<p>可以借助Redis中 <code>setbit</code>、<code>getbit</code>、<code>bitcount</code> 等命令来完成数量上千万甚至上亿的场景下，实现用户活跃度统计</p>
<p><strong>8、消息队列</strong></p>
<p>Redis 提供了发布（<code>Publish</code>）与订阅（<code>Subscribe</code>）以及阻塞队列能力，能够实现一个简单的消息队列系统</p>
<figure data-type="image" tabindex="31"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c4caee79346a416f8d5c1c0a37ee23b6~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><strong>方式1：Set 结构</strong></p>
<p>以日期为 key，以用户 ID（对应数据库的 Primary Id）组成的集合为 value</p>
<ul>
<li>查询某个用户的签到状态 <code>sismember key member</code></li>
<li>插入签到状态 <code>sadd key member</code></li>
<li>统计某天用户的签到人数 <code>scard key</code></li>
</ul>
<p><strong>方式2：bitMap 结构</strong></p>
<p>Key的格式为<code>u:sign:uid:yyyyMM</code>，Value则采用长度为4个字节（32位）的位图（最大月份只有31天）。位图的每一位代表一天的签到，1表示已签，0表示未签。</p>
<pre><code class="language-r"># 用户2月17号签到
SETBIT u:sign:1000:201902 16 1 # 偏移量是从0开始，所以要把17减1

# 检查2月17号是否签到
GETBIT u:sign:1000:201902 16 # 偏移量是从0开始，所以要把17减1

# 统计2月份的签到次数
BITCOUNT u:sign:1000:201902

# 获取2月份前28天的签到数据
BITFIELD u:sign:1000:201902 get u28 0

# 获取2月份首次签到的日期
BITPOS u:sign:1000:201902 1 # 返回的首次签到的偏移量，加上1即为当月的某一天
</code></pre>
<p><em><strong>两者对比</strong></em></p>
<ol>
<li>使用 set 的方式所占用的内存只与数量相关，和存储哪些 ID 无关</li>
<li>使用 bitmap 的方式所占用的内存与数量没有绝对的关系，而是与最高位有关，比如假设 ID 为 500 W的用户签到了，那么从 1~4999999 用户不管是否签到，所占用的内存都是 500 w个bit，这边是最坏的情况</li>
<li>使用 bitmap 最大可以存储 2^32-1也就是 512M 数据</li>
<li>使用 bitmap 只适用存储只有两个状态的数据，比如用户签到，资源（视频、文章、商品）的已读或未读状态</li>
</ol>
<figure data-type="image" tabindex="32"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ef46337f18a74d958e603fb8e7c9c0d3~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>Redis中 ZSet 是选择使用 <code>跳表</code> 而不是红黑树</p>
<blockquote>
<p>什么是跳表</p>
</blockquote>
<ul>
<li>跳表是一个随机化的数据结构，实质上就是一种可以进行二分查找的有序链表。</li>
<li>跳表在原有的有序链表上增加了多级索引，通过索引来实现快速查找</li>
<li>跳表不仅能提高搜索性能，同时也可以提高插入和删除操作的性能</li>
</ul>
<figure data-type="image" tabindex="33"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/67d95768427141ce964c935fc7fd544e~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p><em><strong>总结：</strong></em></p>
<ol>
<li>跳表是可以实现二分查找的有序链表</li>
<li>每个元素插入时随机生成它的 level</li>
<li>最底层包含所有的元素</li>
<li>如果一个元素出现在 level(x)，那么它肯定出现在 x 以下的 level 中</li>
<li>每个索引节点包含两个指针，一个向下，一个向右</li>
<li>跳表查询、插入、删除的时间复杂度为 O(log n)，与平衡二叉树接近</li>
</ol>
<blockquote>
<p>为什么不选择红黑树来实现</p>
</blockquote>
<p>首先来分析下 Redis 的有序集合支持的操作：</p>
<ol>
<li>插入元素</li>
<li>删除元素</li>
<li>查找元素</li>
<li>有序输出所有元素</li>
<li>查找区间内的所有元素</li>
</ol>
<p>其中前 4 项红黑树都可以完成，且时间复杂度与跳表一致，但是最后一个红黑树的效率就没有跳表高了。在跳表中，要查找区间的元素，只要定位到两个区间端点在最低层级的位置，然后按顺序遍历元素就可以了，非常高效。</p>
<hr>
<p>好了，以上便是本篇的所有内容，如果觉得对你有帮助的小伙伴不妨点个关注做个伴，便是对小菜最大的支持。不要空谈，不要贪懒，和小菜一起做个<code>吹着牛X做架构</code>的程序猿吧~ 咱们下文再见！</p>
<blockquote>
<p>今天的你多努力一点，明天的你就能少说一句求人的话！</p>
<p><em>我是小菜，一个和你一起变强的男人。</em> <code>💋</code></p>
<p>微信公众号已开启，<strong>菜农曰</strong>，没关注的同学们记得关注哦！</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[限流算法]]></title>
        <id>https://MouseHappy123.github.io/post/xian-liu-suan-fa/</id>
        <link href="https://MouseHappy123.github.io/post/xian-liu-suan-fa/">
        </link>
        <updated>2023-05-13T13:49:47.000Z</updated>
        <content type="html"><![CDATA[<figure data-type="image" tabindex="1"><img src="https://jsdelivr.codeqihan.com/gh/niumoo/wdbyte-img/git/2021/20220314204028.png" alt="" loading="lazy"></figure>
<h2 id="前言">前言</h2>
<p>最近几年，随着微服务的流行，服务和服务之间的依赖越来越强，调用关系越来越复杂，服务和服务之间的<strong>稳定性</strong>越来越重要。在遇到突发的请求量激增，恶意的用户访问，亦或请求频率过高给下游服务带来较大压力时，我们常常需要通过缓存、限流、熔断降级、负载均衡等多种方式保证服务的稳定性。其中<strong>限流</strong>是不可或缺的一环，这篇文章介绍<strong>限流</strong>相关知识。</p>
<h2 id="1-限流">1. 限流</h2>
<p><strong>限流</strong>顾名思义，就是对请求或并发数进行限制；通过对一个时间窗口内的请求量进行限制来保障系统的正常运行。如果我们的服务资源有限、处理能力有限，就需要对调用我们服务的上游请求进行限制，以防止自身服务由于资源耗尽而停止服务。</p>
<p>在限流中有两个概念需要了解。</p>
<ul>
<li><strong>阈值</strong>：在一个单位时间内允许的请求量。如 QPS 限制为10，说明 1 秒内最多接受 10 次请求。</li>
<li><strong>拒绝策略</strong>：超过阈值的请求的拒绝策略，常见的拒绝策略有直接拒绝、排队等待等。</li>
</ul>
<h2 id="2-固定窗口算法">2. 固定窗口算法</h2>
<p><strong>固定窗口算法</strong>又叫<strong>计数器算法</strong>，是一种<strong>简单</strong>方便的限流算法。主要通过一个支持<strong>原子操作</strong>的计数器来累计 1 秒内的请求次数，当 1 秒内计数达到限流阈值时触发拒绝策略。每过 1 秒，计数器重置为 0 开始重新计数。</p>
<h3 id="21-代码实现">2.1. 代码实现</h3>
<p>下面是简单的代码实现，QPS 限制为 2，这里的代码做了一些<strong>优化</strong>，并没有单独开一个线程去每隔 1 秒重置计数器，而是在每次调用时进行时间间隔计算来确定是否先重置计数器。</p>
<pre><code class="language-java">/**
 * @author https://www.wdbyte.com
 */
public class RateLimiterSimpleWindow {
    // 阈值
    private static Integer QPS = 2;
    // 时间窗口（毫秒）
    private static long TIME_WINDOWS = 1000;
    // 计数器
    private static AtomicInteger REQ_COUNT = new AtomicInteger();
    
    private static long START_TIME = System.currentTimeMillis();

    public synchronized static boolean tryAcquire() {
        if ((System.currentTimeMillis() - START_TIME) &gt; TIME_WINDOWS) {
            REQ_COUNT.set(0);
            START_TIME = System.currentTimeMillis();
        }
        return REQ_COUNT.incrementAndGet() &lt;= QPS;
    }

    public static void main(String[] args) throws InterruptedException {
        for (int i = 0; i &lt; 10; i++) {
            Thread.sleep(250);
            LocalTime now = LocalTime.now();
            if (!tryAcquire()) {
                System.out.println(now + &quot; 被限流&quot;);
            } else {
                System.out.println(now + &quot; 做点什么&quot;);
            }
        }
    }
}
</code></pre>
<p>运行结果：</p>
<pre><code class="language-log">20:53:43.038922 做点什么
20:53:43.291435 做点什么
20:53:43.543087 被限流
20:53:43.796666 做点什么
20:53:44.050855 做点什么
20:53:44.303547 被限流
20:53:44.555008 被限流
20:53:44.809083 做点什么
20:53:45.063828 做点什么
20:53:45.314433 被限流
</code></pre>
<p>从输出结果中可以看到大概每秒操作 3 次，由于限制 QPS 为 2，所以平均会有一次被限流。看起来可以了，不过我们思考一下就会发现这种简单的限流方式是有问题的，虽然我们限制了 QPS 为 2，但是当遇到时间窗口的临界突变时，如 1s 中的后 500 ms 和第 2s 的前 500ms 时，虽然是加起来是 1s 时间，却可以被请求 4 次。</p>
<figure data-type="image" tabindex="2"><img src="https://jsdelivr.codeqihan.com/gh/niumoo/wdbyte-img/git/2021/20220223215006.png" alt="固定窗口算法" loading="lazy"></figure>
<p>简单修改测试代码，可以进行验证：</p>
<pre><code class="language-java">// 先休眠 400ms，可以更快的到达时间窗口。
Thread.sleep(400);
for (int i = 0; i &lt; 10; i++) {
    Thread.sleep(250);
    if (!tryAcquire()) {
        System.out.println(&quot;被限流&quot;);
    } else {
        System.out.println(&quot;做点什么&quot;);
    }
}
</code></pre>
<p>得到输出中可以看到连续 4 次请求，间隔 250 ms 没有却被限制。：</p>
<pre><code class="language-auto">20:51:17.395087 做点什么
20:51:17.653114 做点什么
20:51:17.903543 做点什么
20:51:18.154104 被限流
20:51:18.405497 做点什么
20:51:18.655885 做点什么
20:51:18.906177 做点什么
20:51:19.158113 被限流
20:51:19.410512 做点什么
20:51:19.661629 做点什么
</code></pre>
<h2 id="3-滑动窗口算法">3. 滑动窗口算法</h2>
<p>我们已经知道固定窗口算法的实现方式以及它所存在的问题，而滑动窗口算法是对固定窗口算法的改进。既然固定窗口算法在遇到时间窗口的临界突变时会有问题，那么我们在遇到下一个时间窗口前也调整时间窗口不就可以了吗？</p>
<p>下面是滑动窗口的示意图。</p>
<figure data-type="image" tabindex="3"><img src="https://jsdelivr.codeqihan.com/gh/niumoo/wdbyte-img/git/2021/20220223215316.png" alt="滑动窗口算法" loading="lazy"></figure>
<p>上图的示例中，每 500ms 滑动一次窗口，可以发现窗口滑动的间隔越短，时间窗口的临界突变问题发生的概率也就越小，不过只要有时间窗口的存在，还是有可能发生<strong>时间窗口的临界突变问题</strong>。</p>
<h3 id="31-代码实现">3.1. 代码实现</h3>
<p>下面是基于以上滑动窗口思路实现的简单的滑动窗口限流工具类。</p>
<pre><code class="language-java">package com.wdbyte.rate.limiter;

import java.time.LocalTime;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * 滑动窗口限流工具类
 *
 * @author https://www.wdbyte.com
 */
public class RateLimiterSlidingWindow {
    /**
     * 阈值
     */
    private int qps = 2;
    /**
     * 时间窗口总大小（毫秒）
     */
    private long windowSize = 1000;
    /**
     * 多少个子窗口
     */
    private Integer windowCount = 10;
    /**
     * 窗口列表
     */
    private WindowInfo[] windowArray = new WindowInfo[windowCount];

    public RateLimiterSlidingWindow(int qps) {
        this.qps = qps;
        long currentTimeMillis = System.currentTimeMillis();
        for (int i = 0; i &lt; windowArray.length; i++) {
            windowArray[i] = new WindowInfo(currentTimeMillis, new AtomicInteger(0));
        }
    }

    /**
     * 1. 计算当前时间窗口
     * 2. 更新当前窗口计数 &amp; 重置过期窗口计数
     * 3. 当前 QPS 是否超过限制
     *
     * @return
     */
    public synchronized boolean tryAcquire() {
        long currentTimeMillis = System.currentTimeMillis();
        // 1. 计算当前时间窗口
        int currentIndex = (int)(currentTimeMillis % windowSize / (windowSize / windowCount));
        // 2.  更新当前窗口计数 &amp; 重置过期窗口计数
        int sum = 0;
        for (int i = 0; i &lt; windowArray.length; i++) {
            WindowInfo windowInfo = windowArray[i];
            if ((currentTimeMillis - windowInfo.getTime()) &gt; windowSize) {
                windowInfo.getNumber().set(0);
                windowInfo.setTime(currentTimeMillis);
            }
            if (currentIndex == i &amp;&amp; windowInfo.getNumber().get() &lt; qps) {
                windowInfo.getNumber().incrementAndGet();
            }
            sum = sum + windowInfo.getNumber().get();
        }
        // 3. 当前 QPS 是否超过限制
        return sum &lt;= qps;
    }

    private class WindowInfo {
        // 窗口开始时间
        private Long time;
        // 计数器
        private AtomicInteger number;

        public WindowInfo(long time, AtomicInteger number) {
            this.time = time;
            this.number = number;
        }
        // get...set...
    }
}
</code></pre>
<p>下面是测试用例，设置 QPS 为 2,测试次数 20 次，每次间隔 300 毫秒，预计成功次数在 12 次左右。</p>
<pre><code class="language-java">public static void main(String[] args) throws InterruptedException {
    int qps = 2, count = 20, sleep = 300, success = count * sleep / 1000 * qps;
    System.out.println(String.format(&quot;当前QPS限制为:%d,当前测试次数:%d,间隔:%dms,预计成功次数:%d&quot;, qps, count, sleep, success));
    success = 0;
    RateLimiterSlidingWindow myRateLimiter = new RateLimiterSlidingWindow(qps);
    for (int i = 0; i &lt; count; i++) {
        Thread.sleep(sleep);
        if (myRateLimiter.tryAcquire()) {
            success++;
            if (success % qps == 0) {
                System.out.println(LocalTime.now() + &quot;: success, &quot;);
            } else {
                System.out.print(LocalTime.now() + &quot;: success, &quot;);
            }
        } else {
            System.out.println(LocalTime.now() + &quot;: fail&quot;);
        }
    }
    System.out.println();
    System.out.println(&quot;实际测试成功次数:&quot; + success);
}
</code></pre>
<p>下面是测试的结果。</p>
<pre><code class="language-auto">当前QPS限制为:2,当前测试次数:20,间隔:300ms,预计成功次数:12
16:04:27.077782: success, 16:04:27.380715: success, 
16:04:27.684244: fail
16:04:27.989579: success, 16:04:28.293347: success, 
16:04:28.597658: fail
16:04:28.901688: fail
16:04:29.205262: success, 16:04:29.507117: success, 
16:04:29.812188: fail
16:04:30.115316: fail
16:04:30.420596: success, 16:04:30.725897: success, 
16:04:31.028599: fail
16:04:31.331047: fail
16:04:31.634127: success, 16:04:31.939411: success, 
16:04:32.242380: fail
16:04:32.547626: fail
16:04:32.847965: success, 
实际测试成功次数:11
</code></pre>
<h2 id="4-滑动日志算法">4. 滑动日志算法</h2>
<p>滑动日志算法是实现限流的另一种方法，这种方法比较简单。基本逻辑就是记录下所有的请求时间点，新请求到来时先判断最近指定时间范围内的请求数量是否超过指定阈值，由此来确定是否达到限流，这种方式没有了时间窗口突变的问题，限流比较准确，但是因为要记录下每次请求的时间点，所以<strong>占用的内存较多</strong>。</p>
<h3 id="41-代码实现">4.1. 代码实现</h3>
<p>下面是简单实现的 一个滑动日志算法，因为滑动日志要每次请求单独存储一条记录，可能占用内存过多。所以下面这个实现其实不算严谨的滑动日志，更像一个把 1 秒时间切分成 1000 个时间窗口的滑动窗口算法。</p>
<pre><code class="language-java">package com.wdbyte.rate.limiter;

import java.time.LocalTime;
import java.util.HashSet;
import java.util.Set;
import java.util.TreeMap;

/**
 * 滑动日志方式限流
 * 设置 QPS 为 2.
 *
 * @author https://www.wdbyte.com
 */
public class RateLimiterSildingLog {

    /**
     * 阈值
     */
    private Integer qps = 2;
    /**
     * 记录请求的时间戳,和数量
     */
    private TreeMap&lt;Long, Long&gt; treeMap = new TreeMap&lt;&gt;();

    /**
     * 清理请求记录间隔, 60 秒
     */
    private long claerTime = 60 * 1000;

    public RateLimiterSildingLog(Integer qps) {
        this.qps = qps;
    }

    public synchronized boolean tryAcquire() {
        long now = System.currentTimeMillis();
        // 清理过期的数据老数据，最长 60 秒清理一次
        if (!treeMap.isEmpty() &amp;&amp; (treeMap.firstKey() - now) &gt; claerTime) {
            Set&lt;Long&gt; keySet = new HashSet&lt;&gt;(treeMap.subMap(0L, now - 1000).keySet());
            for (Long key : keySet) {
                treeMap.remove(key);
            }
        }
        // 计算当前请求次数
        int sum = 0;
        for (Long value : treeMap.subMap(now - 1000, now).values()) {
            sum += value;
        }
        // 超过QPS限制，直接返回 false
        if (sum + 1 &gt; qps) {
            return false;
        }
        // 记录本次请求
        if (treeMap.containsKey(now)) {
            treeMap.compute(now, (k, v) -&gt; v + 1);
        } else {
            treeMap.put(now, 1L);
        }
        return sum &lt;= qps;
    }

    public static void main(String[] args) throws InterruptedException {
        RateLimiterSildingLog rateLimiterSildingLog = new RateLimiterSildingLog(3);
        for (int i = 0; i &lt; 10; i++) {
            Thread.sleep(250);
            LocalTime now = LocalTime.now();
            if (rateLimiterSildingLog.tryAcquire()) {
                System.out.println(now + &quot; 做点什么&quot;);
            } else {
                System.out.println(now + &quot; 被限流&quot;);
            }
        }
    }
}
</code></pre>
<p>代码中把阈值 QPS 设定为 3，运行可以得到如下日志：</p>
<pre><code class="language-auto">20:51:17.395087 做点什么
20:51:17.653114 做点什么
20:51:17.903543 做点什么
20:51:18.154104 被限流
20:51:18.405497 做点什么
20:51:18.655885 做点什么
20:51:18.906177 做点什么
20:51:19.158113 被限流
20:51:19.410512 做点什么
20:51:19.661629 做点什么
</code></pre>
<h2 id="5-漏桶算法">5. 漏桶算法</h2>
<p>漏桶算法中的漏桶是一个形象的比喻，这里可以用生产者消费者模式进行说明，请求是一个生产者，每一个请求都如一滴水，请求到来后放到一个队列（漏桶）中，而桶底有一个孔，不断的漏出水滴，就如消费者不断的在消费队列中的内容，消费的速率（漏出的速度）等于限流阈值。即假如 QPS 为 2，则每 <code>1s / 2= 500ms</code> 消费一次。漏桶的桶有大小，就如队列的容量，当请求堆积超过指定容量时，会触发拒绝策略。</p>
<p>下面是漏桶算法的示意图。</p>
<figure data-type="image" tabindex="4"><img src="https://jsdelivr.codeqihan.com/gh/niumoo/wdbyte-img/git/2021/20220225161827.png" alt="漏桶算法" loading="lazy"></figure>
<p>由介绍可以知道，漏桶模式中的消费处理总是能以恒定的速度进行，可以很好的<strong>保护自身系统</strong>不被突如其来的流量冲垮；但是这也是漏桶模式的缺点，假设 QPS 为 2，同时 2 个请求进来，2 个请求并不能同时进行处理响应，因为每 <code>1s / 2= 500ms</code> 只能处理一个请求。</p>
<h2 id="6-令牌桶算法">6. 令牌桶算法</h2>
<p>令牌桶算法同样是实现限流是一种常见的思路，最为常用的 Google 的 Java 开发工具包 Guava 中的限流工具类 RateLimiter 就是令牌桶的一个实现。令牌桶的实现思路类似于生产者和消费之间的关系。</p>
<p>系统服务作为生产者，按照指定频率向桶（容器）中添加令牌，如 QPS 为 2，每 500ms 向桶中添加一个令牌，如果桶中令牌数量达到阈值，则不再添加。</p>
<p>请求执行作为消费者，每个请求都需要去桶中拿取一个令牌，取到令牌则继续执行；如果桶中无令牌可取，就触发拒绝策略，可以是超时等待，也可以是直接拒绝本次请求，由此达到限流目的。</p>
<p>下面是令牌桶限流算法示意图。</p>
<figure data-type="image" tabindex="5"><img src="https://jsdelivr.codeqihan.com/gh/niumoo/wdbyte-img/git/2021/20220225160801.png" alt="令牌桶算法" loading="lazy"></figure>
<p>思考令牌桶的实现可以以下特点。</p>
<ol>
<li>1s / 阈值（QPS） = 令牌添加时间间隔。</li>
<li>桶的容量等于限流的阈值，令牌数量达到阈值时，不再添加。</li>
<li>可以适应流量突发，N 个请求到来只需要从桶中获取 N 个令牌就可以继续处理。</li>
<li>有启动过程，令牌桶启动时桶中无令牌，然后按照令牌添加时间间隔添加令牌，若启动时就有阈值数量的请求过来，会因为桶中没有足够的令牌而触发拒绝策略，不过如 RateLimiter 限流工具已经优化了这类问题。</li>
</ol>
<h3 id="61-代码实现">6.1. 代码实现</h3>
<p>Google 的 Java 开发工具包 Guava 中的限流工具类 RateLimiter 就是令牌桶的一个实现，日常开发中我们也不会手动实现了，这里直接使用 RateLimiter 进行测试。</p>
<p>引入依赖：</p>
<pre><code class="language-xml">&lt;exclusion&gt;
  	&lt;groupId&gt;com.google.guava&lt;/groupId&gt;
    &lt;artifactId&gt;guava&lt;/artifactId&gt;
  	&lt;version&gt;31.0.1-jre&lt;/version&gt;
&lt;/exclusion&gt;
</code></pre>
<p>RateLimiter 限流体验：</p>
<pre><code class="language-java">// qps 2
RateLimiter rateLimiter = RateLimiter.create(2);
for (int i = 0; i &lt; 10; i++) {
    String time = LocalDateTime.now().format(DateTimeFormatter.ISO_LOCAL_TIME);
    System.out.println(time + &quot;:&quot; + rateLimiter.tryAcquire());
    Thread.sleep(250);
}
</code></pre>
<p>代码中限制 QPS 为 2，也就是每隔 500ms 生成一个令牌，但是程序每隔 250ms 获取一次令牌，所以两次获取中只有一次会成功。</p>
<pre><code class="language-shell">17:19:06.797557:true
17:19:07.061419:false
17:19:07.316283:true
17:19:07.566746:false
17:19:07.817035:true
17:19:08.072483:false
17:19:08.326347:true
17:19:08.577661:false
17:19:08.830252:true
17:19:09.085327:false
</code></pre>
<h3 id="62-思考">6.2. 思考</h3>
<p>虽然演示了 Google Guava 工具包中的 RateLimiter 的实现，但是我们需要思考一个问题，就是令牌的添加方式，如果按照指定间隔添加令牌，那么需要开一个线程去定时添加，如果有很多个接口很多个 RateLimiter 实例，<strong>线程数会随之增加</strong>，这显然不是一个好的办法。显然 Google 也考虑到了这个问题，在 RateLimiter 中，是<strong>在每次令牌获取时才进行计算令牌是否足够的</strong>。它通过存储的下一个令牌生成的时间，和当前获取令牌的时间差，再结合阈值，去计算令牌是否足够，同时再记录下一个令牌的生成时间以便下一次调用。</p>
<p>下面是 Guava 中 RateLimiter 类的子类 SmoothRateLimiter 的 <code>resync()</code> 方法的代码分析，可以看到其中的令牌计算逻辑。</p>
<pre><code class="language-java">void resync(long nowMicros) { // 当前微秒时间
    // 当前时间是否大于下一个令牌生成时间
    if (nowMicros &gt; this.nextFreeTicketMicros) { 
      	// 可生成的令牌数 newPermits = （当前时间 - 下一个令牌生成时间）/ 令牌生成时间间隔。
      	// 如果 QPS 为2，这里的 coolDownIntervalMicros 就是 500000.0 微秒(500ms)
        double newPermits = (double)(nowMicros - this.nextFreeTicketMicros) / this.coolDownIntervalMicros();
				// 更新令牌库存 storedPermits。
      	this.storedPermits = Math.min(this.maxPermits, this.storedPermits + newPermits);
				// 更新下一个令牌生成时间 nextFreeTicketMicros
      	this.nextFreeTicketMicros = nowMicros;
    }
}
</code></pre>
<h2 id="7-redis-分布式限流">7. Redis 分布式限流</h2>
<p>Redis 是一个开源的内存数据库，可以用来作为数据库、缓存、消息中间件等。Redis 是单线程的，又在内存中操作，所以速度极快，得益于 Redis 的各种特性，所以使用 Redis 实现一个限流工具是十分方便的。</p>
<p>下面的演示都基于Spring Boot 项目，并需要以下依赖。</p>
<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>
<p>配置 Redis 信息。</p>
<pre><code class="language-yml">spring:
  redis:
    database: 0
    password: 
    port: 6379
    host: 127.0.0.1
    lettuce:
      shutdown-timeout: 100ms
      pool:
        min-idle: 5
        max-idle: 10
        max-active: 8
        max-wait: 1ms
</code></pre>
<h3 id="71-固定窗口限流">7.1. 固定窗口限流</h3>
<p>Redis 中的固定窗口限流是使用 <code>incr</code> 命令实现的，<code>incr</code> 命令通常用来自增计数；如果我们使用时间戳信息作为 key，自然就可以统计每秒的请求量了，以此达到限流目的。</p>
<p>这里有两点要注意。</p>
<ol>
<li>对于不存在的 key，第一次新增时，value 始终为 1。</li>
<li>INCR 和 EXPIRE 命令操作应该在一个<strong>原子操作</strong>中提交，以保证每个 key 都正确设置了过期时间，不然会有 key 值无法自动删除而导致的内存溢出。</li>
</ol>
<p>由于 Redis 中实现事务的复杂性，所以这里直接只用 <code>lua</code> 脚本来实现原子操作。下面是 <code>lua</code> 脚本内容。</p>
<pre><code class="language-lua">local count = redis.call(&quot;incr&quot;,KEYS[1])
if count == 1 then
  redis.call('expire',KEYS[1],ARGV[2])
end
if count &gt; tonumber(ARGV[1]) then
  return 0
end
return 1
</code></pre>
<p>下面是使用 Spring Boot 中 <code>RedisTemplate</code> 来实现的 <code>lua</code> 脚本调用测试代码。</p>
<pre><code class="language-java">/**
 * @author https://www.wdbyte.com
 */
@SpringBootTest
class RedisLuaLimiterByIncr {
    private static String KEY_PREFIX = &quot;limiter_&quot;;
    private static String QPS = &quot;4&quot;;
    private static String EXPIRE_TIME = &quot;1&quot;;

    @Autowired
    private StringRedisTemplate stringRedisTemplate;

    @Test
    public void redisLuaLimiterTests() throws InterruptedException, IOException {
        for (int i = 0; i &lt; 15; i++) {
            Thread.sleep(200);
            System.out.println(LocalTime.now() + &quot; &quot; + acquire(&quot;user1&quot;));
        }
    }

    /**
     * 计数器限流
     *
     * @param key
     * @return
     */
    public boolean acquire(String key) {
        // 当前秒数作为 key
        key = KEY_PREFIX + key + System.currentTimeMillis() / 1000;
        DefaultRedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;();
        redisScript.setResultType(Long.class);
        //lua文件存放在resources目录下
        redisScript.setScriptSource(new ResourceScriptSource(new ClassPathResource(&quot;limiter.lua&quot;)));
        return stringRedisTemplate.execute(redisScript, Arrays.asList(key), QPS, EXPIRE_TIME) == 1;
    }
}
</code></pre>
<p>代码中虽然限制了 QPS 为 4，但是因为这种限流实现是把毫秒时间戳作为 key 的，所以会有临界窗口突变的问题，下面是运行结果，可以看到因为时间窗口的变化，导致了 QPS 超过了限制值 4。</p>
<pre><code class="language-shell">17:38:23.122044 true
17:38:23.695124 true
17:38:23.903220 true
# 此处有时间窗口变化，所以下面继续 true
17:38:24.106206 true
17:38:24.313458 true
17:38:24.519431 true
17:38:24.724446 true
17:38:24.932387 false
17:38:25.137912 true
17:38:25.355595 true
17:38:25.558219 true
17:38:25.765801 true
17:38:25.969426 false
17:38:26.176220 true
17:38:26.381918 true
</code></pre>
<h3 id="73-滑动窗口限流">7.3. 滑动窗口限流</h3>
<p>通过对上面的基于 <code>incr</code> 命令实现的 Redis 限流方式的测试，我们已经发现了固定窗口限流所带来的问题，在这篇文章的第三部分已经介绍了滑动窗口限流的优势，它可以大幅度降低因为窗口临界突变带来的问题，那么如何使用 Redis 来实现滑动窗口限流呢？</p>
<p>这里主要使用 <code>ZSET</code> 有序集合来实现滑动窗口限流，<code>ZSET</code> 集合有下面几个特点：</p>
<ol>
<li>ZSET 集合中的 key 值可以自动排序。</li>
<li>ZSET 集合中的 value 不能有重复值。</li>
<li>ZSET 集合可以方便的使用 ZCARD 命令获取元素个数。</li>
<li>ZSET 集合可以方便的使用 ZREMRANGEBYLEX 命令移除指定范围的 key 值。</li>
</ol>
<p>基于上面的四点特性，可以编写出基于 <code>ZSET</code> 的滑动窗口限流 <code>lua</code> 脚本。</p>
<pre><code class="language-lua">--KEYS[1]: 限流 key
--ARGV[1]: 时间戳 - 时间窗口
--ARGV[2]: 当前时间戳（作为score）
--ARGV[3]: 阈值
--ARGV[4]: score 对应的唯一value
-- 1. 移除时间窗口之前的数据
redis.call('zremrangeByScore', KEYS[1], 0, ARGV[1])
-- 2. 统计当前元素数量
local res = redis.call('zcard', KEYS[1])
-- 3. 是否超过阈值
if (res == nil) or (res &lt; tonumber(ARGV[3])) then
    redis.call('zadd', KEYS[1], ARGV[2], ARGV[4])
    return 1
else
    return 0
end
</code></pre>
<p>下面是使用 Spring Boot 中 <code>RedisTemplate</code> 来实现的 <code>lua</code> 脚本调用测试代码。</p>
<pre><code class="language-java">@SpringBootTest
class RedisLuaLimiterByZset {

    private String KEY_PREFIX = &quot;limiter_&quot;;
    private String QPS = &quot;4&quot;;

    @Autowired
    private StringRedisTemplate stringRedisTemplate;

    @Test
    public void redisLuaLimiterTests() throws InterruptedException, IOException {
        for (int i = 0; i &lt; 15; i++) {
            Thread.sleep(200);
            System.out.println(LocalTime.now() + &quot; &quot; + acquire(&quot;user1&quot;));
        }
    }

    /**
     * 计数器限流
     *
     * @param key
     * @return
     */
    public boolean acquire(String key) {
        long now = System.currentTimeMillis();
        key = KEY_PREFIX + key;
        String oldest = String.valueOf(now - 1_000);
        String score = String.valueOf(now);
        String scoreValue = score;
        DefaultRedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;();
        redisScript.setResultType(Long.class);
        //lua文件存放在resources目录下
        redisScript.setScriptSource(new ResourceScriptSource(new ClassPathResource(&quot;limiter2.lua&quot;)));
        return stringRedisTemplate.execute(redisScript, Arrays.asList(key), oldest, score, QPS, scoreValue) == 1;
    }
}
</code></pre>
<p>代码中限制 QPS 为 4，运行结果信息与之一致。</p>
<pre><code class="language-auto">17:36:37.150370 true
17:36:37.716341 true
17:36:37.922577 true
17:36:38.127497 true
17:36:38.335879 true
17:36:38.539225 false
17:36:38.745903 true
17:36:38.952491 true
17:36:39.159497 true
17:36:39.365239 true
17:36:39.570572 false
17:36:39.776635 true
17:36:39.982022 true
17:36:40.185614 true
17:36:40.389469 true
</code></pre>
<p>这里介绍了 Redis 实现限流的两种方式，当然使用 Redis 也可以实现漏桶和令牌桶两种限流算法，这里就不做演示了，感兴趣的可以自己研究下。</p>
<h2 id="8-总结">8. 总结</h2>
<p>这篇文章介绍实现限流的几种方式，主要是<strong>窗口算法和桶算法</strong>，两者各有优势。</p>
<ul>
<li>窗口算法实现简单，逻辑清晰，可以很直观的得到当前的 QPS 情况，但是会有时间窗口的临界突变问题，而且不像桶一样有队列可以缓冲。</li>
<li>桶算法虽然稍微复杂，不好统计 QPS 情况，但是桶算法也有优势所在。
<ul>
<li>漏桶模式消费速率恒定，可以很好的<strong>保护自身系统</strong>，可以对流量进行整形，但是面对突发流量不能快速响应。</li>
<li>令牌桶模式可以面对突发流量，但是启动时会有缓慢加速的过程，不过常见的开源工具中已经对此优化。</li>
</ul>
</li>
</ul>
<p><strong>单机限流与分布式限流</strong></p>
<p>上面演示的基于代码形式的窗口算法和桶算法限流都适用于单机限流，如果需要分布式限流可以结合注册中心、负载均衡计算每个服务的限流阈值，但这样会降低一定精度，如果对精度要求不是太高，可以使用。</p>
<p>而 Redis 的限流，由于 Redis 的单机性，本身就可以用于分布式限流。使用 Redis 可以实现各种可以用于限流算法，如果觉得麻烦也可以使用开源工具如 redisson，已经封装了基于 Redis 的限流。</p>
<p><strong>其他限流工具</strong></p>
<p>文中已经提到了 <code>Guava</code> 的限流工具包，不过它毕竟是单机的，开源社区中也有很多分布式限流工具，如阿里开源的 Sentinel 就是不错的工具，Sentinel 以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。</p>
<p>一如既往，文章中的代码存放在：<a href="https://github.com/niumoo/JavaNotes/tree/master/core-java-rate-limiter">github.com/niumoo/JavaNotes</a></p>
<h2 id="参考">参考</h2>
<p>Redis INCR:<a href="https://redis.io/commands/incr">https://redis.io/commands/incr</a></p>
<p>Rate Limiting Wikipedia：<a href="https://en.wikipedia.org/wiki/Rate_limiting">https://en.wikipedia.org/wiki/Rate_limiting</a></p>
<p>SpringBoot Redis:<a href="https://www.cnblogs.com/lenve/p/10965667.html">https://www.cnblogs.com/lenve/p/10965667.html</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redis分布式锁]]></title>
        <id>https://MouseHappy123.github.io/post/ru-he-yong-redis-shi-xian-fen-bu-shi-suo/</id>
        <link href="https://MouseHappy123.github.io/post/ru-he-yong-redis-shi-xian-fen-bu-shi-suo/">
        </link>
        <updated>2023-05-13T13:41:41.000Z</updated>
        <content type="html"><![CDATA[<h3 id="前言">前言</h3>
<p>日常开发中，秒杀下单、抢红包等等业务场景，都需要用到分布式锁。而Redis非常适合作为分布式锁使用。本文将分七个方案展开，跟大家探讨Redis分布式锁的正确使用方式。如果有不正确的地方，欢迎大家指出哈，一起学习一起进步。</p>
<p>公众号：<strong>捡田螺的小男孩</strong></p>
<ul>
<li>
<p>什么是分布式锁</p>
</li>
<li>
<p>方案一：SETNX + EXPIRE</p>
</li>
<li>
<p>方案二：SETNX + value值是（系统时间+过期时间）</p>
</li>
<li>
<p>方案三：使用Lua脚本(包含SETNX + EXPIRE两条指令)</p>
</li>
<li>
<p>方案四：SET的扩展命令（SET EX PX NX）</p>
</li>
<li>
<p>方案五：SET EX PX NX + 校验唯一随机值,再释放锁</p>
</li>
<li>
<p>方案六: 开源框架:Redisson</p>
</li>
<li>
<p>方案七：多机实现的分布式锁Redlock</p>
</li>
<li>
<p>github地址，感谢每颗star</p>
</li>
</ul>
<blockquote>
<p><a href="https://github.com/whx123/JavaHome" title="https://github.com/whx123/JavaHome">github.com/whx123/Java…</a></p>
</blockquote>
<h3 id="什么是分布式锁">什么是分布式锁</h3>
<blockquote>
<p>分布式锁其实就是，控制分布式系统不同进程共同访问共享资源的一种锁的实现。如果不同的系统或同一个系统的不同主机之间共享了某个临界资源，往往需要互斥来防止彼此干扰，以保证一致性。</p>
</blockquote>
<p>我们先来看下，一把靠谱的分布式锁应该有哪些特征：</p>
<figure data-type="image" tabindex="1"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/42884a1613344c11be5fef3b9e8ed7c5~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<ul>
<li><strong>互斥性</strong>: 任意时刻，只有一个客户端能持有锁。</li>
<li><strong>锁超时释放</strong>：持有锁超时，可以释放，防止不必要的资源浪费，也可以防止死锁。</li>
<li><strong>可重入性</strong>:一个线程如果获取了锁之后,可以再次对其请求加锁。</li>
<li><strong>高性能和高可用</strong>：加锁和解锁需要开销尽可能低，同时也要保证高可用，避免分布式锁失效。</li>
<li><strong>安全性</strong>：锁只能被持有的客户端删除，不能被其他客户端删除</li>
</ul>
<h3 id="redis分布式锁方案一setnx-expire">Redis分布式锁方案一：SETNX + EXPIRE</h3>
<p>提到Redis的分布式锁，很多小伙伴马上就会想到<code>setnx</code>+ <code>expire</code>命令。即先用<code>setnx</code>来抢锁，如果抢到之后，再用<code>expire</code>给锁设置一个过期时间，防止锁忘记了释放。</p>
<blockquote>
<p>SETNX 是SET IF NOT EXISTS的简写.日常命令格式是SETNX key value，如果 key不存在，则SETNX成功返回1，如果这个key已经存在了，则返回0。</p>
</blockquote>
<p>假设某电商网站的某商品做秒杀活动，key可以设置为key_resource_id,value设置任意值，伪代码如下：</p>
<pre><code class="language-csharp">if（jedis.setnx(key_resource_id,lock_value) == 1）{ //加锁
    expire（key_resource_id，100）; //设置过期时间
    try {
        do something  //业务请求
    }catch(){
　　}
　　finally {
       jedis.del(key_resource_id); //释放锁
    }
}
</code></pre>
<p>但是这个方案中，<code>setnx</code>和<code>expire</code>两个命令分开了，<strong>不是原子操作</strong>。如果执行完<code>setnx</code>加锁，正要执行<code>expire</code>设置过期时间时，进程crash或者要重启维护了，那么这个锁就“长生不老”了，<strong>别的线程永远获取不到锁啦</strong>。</p>
<h3 id="redis分布式锁方案二setnx-value值是系统时间过期时间">Redis分布式锁方案二：SETNX + value值是(系统时间+过期时间)</h3>
<p>为了解决方案一，<strong>发生异常锁得不到释放的场景</strong>，有小伙伴认为，可以把过期时间放到<code>setnx</code>的value值里面。如果加锁失败，再拿出value值校验一下即可。加锁代码如下：</p>
<pre><code class="language-kotlin">long expires = System.currentTimeMillis() + expireTime; //系统时间+设置的过期时间
String expiresStr = String.valueOf(expires);

// 如果当前锁不存在，返回加锁成功
if (jedis.setnx(key_resource_id, expiresStr) == 1) {
        return true;
} 
// 如果锁已经存在，获取锁的过期时间
String currentValueStr = jedis.get(key_resource_id);

// 如果获取到的过期时间，小于系统当前时间，表示已经过期
if (currentValueStr != null &amp;&amp; Long.parseLong(currentValueStr) &lt; System.currentTimeMillis()) {

     // 锁已过期，获取上一个锁的过期时间，并设置现在锁的过期时间（不了解redis的getSet命令的小伙伴，可以去官网看下哈）
    String oldValueStr = jedis.getSet(key_resource_id, expiresStr);
    
    if (oldValueStr != null &amp;&amp; oldValueStr.equals(currentValueStr)) {
         // 考虑多线程并发的情况，只有一个线程的设置值和当前值相同，它才可以加锁
         return true;
    }
}
        
//其他情况，均返回加锁失败
return false;
}
</code></pre>
<p>这个方案的优点是，巧妙移除<code>expire</code>单独设置过期时间的操作，把<strong>过期时间放到setnx的value值</strong>里面来。解决了方案一发生异常，锁得不到释放的问题。但是这个方案还有别的缺点：</p>
<blockquote>
<ul>
<li>过期时间是客户端自己生成的（System.currentTimeMillis()是当前系统的时间），必须要求分布式环境下，每个客户端的时间必须同步。</li>
<li>如果锁过期的时候，并发多个客户端同时请求过来，都执行jedis.getSet()，最终只能有一个客户端加锁成功，但是该客户端锁的过期时间，可能被别的客户端覆盖</li>
<li>该锁没有保存持有者的唯一标识，可能被别的客户端释放/解锁。</li>
</ul>
</blockquote>
<h3 id="redis分布式锁方案三使用lua脚本包含setnx-expire两条指令">Redis分布式锁方案三：使用Lua脚本(包含SETNX + EXPIRE两条指令)</h3>
<p>实际上，我们还可以使用Lua脚本来保证原子性（包含setnx和expire两条指令），lua脚本如下：</p>
<pre><code class="language-vbnet">if redis.call('setnx',KEYS[1],ARGV[1]) == 1 then
   redis.call('expire',KEYS[1],ARGV[2])
else
   return 0
end;
</code></pre>
<p>加锁代码如下：</p>
<pre><code class="language-ini"> String lua_scripts = &quot;if redis.call('setnx',KEYS[1],ARGV[1]) == 1 then&quot; +
            &quot; redis.call('expire',KEYS[1],ARGV[2]) return 1 else return 0 end&quot;;   
Object result = jedis.eval(lua_scripts, Collections.singletonList(key_resource_id), Collections.singletonList(values));
//判断是否成功
return result.equals(1L);
</code></pre>
<p>这个方案还是有缺点的哦，至于哪些缺点，你先思考一下。也可以想下。跟方案二对比，哪个更好？</p>
<h3 id="redis分布式锁方案方案四set的扩展命令set-ex-px-nx">Redis分布式锁方案方案四：SET的扩展命令（SET EX PX NX）</h3>
<p>除了使用，使用Lua脚本，保证<code>SETNX + EXPIRE</code>两条指令的原子性，我们还可以巧用Redis的SET指令扩展参数！（<code>SET key value[EX seconds][PX milliseconds][NX|XX]</code>），它也是原子性的！（v2.6.12实现的）</p>
<blockquote>
<p>SET key value[EX seconds][PX milliseconds][NX|XX]</p>
<ul>
<li>NX :表示key不存在的时候，才能set成功，也即保证只有第一个客户端请求才能获得锁，而其他客户端请求只能等其释放锁，才能获取。</li>
<li>EX seconds :设定key的过期时间，时间单位是秒。</li>
<li>PX milliseconds: 设定key的过期时间，单位为毫秒</li>
<li>XX: 仅当key存在时设置值</li>
</ul>
</blockquote>
<p>伪代码demo如下：</p>
<pre><code class="language-csharp">if（jedis.set(key_resource_id, lock_value, &quot;NX&quot;, &quot;EX&quot;, 100s) == 1）{ //加锁
    try {
        do something  //业务处理
    }catch(){
　　}
　　finally {
       jedis.del(key_resource_id); //释放锁
    }
}
</code></pre>
<p>但是呢，这个方案还是可能存在问题：</p>
<ul>
<li>问题一：<strong>锁过期释放了，业务还没执行完</strong>。假设线程a获取锁成功，一直在执行临界区的代码。但是100s过去后，它还没执行完。但是，这时候锁已经过期了，此时线程b又请求过来。显然线程b就可以获得锁成功，也开始执行临界区的代码。那么问题就来了，临界区的业务代码都不是严格串行执行的啦。</li>
<li>问题二：<strong>锁被别的线程误删</strong>。假设线程a执行完后，去释放锁。但是它不知道当前的锁可能是线程b持有的（线程a去释放锁时，有可能过期时间已经到了，此时线程b进来占有了锁）。那线程a就把线程b的锁释放掉了，但是线程b临界区业务代码可能都还没执行完呢。</li>
</ul>
<h3 id="方案五set-ex-px-nx-校验唯一随机值再删除">方案五：SET EX PX NX + 校验唯一随机值,再删除</h3>
<p>既然锁可能被别的线程误删，那我们给value值设置一个标记当前线程唯一的随机数，在删除的时候，校验一下，不就OK了嘛。伪代码如下：</p>
<pre><code class="language-csharp">if（jedis.set(key_resource_id, uni_request_id, &quot;NX&quot;, &quot;EX&quot;, 100s) == 1）{ //加锁
    try {
        do something  //业务处理
    }catch(){
　　}
　　finally {
       //判断是不是当前线程加的锁,是才释放
       if (uni_request_id.equals(jedis.get(key_resource_id))) {
        jedis.del(lockKey); //释放锁
        }
    }
}
</code></pre>
<p>在这里，<strong>判断是不是当前线程加的锁</strong>和<strong>释放锁</strong>不是一个原子操作。如果调用jedis.del()释放锁的时候，可能这把锁已经不属于当前客户端，会解除他人加的锁。</p>
<figure data-type="image" tabindex="2"><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9237655e6b1a47038d2774231e507e11~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>为了更严谨，一般也是用lua脚本代替。lua脚本如下：</p>
<pre><code class="language-vbnet">if redis.call('get',KEYS[1]) == ARGV[1] then 
   return redis.call('del',KEYS[1]) 
else
   return 0
end;
</code></pre>
<h3 id="redis分布式锁方案六redisson框架">Redis分布式锁方案六：Redisson框架</h3>
<p>方案五还是可能存在<strong>锁过期释放，业务没执行完</strong>的问题。有些小伙伴认为，稍微把锁过期时间设置长一些就可以啦。其实我们设想一下，是否可以给获得锁的线程，开启一个定时守护线程，每隔一段时间检查锁是否还存在，存在则对锁的过期时间延长，防止锁过期提前释放。</p>
<p>当前开源框架Redisson解决了这个问题。我们一起来看下Redisson底层原理图吧：</p>
<figure data-type="image" tabindex="3"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/367cd1a7a3fb4d398988e4166416d71d~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>只要线程一加锁成功，就会启动一个<code>watch dog</code>看门狗，它是一个后台线程，会每隔10秒检查一下，如果线程1还持有锁，那么就会不断的延长锁key的生存时间。因此，Redisson就是使用Redisson解决了<strong>锁过期释放，业务没执行完</strong>问题。</p>
<h3 id="redis分布式锁方案七多机实现的分布式锁redlockredisson">Redis分布式锁方案七：多机实现的分布式锁Redlock+Redisson</h3>
<p>前面六种方案都只是基于单机版的讨论，还不是很完美。其实Redis一般都是集群部署的：</p>
<figure data-type="image" tabindex="4"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7349794feeee458aa71c27f27a0b2428~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>如果线程一在Redis的master节点上拿到了锁，但是加锁的key还没同步到slave节点。恰好这时，master节点发生故障，一个slave节点就会升级为master节点。线程二就可以获取同个key的锁啦，但线程一也已经拿到锁了，锁的安全性就没了。</p>
<p>为了解决这个问题，Redis作者 antirez提出一种高级的分布式锁算法：Redlock。Redlock核心思想是这样的：</p>
<blockquote>
<p>搞多个Redis master部署，以保证它们不会同时宕掉。并且这些master节点是完全相互独立的，相互之间不存在数据同步。同时，需要确保在这多个master实例上，是与在Redis单实例，使用相同方法来获取和释放锁。</p>
</blockquote>
<p>我们假设当前有5个Redis master节点，在5台服务器上面运行这些Redis实例。</p>
<figure data-type="image" tabindex="5"><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0df0a36c7ccd439291a8a869ff4ddad3~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" alt="" loading="lazy"></figure>
<p>RedLock的实现步骤:如下</p>
<blockquote>
<ul>
<li>1.获取当前时间，以毫秒为单位。</li>
<li>2.按顺序向5个master节点请求加锁。客户端设置网络连接和响应超时时间，并且超时时间要小于锁的失效时间。（假设锁自动失效时间为10秒，则超时时间一般在5-50毫秒之间,我们就假设超时时间是50ms吧）。如果超时，跳过该master节点，尽快去尝试下一个master节点。</li>
<li>3.客户端使用当前时间减去开始获取锁时间（即步骤1记录的时间），得到获取锁使用的时间。当且仅当超过一半（N/2+1，这里是5/2+1=3个节点）的Redis master节点都获得锁，并且使用的时间小于锁失效时间时，锁才算获取成功。（如上图，10s&gt; 30ms+40ms+50ms+4m0s+50ms）</li>
<li>如果取到了锁，key的真正有效时间就变啦，需要减去获取锁所使用的时间。</li>
<li>如果获取锁失败（没有在至少N/2+1个master实例取到锁，有或者获取锁时间已经超过了有效时间），客户端要在所有的master节点上解锁（即便有些master节点根本就没有加锁成功，也需要解锁，以防止有些漏网之鱼）。</li>
</ul>
</blockquote>
<p>简化下步骤就是：</p>
<ul>
<li>按顺序向5个master节点请求加锁</li>
<li>根据设置的超时时间来判断，是不是要跳过该master节点。</li>
<li>如果大于等于三个节点加锁成功，并且使用的时间小于锁的有效期，即可认定加锁成功啦。</li>
<li>如果获取锁失败，解锁！</li>
</ul>
<p>Redisson实现了redLock版本的锁，有兴趣的小伙伴，可以去了解一下哈~</p>
<h3 id="公众号">公众号</h3>
<ul>
<li>欢迎关注公众号：捡田螺的小男孩</li>
</ul>
<h3 id="参考与感谢">参考与感谢</h3>
<ul>
<li><a href="https://juejin.cn/post/6844903656911798285" title="https://juejin.cn/post/6844903656911798285">redis系列：分布式锁</a></li>
<li><a href="https://www.infoq.cn/article/dvaaj71f4fbqsxmgvdce" title="https://www.infoq.cn/article/dvaaj71f4fbqsxmgvdce">浅析 Redis 分布式锁解决方案</a></li>
<li><a href="https://juejin.cn/post/6844904082860146695#heading-3" title="https://juejin.cn/post/6844904082860146695#heading-3">细说Redis分布式锁🔒</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU5ODUwNzY1Nw==&amp;mid=2247484155&amp;idx=1&amp;sn=0c73f45f2f641ba0bf4399f57170ac9b&amp;scene=21#wechat_redirect" title="https://mp.weixin.qq.com/s?__biz=MzU5ODUwNzY1Nw==&amp;mid=2247484155&amp;idx=1&amp;sn=0c73f45f2f641ba0bf4399f57170ac9b&amp;scene=21#wechat_redirect">Redlock：Redis分布式锁最牛逼的实现</a></li>
</ul>
<h3 id="解决redis主从架构导致的分布式锁失效问题">解决Redis主从架构导致的分布式锁失效问题</h3>
<p><a href="https://juejin.cn/post/7038473714970656775#heading-14">分布式锁</a></p>
<ul>
<li>①红锁算法：多台独立的Redis同时写入数据，锁失效时间之内，一半以上的机器写成功则返回获取锁成功，否则返回获取锁失败，失败时会释放掉那些成功的机器上的锁。
<ul>
<li>优点：可以完美解决掉主从架构带来的锁失效问题</li>
<li>缺点：成本高，需要线上部署多台独立的Redis节点</li>
<li>这种算法是Redis官方提出的解决方案：<a href="https://redis.io/topics/distlock" title="https://redis.io/topics/distlock">红锁算法</a></li>
</ul>
</li>
<li>②额外记录锁状态：再通过额外的中间件等独立部署的节点记录锁状态，比如在DB中记录锁状态，在尝试获取分布式锁之前需先查询DB中的锁持有记录，如果还在持有则继续阻塞，只有当状态为未持有时再尝试获取分布式锁。
<ul>
<li>优点：可以依赖于项目中现有的节点实现，节约部署成本</li>
<li>缺点：
<ul>
<li>实现需要配合定时器实现过期失效，保证锁的合理失效机制</li>
<li>获取锁的性能方面堪忧，会大大增加获取锁的性能开销</li>
<li>所有过程都需自己实现，实现难度比较复杂</li>
</ul>
</li>
<li>总结：这种方式类似于两把分布式锁叠加实现，先获取一把后再获取另一把</li>
</ul>
</li>
<li>③Zookeeper实现：使用Zookeeper代替Redis实现，因为Zookeeper追求的是高稳定，所以Zookeeper实现分布式锁时，不会出现这个问题</li>
</ul>
<h3 id="redisson框架中的分布式锁">Redisson框架中的分布式锁</h3>
<blockquote>
<p>Redisson是用Java语言编写的。它是一个基于Redis的分布式Java对象和服务框架，提供了丰富的功能和API来简化分布式应用程序的开发。</p>
</blockquote>
<p>在上述的内容中，曾从分布式锁的引出到自己实现的每个细节问题进行了分析，但实际开发过程中并不需要我们自己去实现，因为自己实现的分布式锁多多少少会存在一些隐患问题。而这些工作实际已经有框架封装了，比如：<a href="https://redisson.org/" title="https://redisson.org">Redisson框架</a>，其内部已经基于redis为我们封装好了分布式锁，开发过程中屏蔽了底层处理，让我们能够像使用<code>ReetrantLock</code>一样使用分布式锁，如下：</p>
<pre><code class="language-xml">/* ---------pom.xml文件-------- */
&lt;dependency&gt;
    &lt;groupId&gt;org.redisson&lt;/groupId&gt;
    &lt;artifactId&gt;redisson&lt;/artifactId&gt;
    &lt;version&gt;3.8.2&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<pre><code class="language-yml">/* ---------application.yml文件-------- */
spring:
    redis:
      database: 0
      host: 192.168.12.130
      port: 6379
      password: 123456
      timeout: 2m
</code></pre>
<pre><code class="language-java">// 注入redisson的客户端
@Autowired
private RedissonClient redisson;

// 写入redis的key值
String lockKey = &quot;lock-&quot; + inventory.getInventoryId();
// 获取一个Rlock锁对象
RLock lock = redisson.getLock(lockKey);
// 获取锁，并为其设置过期时间为10s
lock.lock(10,TimeUnit.SECONDS);
try{
    // 执行业务逻辑....
} finally {
    // 释放锁
    lock.unlock();
}
/* ---------RedissonClient配置类-------- */      
@Configuration
public class RedissonConfig {
    // 读取配置文件中的配置信息
    @Value(&quot;${spring.redis.host}&quot;)
    private String host;
    @Value(&quot;${spring.redis.port}&quot;)
    private String port;
    @Value(&quot;${spring.redis.password}&quot;)
    private String password;

    // 注入RedissonClient的Bean交由Spring管理
    @Bean
    public RedissonClient redisson() {
        //单机模式
        Config config = new Config();
        config.useSingleServer().
            setAddress(&quot;redis://&quot; + host + &quot;:&quot; + port).
            setPassword(password).setDatabase(0);
        return Redisson.create(config);
    }
}
</code></pre>
<p>如上源码，即可获得一把最基本的分布式锁，同时除开最基本的加锁方法外，还支持其他形式的获取锁：</p>
<ul>
<li><code>lock.tryLock(20,10,TimeUnit.SECONDS)</code>：非阻塞式获取锁，在获取锁失败后的20s内一直尝试重新获取锁，超出20s则直接返回获取锁失败</li>
<li><code>lock.lockAsync(10,TimeUnit.SECONDS)</code>：异步阻塞式获取锁，可以支持异步获取加锁的结果，该方法会返回一个<code>Future</code>对象，可通过<code>Future</code>对象异步获取加锁结果</li>
<li><code>lock.tryLockAsync(20,10,TimeUnit.SECONDS)</code>：异步非阻塞式获取锁，比上面那个多了一个超时时间</li>
</ul>
<p>同时Redisson框架中的锁实现还不仅仅只有一种，如下：</p>
<ul>
<li>FairLock公平锁：与<code>ReetrantLock</code>一样，支持创建公平锁，即先到的线程一定优化获取锁</li>
<li>MultiLock连锁：多个<code>RLock</code>对象组成一把锁，也就是几把锁组成的一把锁，可以用来实现红锁算法，因为<code>RLock</code>对象可以不是一个<code>Redisson</code>创建出来的，也就是可以使用多个Redis客户端的连接对象获取多把锁组成连锁，只有当所有个锁获取成功后，才能返回获取锁成功，如果获取一把个锁失败，则返回获取锁失败</li>
<li>RedLock红锁：和前面分析的Redis官方给出的红锁算法实现一致，继承了连锁，主要用于解决主从架构锁失效的问题</li>
</ul>
<h3 id="redisson框架中的连锁分析">Redisson框架中的连锁分析</h3>
<p>连锁向上继承了<code>RLock</code>，向下为<code>RedLock</code>提供了实现，所以它是Redisson框架中最为关键的一种锁，先来看看它的使用方式：</p>
<pre><code class="language-java">// 获取多个RLock锁对象（redisson可以是不同的客户端）
RLock lock1 = redisson.getLock(&quot;lock-1&quot;);
RLock lock2 = redisson.getLock(&quot;lock-2&quot;);
RLock lock3 = redisson.getLock(&quot;lock-3&quot;);

// 将多把锁组合成一把连锁，通过连锁进行获取锁与释放锁操作
RedissonMultiLock lock = new RedissonMultiLock(lock1,lock2,lock3);
// 获取锁：一半以上的锁获取成功才能成功，反之删除写入成功的节点数据
lock.lock();
// 释放锁
lock.unlock();
</code></pre>
<p>使用方式并不难理解，只需要创建多个<code>RLock</code>锁对象后，再通过多个锁对象组和成一把连锁，通过连锁对象进行获取锁与释放锁的操作即可。</p>
<h3 id="redisson框架中的连锁源码实现分析">Redisson框架中的连锁源码实现分析</h3>
<p>OK~，上面简单的给出了<code>MultiLock</code>连锁的使用方式，接下来重点分析一下它的源码实现，源码如下：</p>
<pre><code class="language-java">// RedissonMultiLock类 → lock()方法
public void lock() {
    try {
        // 调用了lockInterruptibly获取锁
        this.lockInterruptibly();
    } catch (InterruptedException var2) {
        // 如果出现异常则中断当前线程
        Thread.currentThread().interrupt();
    }
}

// RedissonMultiLock类 → lockInterruptibly()方法
public void lockInterruptibly() throws InterruptedException {
    // 这里传入了-1
    this.lockInterruptibly(-1L, (TimeUnit)null);
}

// RedissonMultiLock类 → lockInterruptibly()重载方法
public void lockInterruptibly(long leaseTime, TimeUnit unit)
                throws InterruptedException {
    // 计算基础阻塞时间：使用锁的个数*1500ms。
    // 比如之前的案例：3*1500=4500ms
    long baseWaitTime = (long)(this.locks.size() * 1500);
    long waitTime = -1L;
    // 前面传入了-1，所以进入的是if分支
    if (leaseTime == -1L) {
        // 挂起时间为4500，单位毫秒（MS）
        waitTime = baseWaitTime;
        unit = TimeUnit.MILLISECONDS;
    } 
    // 这里是对于外部获取锁时，指定了时间情况时的处理逻辑
    else {
        // 将外部传入的时间转换为毫秒值
        waitTime = unit.toMillis(leaseTime);
        // 如果外部给定的时间小于2000ms，那么赋值为2s
        if (waitTime &lt;= 2000L) {
            waitTime = 2000L;
        } 
        // 如果传入的时间小于前面计算出的基础时间
        else if (waitTime &lt;= baseWaitTime) {
            // 获取基础时间的一半，如baseWaitTime=4500ms，waitTime=2250ms
            waitTime = ThreadLocalRandom.current().
                nextLong(waitTime / 2L, waitTime);
        } else {
            // 如果外部给定的时间大于前面计算出的基础时间会进这里
            // 将基础时间作为阻塞时长
            waitTime = ThreadLocalRandom.current().
                nextLong(baseWaitTime, waitTime);
        }
        // 最终计算出挂起的时间
        waitTime = unit.convert(waitTime, TimeUnit.MILLISECONDS);
    }
    // 自旋尝试获取锁，直至获取锁成功
    while(!this.tryLock(waitTime, leaseTime, unit)) {
        ;
    }
}
</code></pre>
<p>上述源码中，实际上不难理解，比之前文章中分析的JUC的源码可读性强很多，上述代码中，简单的计算了一下时间后，最终自旋调用了<code>tryLock</code>获取锁的方法一直尝试获取锁。接着来看看<code>tryLock</code>方法：</p>
<pre><code class="language-java">// RedissonMultiLock类 → tryLock()方法
public boolean tryLock(long waitTime, long leaseTime,
        TimeUnit unit) throws InterruptedException {
    long newLeaseTime = -1L;
    // 如果外部获取锁时，给定了过期时间
    if (leaseTime != -1L) {
        // 将newLeaseTime变为给定时间的两倍
        newLeaseTime = unit.toMillis(waitTime) * 2L;
    }
    
    // 获取当前时间
    long time = System.currentTimeMillis();
    long remainTime = -1L;
    // 如果不是非阻塞式获取锁
    if (waitTime != -1L) {
        // 将过期时间改为用户给定的时间
        remainTime = unit.toMillis(waitTime);
    }
    // 该方法是空实现，留下的拓展接口，直接返回了传入的值
    long lockWaitTime = this.calcLockWaitTime(remainTime);
    // 返回0，也是拓展接口，留给子类拓展的，红锁中就拓展了这两方法
    // 这个变量是允许失败的最大次数，红锁中为个数的一半
    int failedLocksLimit = this.failedLocksLimit();
    // 获取组成连锁的所有RLock锁集合
    List&lt;RLock&gt; acquiredLocks = new ArrayList(this.locks.size());
    // 获取list的迭代器对象
    ListIterator iterator = this.locks.listIterator();
    
    // 通过List的迭代器遍历整个连锁集合
    while(iterator.hasNext()) {
        RLock lock = (RLock)iterator.next();
        
        boolean lockAcquired;
        // 尝试获取锁
        try {
            // 如果是非阻塞式获取锁
            if (waitTime == -1L &amp;&amp; leaseTime == -1L) {
                // 直接尝试获取锁
                lockAcquired = lock.tryLock();
            } else {
                // 比较阻塞时间和过期时间的大小
                long awaitTime = Math.min(lockWaitTime, remainTime);
                // 尝试重新获取锁
                lockAcquired = lock.tryLock(awaitTime, 
                    newLeaseTime, TimeUnit.MILLISECONDS);
            }
        // 如果redis连接中断/关闭了
        } catch (RedisConnectionClosedException var21) {
            // 回滚获取成功的锁（删除写入成功的key）
            this.unlockInner(Arrays.asList(lock));
            lockAcquired = false;
        // 如果在给定时间内未获取到锁
        } catch (RedisResponseTimeoutException var22) {
            // 也回滚所有获取成功的个锁
            this.unlockInner(Arrays.asList(lock));
            lockAcquired = false;
        } catch (Exception var23) {
            // 如果是其他原因导致的，则直接返回获取锁失败
            lockAcquired = false;
        }
        
        // 如果获取一把个锁成功
        if (lockAcquired) {
            // 那么则记录获取成功的个锁
            acquiredLocks.add(lock);
        } else {
            // 如果获取一把个锁失败，此次失败的次数已经达到了
            // 最大的失败次数，那么直接退出循环，放弃加锁操作
            if (this.locks.size() - acquiredLocks.size() 
                == this.failedLocksLimit()) {
                break;
            }
            // 允许失败的次数未0，获取一个个锁失败则回滚
            if (failedLocksLimit == 0) {
                // 回滚所有成功的锁 
                this.unlockInner(acquiredLocks);
                // 如果是非阻塞式获取锁，则直接返回获取锁失败
                if (waitTime == -1L &amp;&amp; leaseTime == -1L) {
                    return false;
                }
                // 获取最新的失败锁的个数
                failedLocksLimit = this.failedLocksLimit();
                acquiredLocks.clear();
                // 移动迭代器的指针位置到上一个
                while(iterator.hasPrevious()) {
                    iterator.previous();
                }
            
            // 如果允许失败的次数不为0
            } else {
                // 每获取个锁失败一次就减少一个数
                --failedLocksLimit;
            }
        }
        // 如果不是非阻塞式获取锁
        if (remainTime != -1L) {
            // 计算本次获取锁的所耗时长
            remainTime -= System.currentTimeMillis() - time;
            time = System.currentTimeMillis();
            // 如果已经超出了给定时间，则回滚所有成功的锁
            if (remainTime &lt;= 0L) {
                this.unlockInner(acquiredLocks);
                // 返回获取锁失败
                return false;
            }
        }
    }
    
    // 能执行到这里肯定是已经获取锁成功了
    // 判断是否设置了过期时间，如果设置了
    if (leaseTime != -1L) {
        // 获取加锁成功的个锁集合
        List&lt;RFuture&lt;Boolean&gt;&gt; futures = new ArrayList(acquiredLocks.size());
        Iterator var25 = acquiredLocks.iterator();

        // 迭代为每个获取成功的个锁创建异步任务对象
        while(var25.hasNext()) {
            RLock rLock = (RLock)var25.next();
            RFuture&lt;Boolean&gt; future =
                rLock.expireAsync(unit.toMillis(leaseTime),
                TimeUnit.MILLISECONDS);
            futures.add(future);
        }
        // 获取Future的个锁集合迭代器对象
        var25 = futures.iterator();
        
        // 迭代每个Futrue对象
        while(var25.hasNext()) {
            RFuture&lt;Boolean&gt; rFuture = (RFuture)var25.next();
            // 异步为每个获取个锁成功的对象设置过期时间
            rFuture.syncUninterruptibly();
        }
    }
    // 返回获取锁成功
    return true;
}
</code></pre>
<p>如上源码，流程先不分析，先感慨一句：虽然看着长，但！！！真心的比JUC中的源码可读性和易读性高N倍，每句代码都容易弄懂，阅读起来并不算费劲。<br>
OK~，感慨完之后来总结一下<code>tryLock</code>加锁方法的总体逻辑：</p>
<ul>
<li>①计算出阻塞时间、最大失败数以及过期时间，然后获取所有组成连锁的个锁集合</li>
<li>②迭代每把个锁，尝试对每把个锁进行加锁，加锁是也会判断获取锁的方式是否为非阻塞式的：
<ul>
<li>是：直接获取锁</li>
<li>否：阻塞式获取锁，在给定时间内会不断尝试获取锁</li>
</ul>
</li>
<li>③判断个锁是否获取成功：
<ul>
<li>成功：将获取成功的个锁添加到加锁成功的集合<code>acquiredLocks</code>集合中</li>
<li>失败：判断此次获取锁失败的次数是否已经达到了允许的最大失败次数：
<ul>
<li>是：放弃获取锁，回滚所有获取成功的锁，返回获取锁失败</li>
<li>否：允许失败次数自减，继续尝试获取下一把个锁</li>
<li>注意：连锁模式下最大失败次数=0，红锁模式下为个锁数量的一半</li>
</ul>
</li>
</ul>
</li>
<li>④判断目前获取锁过程的耗时是否超出了给定的阻塞时长：
<ul>
<li>是：回滚所有获取成功的锁，然后返回获取锁失败</li>
<li>否：继续获取下把个锁</li>
</ul>
</li>
<li>⑤如果连锁获取成功(代表所有个都锁获取成功)，判断是否指定了过期时间：
<ul>
<li>是：异步为每个加锁成功的个锁设置过期时间并返回获取锁成功</li>
<li>否：直接返回获取锁成功</li>
</ul>
</li>
</ul>
<p>虽然获取锁的代码看着长，但其逻辑并不算复杂，上述过程是连锁的实现，而红锁则是依赖于连锁实现的，也比较简单，只是重写<code>failedLocksLimit()</code>获取允许失败次数的方法，允许获取锁失败的次数变为了个锁数量的一半以及略微加了一些小拓展，感兴趣的可以自己去分析其实现。</p>
<p>接着来看看释放锁的源码实现：</p>
<pre><code class="language-java">// RedissonMultiLock类 → unlock()方法
@Override
public void unlock() {
    // 创建为没把个锁创建一个Future
    List&lt;RFuture&lt;Void&gt;&gt; futures = new
        ArrayList&lt;RFuture&lt;Void&gt;&gt;(locks.size());
    // 遍历所有个锁
    for (RLock lock : locks) {
        // 释放锁
        futures.add(lock.unlockAsync());
    }
    // 阻塞等待所有锁释放成功后再返回
    for (RFuture&lt;Void&gt; future : futures) {
        future.syncUninterruptibly();
    }
}

// RedissonMultiLock类 → unlockInnerAsync()方法
protected RFuture&lt;Boolean&gt; unlockInnerAsync(long threadId) {
    // 获取个锁的Key名称并通过Lua脚本释放锁（确保原子性）
    return commandExecutor.evalWriteAsync(getName(), 
        LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN,
        &quot;if (redis.call('exists', KEYS[1]) == 0) then &quot; +
            &quot;redis.call('publish', KEYS[2], ARGV[1]); &quot; +
            &quot;return 1; &quot; +
        &quot;end;&quot; +
        &quot;if (redis.call('hexists', KEYS[1], ARGV[3]) == 0) then &quot; +
            &quot;return nil;&quot; +
        &quot;end; &quot; +
        &quot;local counter = redis.call('hincrby', KEYS[1], ARGV[3], -1); &quot; +
        &quot;if (counter &gt; 0) then &quot; +
            &quot;redis.call('pexpire', KEYS[1], ARGV[2]); &quot; +
            &quot;return 0; &quot; +
        &quot;else &quot; +
            &quot;redis.call('del', KEYS[1]); &quot; +
            &quot;redis.call('publish', KEYS[2], ARGV[1]); &quot; +
            &quot;return 1; &quot;+
        &quot;end; &quot; +
        &quot;return nil;&quot;,
        Arrays.&lt;Object&gt;asList(getName(), getChannelName()),
        LockPubSub.unlockMessage, internalLockLeaseTime,
        getLockName(threadId));
}
</code></pre>
<p>释放锁的逻辑更加简单，遍历所有的个锁，然后异步通过Lua脚本删除所有的<code>key</code>，在连锁的释放代码中会同步阻塞等待所有锁的<code>Key</code>删除后再返回。</p>
<h3 id="zookeeper实现分布式锁剖析">Zookeeper实现分布式锁剖析</h3>
<p>Zookeeper分布式锁是依赖于其内部的顺序临时节点实现的，其原理就类似于最开始举例的那个文件夹分布式锁，</p>
<blockquote>
<p>我们只需要找一个多个进程之间所有线程可见的区域实现这个互斥量即可。<br>
比如：在一台服务器的同一路径下创建一个文件夹。获取锁操作则是创建文件夹，反之，释放锁的逻辑则是删除文件夹，这样可以很好的实现一把分布式锁，因为OS特性规定，在同一路径下，相同名字的文件夹只能创建一个。所以当两条线程同时执行获取锁逻辑时，永远只会有一条线程创建成功，成功创建文件夹的那条线程则代表获取锁成功，那么可以去执行业务逻辑。当这条线程执行完业务后，再删除掉文件夹，代表释放锁，以便于其他线程可以再次获取锁。</p>
</blockquote>
<p>因为Zookeeper实际上就类似于一个文件系统的结构。我们可以通过<a href="https://curator.apache.org/" title="https://curator.apache.org/">Curator</a>框架封装的API操作Zookeeper，完成分布式锁的实现。如下：</p>
<pre><code class="language-java">// 创建分布式锁对象
InterProcessMutex lock = InterProcessMutex(client,
    &quot;/locks/distributed_商品ID&quot;);
lock.acquire(); // 获取锁/加锁

// 执行业务逻辑...

lock.release(); // 释放锁
</code></pre>
<p>如上，通过Curator实现分布式锁非常简单，因为已经封装好了API，所以应用起来也非常方便，同时Zookeeper也可以实现公平锁与非公平锁两种方案，如下：</p>
<ul>
<li>公平锁：先请求锁的线程一定先获取锁
<ul>
<li>实现方式：通过临时顺序节点实现，每条线程请求锁时为其创建一个有序节点，创建完成之后判断自己创建的节点是不是最小的，如果是则直接获取锁成功，反之获取锁失败，创建一个监听器，监听自己节点的前一个节点状态，当前一个节点被删除（代表前一个节点的创建线程释放了锁）自己尝试获取锁</li>
<li>优劣势：可以保证请求获取锁的有序性，但性能方面比非公平锁低</li>
</ul>
</li>
<li>非公平锁：先请求锁的线程不一定先获取锁
<ul>
<li>实现方式：多条线程在同一目录下，同时创建一个名字相同的节点，谁创建成功代表获取锁成功，反之则代表获取锁失败</li>
<li>优劣势：性能良好，但无法保证请求获取锁时的有序性</li>
</ul>
</li>
</ul>
<p>对于这两种实现方式，非公平锁的方案与前面的<code>Redis</code>实现差不多，所以不再分析。下面重点来分析一下Zookeeper实现分布式的公平锁的大致原理。但在分析之前先简单说明一些Zookeeper中会用到的概念。如下：</p>
<ul>
<li>节点类型：
<ul>
<li>①持久节点：被创建后会一直存在的节点信息，除非有删除操作主动清楚才会销毁</li>
<li>②持久顺序节点：持久节点的有序版本，每个新创建的节点会在后面维护自增值保持先后顺序，可以用于实现分布式全局唯一ID</li>
<li>③临时节点：被创建后与客户端的会话生命周期保持一致，连接断开则自动销毁</li>
<li>④临时顺序节点：临时节点的有序版本，与其多了一个有序性。分布式锁则依赖这种类型实现</li>
</ul>
</li>
<li>监视器：当zookeeper创建一个节点时，会为该节点注册一个监视器，当节点状态发生改变时，watch会被触发，zooKeeper将会向客户端发送一条通知。不过值得注意的是watch只能被触发一次</li>
</ul>
<p>ok~，假设目前<code>8001</code>服务中的线程<code>T1</code>尝试获取锁，那么会<code>T1</code>会在<code>Zookeeper</code>的<code>/locks/distributed_商品ID</code>目录下创建一个临时节点，<code>Zookeeper</code>内部会生成一个名字为<code>xxx....-0000001</code>临时顺序节点。当第二条线程来尝试获取锁时，也会在相同位置创建一个临时顺序节点，名字为<code>xxx....-0000002</code>。值得注意的是最后的数字是一个递增的状态，从1开始自增，Zookeeper会维护这个先后顺序。如下图：<br>
<img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cd362a2cc5d3469d98c70a26aa4c18f1~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image?" alt="创建临时节点" loading="lazy"><br>
当线程创建节点完成后，会查询<code>/locks/distributed_商品ID</code>目录下所有的子节点，然后会判断自己创建的节点是不是在所有节点的第一个，也就是判断自己的节点是否为最小的子节点，如果是的话则获取锁成功，因为当前线程是第一个来获取分布式锁的线程，在它之前是没有线程获取锁的，所以当然可以加锁成功，然后开始执行业务逻辑。如下：<br>
<img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3fde28ce9a384b6fb740ef47ace87e39~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image?" alt="8001线程T1获取分布式锁" loading="lazy"><br>
而第二条线程创建节点成功后，也会去判断自己是否是最小的节点。哦豁！第二条线程判断的时候会发现，在自己的节点之前还有一个<code>xxx...-0001</code>节点，所以代表在自己前面已经有线程持有了分布式锁，所以会对上个节点加上一个监视器，监视上个节点的状态变化。如下：<br>
<img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2adb15356571459686fd2246a427305f~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image?" alt="Zookeeper实现分布式公平锁" loading="lazy"><br>
此时，第一条线程<code>T1</code>执行完了业务代码，准备释放锁，也就是删除自己创建的<code>xxx...-0001</code>临时顺序节点。而第二条线程创建的监视器会监视着前面一个节点的状态，当发现前面的节点已经被删除时，就知道前面一条线程已经执行完了业务，释放了锁资源，所以再次尝试获取锁。如下：<br>
<img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4f317ee163ea4138b66c607d56c5781c~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image?" alt="Zookeeper实现分布式锁完整流程" loading="lazy"><br>
第二条线程重新尝试获取锁时，拿到当前目录下的所有节点判断发现，哟！自己是第一个(最小的那个)节点啊？然后获取锁成功，开始执行业务逻辑，后续再来新的线程则依次类推.....</p>
<p>至此，整个Zookeeper实现分布式锁的过程分析完毕，关于自己动手基于Zookeeper实现一遍我这边就不再写了，大家可以自习了解。实际开发过程中，<a href="https://curator.apache.org/" title="https://curator.apache.org/">Curator</a>框架自带的分布式锁实现便已经够用了，同时使用也非常的方便。</p>
<h3 id="分布式锁性能优化">分布式锁性能优化</h3>
<p>经过前述的分析，大家对分布式锁应该都有一个全面认知了，但是请思考：如果对于类似于抢购、秒杀业务，又该如何处理呢？因为在这种场景下，往往在一段时间内会有大量用户去请求同一个商品。从技术角度出发，这样会导致在同一时间内会有大量的线程去请求同一把锁。这会有何种隐患呢？会出现的问题是：虽然并发能抗住，但是对于用户体验感不好，同时大量的用户点击抢购，但是只能有一个用户抢购成功，明显不合理，这又该如何优化？</p>
<blockquote>
<p>参考并发容器中的分段容器，可以将共享资源（商品库存）做提前预热，分段分散到redis中。举个例子：</p>
<blockquote>
<p>1000个库存商品，10W个用户等待抢购，抢购开始时间为下午15:00<br>
提前预热：两点半左右开始将商品数量拆成10份或N份，如：[shopping_01；0-100]、[shopping_02；101-200]、[shopping_03；201-300]、[......]<br>
也就是往redis中写入十个key，值为100，在抢购时，过来的请求随机分散到某个key上去，但是在扣减库存之前，需要先获取锁，这样就同时有了十把锁，性能自然就上去了。</p>
</blockquote>
</blockquote>
<h3 id="分布式锁总结">分布式锁总结</h3>
<p>本篇中从<code>单机锁的隐患 -&gt; 分布式架构下的安全问题引出 -&gt; 分布式锁的实现推导 -&gt; redis实现分布式锁 -&gt; redis实现分布式锁的细节问题分析 -&gt; redisson框架实现及其连锁应用与源码分析 -&gt; zookeeper实现分布式锁 -&gt; zookeeper实现原理</code>这条思路依次剖析了分布式锁的前世今生，总的一句话概括分布式锁的核心原理就是：在多个进程中所有线程都可见的区域实现了互斥量而已。</p>
<p>最后再来说说Redis与Zookeeper实现的区别与项目中如何抉择？</p>
<blockquote>
<p>Redis数据不是实时同步的，主机写入成功后会立即返回，存在主从架构锁失效问题。<br>
Zookeeper数据是实时同步的，主机写入后需一半节点以上写入成功才会返回。<br>
所以如果你的项目追求高性能，可以放弃一定的稳定性，那么推荐使用Redis实现。比如电商、线上教育等类型的项目。<br>
但如果你的项目追求高稳定，愿意牺牲一部分性能换取稳定性，那么推荐使用Zookeeper实现。比如金融、银行、政府等类型的项目。</p>
</blockquote>
]]></content>
    </entry>
</feed>